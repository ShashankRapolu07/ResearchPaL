[
    "Flaming-hot Initiation with Regular Execution Sampling for Large\nLanguage Models\nWeizhe Chen\nUniversity of Southern California\nweizhech@usc.edu\nZhicheng Zhang\nCarnegie Mellon University\nzhichen3@cs.cmu.edu\nGuanlin Liu, Renjie Zheng, Wenlei Shi, Chen Dun, Zheng Wu, Xing Jin, Lin Yan\nByteDance\n{guanlin.liu, renjie.zheng, wenlei.shi}@bytedance.com\n{chen.dun, zheng.wu1, jinxing.9, neil}@bytedance.com\nAbstract\nSince the release of ChatGPT, large language\nmodels (LLMs) have demonstrated remarkable\ncapabilities across various domains. A key chal-\nlenge in developing these general capabilities is\nefficiently sourcing diverse, high-quality data.\nThis becomes especially critical in reasoning-\nrelated tasks with sandbox checkers, such as\nmath or code, where the goal is to generate cor-\nrect solutions to specific problems with higher\nprobability. In this work, we introduce Flaming-\nhot Initiation with Regular Execution (FIRE)\nsampling, a simple yet highly effective method\nto efficiently find good responses. Our em-\npirical findings show that FIRE sampling en-\nhances inference-time generation quality and\nalso benefits training in the alignment stage.\nFurthermore, we explore how FIRE sampling\nimproves performance by promoting diversity\nand analyze the impact of employing FIRE at\ndifferent positions within a response.\n1\nIntroduction\nLarge language models (LLMs) have achieved re-\nmarkable success in a wide range of tasks since\nthe release of ChatGPT (OpenAI, 2022). In ad-\ndition to traditional natural language processing\ntasks such as summarization and sentiment analy-\nsis, LLMs have demonstrated effectiveness in many\nnew domains, including code generation (Chen\net al., 2023; Roziere et al., 2023), human-computer\ninteraction (Li et al., 2023), and math problem-\nsolving (Wei et al., 2022; Yu et al., 2024). Al-\nthough standalone LLMs have limited reasoning\ncapabilities (Sun et al., 2023; Valmeekam et al.,\n2023; Chen et al., 2024b), researchers have tried to\nenhance them by incorporating tool-use and devel-\noping integrated systems known as LLM agents (Xi\net al., 2023; Wang et al., 2024), which expands the\napplications of LLMs to more general domains like\nrobot control (Wang et al., 2023a) and autonomous\ndriving (Mao* et al., 2023).\nTo develop general capabilities, LLMs are typ-\nically trained through a three-stage process: pre-\ntraining, supervised fine-tuning (SFT), and align-\nment (Bai et al., 2022; Ouyang et al., 2022). Dur-\ning pretraining, the model learns from a vast array\nof data gathered from publicly available sources.\nThen, in the SFT and alignment stages, the model\u2019s\nabilities are further refined, allowing it to increase\nreasoning abilities and better follow users\u2019 instruc-\ntions. In order to enhance reasoning tasks, a sand-\nbox checker \u2014 a tool used to verify the correctness\nof solutions \u2014 is often used during training (Liu\net al., 2023b). Therefore, one of the key challenges\nin achieving effective and efficient training is de-\ntermining how to obtain more successful samples\nwithin a fixed number of trials, particularly when\naddressing complex problems.\nIn this paper, we introduce Flaming-hot Initia-\ntion with Regular Execution (FIRE), a simple yet\neffective sampling method for training large lan-\nguage models. Inspired by recent findings on atten-\ntion sink (Xiao et al., 2023), our approach begins\nby sampling the initial token at a very high tem-\nperature and proceeds with the regular sampling\nprocess for the remaining sequence. Our algorithm\ncan be viewed as a simplified and more general\nversion of CoT-decoding (Wang and Zhou, 2024),\nespecially with a focus on training in math and cod-\ning domains where a sandbox checker is available\nat a relatively cheap cost.\nWe first show that our method, at inference time,\ncan improve the pass rate within N trials (pass@n),\nalso known as the best-of-N (BoN) when only the\ncorrectness of the final answer is considered. To\ndemonstrate its effectiveness in training, we show\nthat it can be directly integrated into the reinforce-\nment learning process of large language models.\nOur approach proves to be effective across multiple\nopen-source models and various LLM capabilities,\nincluding mathematical reasoning and coding. We\nhighlight how our method promotes diversity in\n1\narXiv:2410.21236v1  [cs.LG]  28 Oct 2024\ngenerated samples, a key factor linked to perfor-\nmance improvements in pass rate. Importantly, this\ndiversity is maintained even after training with our\nsampling method, indicating room for further en-\nhancement. We also discuss the effects of simple\nvariations of our method, where the temperature\nchange occurs mid-process rather than at the start,\non performance outcomes.\n2\nRelated Works\nResearchers have been exploring two primary direc-\ntions to efficiently improve response quality under a\nfrozen pre-trained LLM. The first direction focuses\non prompting techniques such as Chain-of-Thought\n(Wei et al., 2022) and Tree-of-Thought (Yao et al.,\n2023a).\nThe second direction involves letting\nLLMs fix their own mistakes (Wang et al., 2023b;\nYao et al., 2023b; Shinn et al., 2023; Madaan et al.,\n2023; Chen et al., 2024a). In line with these two\ndirections, there has been increasing focus on con-\ntrolled decoding in LLMs to enhance reasoning\ncapabilities during inference, ranging from search-\nbased approaches applied to policy models (Mud-\ngal et al., 2023; Huang et al., 2024) to utilizing\nvalue models trained in the alignment phase (Liu\net al., 2023a; Feng et al., 2023).\nIn this paper, we also focus on inference time;\nhowever, our approach extends to the sampling\nprocesses used during the training of large lan-\nguage models, as commonly practiced in Instruct-\nGPT (Ouyang et al., 2022). This process consists\nof three key stages: pretraining, supervised fine-\ntuning (SFT), and alignment, also known as rein-\nforcement learning with human feedback (RLHF).",
    "of three key stages: pretraining, supervised fine-\ntuning (SFT), and alignment, also known as rein-\nforcement learning with human feedback (RLHF).\nFor large language models trained in this paradigm,\nthere could be some helpful properties that, without\nstrong theoretical guarantees, are empirically true\nand thus helpful for LLMs. Our work is related to\nattention sink (Xiao et al., 2023). An attention sink\nrefers to a token or set of tokens that dispropor-\ntionately receive attention from other tokens during\nthe attention mechanism within transformer archi-\ntectures. In their study, they found that one of the\nmost identifiable tokens was shown to be the initial\ntoken. While there are no theoretical guarantees,\nthey propose an intuition that initial tokens are vis-\nible and used in all later token generations, making\nthem more readily trained to be attention sinks.\nOur\nwork\nis\nclosely\nrelated\nto\nCoT-\ndecoding\n(Wang\nand\nZhou,\n2024),\nwhich\nuncovers the CoT-paths by enumerating over\nthe top-k alternative tokens and aggregating\nthe responses by scoring the decoded responses\nwith confidence on the final answer. However,\nour approach differs in three key aspects: (1)\nwe introduce a differentiable sampling method\nthat can be directly integrated with existing\ninference and training frameworks, (2) we focus\non improving model performance in scenarios with\na sandbox checker, where aggregating responses\nis less data-efficient, and (3) our method operates\nwithout assumptions about the prompts, even when\na chain of thought (CoT) is included, extending\nbeyond the scope of CoT-decoding.\n3\nFlaming-hot Initiation Regular\nExecution\n3.1\nMethod\nIn this work, we propose a sampling method,\nFlaming-hot Initiation with Regular Execution\n(FIRE), inspired by the attention sink phe-\nnomenon (Xiao et al., 2023) that demonstrates the\nimportance of initial tokens.\nFIRE first samples the initial token at a very high\ntemperature p \u226b1, combined with top-k filtering\nto make the candidate tokens more controllable. At\nhigher temperatures, the candidate tokens are sam-\npled from a probability distribution that approaches\nuniform sampling. After the initial token is sam-\npled, FIRE proceeds with the decoding stage using\na regular temperature setting.\nOur\napproach\nFIRE\nis\nsimilar\nto\nCoT-\ndecoding (Wang and Zhou, 2024) that enumerates\nthe top-k candidates of the initial token. However,\nwhile CoT-decoding focuses more on the decod-\ning stage and extracting Chain-of-Thought without\nprompt, our approach FIRE serves as a general\ndifferentiable sampling method, which can be com-\nbined with existing sampling frameworks and can\nbe more efficient in the training stage where a sand-\nbox checker that judges whether a specific answer\nis correct or not is available with a cheap cost.\nWhile FIRE can be applied to any token in the\ndecoding stage, we restrict its application to the\ninitial token to prevent the generation of random\ntokens that are wrong in the context. For example,\nif we apply FIRE after the prefix \"1+2=\", it would\nsample, in addition to the token \"3\", other tokens\nlike \"4\" or \"5\", which are very likely to be wrong.\nIn contrast, since FIRE is only applied to the initial\ntoken, it would unlikely lead to broken sentences\nor code with syntax errors. In our empirical exper-\n2\nRegular\nFIRE\nModel\nPass%\n#EA\nPass%\n#EA\nDeepSeek\n97.57\n2.26\n98.71\n2.76\nGSM8K\nGemma-2\n86.81\n3.87\n87.57\n4.01\nQwen2\n95.90\n2.58\n98.25\n3.17\nQwen2-RL\n96.90\n2.63\n97.90\n3.26\nDeepSeek\n76.16\n5.63\n78.16\n7.89\nMATH\nGemma-2\n49.20\n9.24\n51.48\n10.39\nQwen2\n76.60\n7.44\n79.08\n9.03\nQwen2.5-72B\n79.30\n2.39\n80.40\n2.60\nTable 1: Inference results for different models on dif-\nferent datasets with best hyperparameters combinations.\nSpecifically, Qwen2-RL is a fine-tuned model trained by\nourselves. We show the pass rate (%) with 40 samples,\nand the effective answers (EA) among the 40 samples.\nRegular\nFIRE\nPass@1\nPass@10\nPass@1\nPass@10\nMBPP\n61.2\n82.8\n50.6\n86.6\nMBPP+\n52.7\n74.2\n44.1\n77.0\nTable 2: Pass rate (%) with different number of samples\nfrom Qwen2-7B-Instruct on MBPP and MBPP+.\niments, we found that the initial token frequently\nconsists of words like \"Let\u2019s\", \"Sure\", \"So\", and\n\"The\", which do not directly convey any informa-\ntion. But what these initial tokens affect is the\nreasoning steps afterward, with the same intuition\nas StreamingLLM (Xiao et al., 2023).\n3.2\nExperiments\nIn this section, we evaluate our algorithm, FIRE,\nby addressing several key research questions that\nguide our experiments.\nHow effective is FIRE during inference?\nWe\nfirst showcase the effectiveness of FIRE sampling\nin inference-only scenarios. We tested four open-\nsource models: Qwen2-7B-Instruct (Qwen2) (Yang\net al., 2024), Qwen2.5-72B-Instruct (Qwen2.5-\n72B) (Yang et al., 2024), DeepSeek-coder-v2-\nInstruct (DeepSeek)(Zhu et al., 2024), and Gemma-\n2-2b-it (Gemma-2)(Team et al., 2024), on a di-\nverse set of datasets, including GSM8K (Cobbe\net al., 2021), MATH (Hendrycks et al., 2021), and\nMBPP(+) (Austin et al., 2021; Liu et al., 2023c). In\nGSM8K and MATH, we extend the prompts with\nphrase \"Please reason step-by-step\" to ensure CoT",
    "MBPP(+) (Austin et al., 2021; Liu et al., 2023c). In\nGSM8K and MATH, we extend the prompts with\nphrase \"Please reason step-by-step\" to ensure CoT\nreasoning in models\u2019 responses, a setting where the\noriginal motivation of CoT-decoding becomes less\nmeaningful as CoT paths would naturally occur.\nFor the regular sampling settings, we use a com-\nbination of nucleus sampling and top-k sampling.\nRegular\nFIRE\np\nk\nmin-p\nn=10\nn=40\nn=10\nn=40\n0.7\n16\n0.01\n66.4\n75.8\n70.0\n78.9\n0\n66.4\n75.8\n70.0\n78.9\n32\n0.01\n66.2\n75.3\n70.1\n78.9\n0\n66.2\n75.2\n70.1\n78.9\n0.9\n16\n0.01\n66.1\n76.6\n69.5\n78.9\n0\n66.2\n76.6\n69.5\n78.9\n32\n0.01\n66.7\n76.4\n69.5\n79.1\n0\n66.8\n74.4\n69.1\n79.0\nTable 3: Pass rate (%) for Qwen2-7B-Instruct on MATH\ndataset with different hyperparameter combinations. p:\nnucleus sampling parameter, k: top-k sampling parame-\nter, min-p: minimum probability threshold (0 indicates\nmin-p is not used). n=10 and n=40 represent the number\nof samples for calculating the pass rate.\nTo ensure a fair comparison, we conducted a thor-\nough enumeration over hyperparameters, including\np, k, and min-p (Huggingface, 2023). Table 1 and\nTable 2 present the aggregated results, where the\nreported numbers represent the best outcomes from\nthe enumeration. We observe that FIRE consis-\ntently improves the pass rate compared to regular\nsettings across all models on different benchmarks.\nTo further demonstrate the consistent improvement\nover different hyperparameters, we provide an ex-\nample result of Qwen2-7B-Instruct on the MATH\ndataset in Table 3. Full results for all models and\ndatasets are provided in the appendix. Table 3 re-\nveals that although FIRE may alter the hyperparam-\neter combination that yields optimal performance,\nit consistently outperforms regular sampling across\nall hyperparameter combinations.\nWhy is FIRE effective?\nFIRE introduces more\ndiversity to the initial token that is generated at a\nhigh temperature, and due to the strong attention\nscores towards initial tokens (Xiao et al., 2023),\nthis diversity benefits the entire subsequent gen-\neration. To measure diversity quantitatively, we\nuse the number of unique answers (effective an-\nswers) within a set of responses as our metric.\nWe choose not to use some popular metrics like\nn-grams since we only control the initial token,\nand in tasks with long reasoning paths, such as\nmath and coding, similar n-grams will likely al-\nways appear, making it unsuitable for measuring\ndiversity. As shown in Figure 1, Table 1 (#EA),\nFIRE demonstrates increased diversity across vari-\nous models and datasets, which contributes to en-\nhanced pass@n performance. As anticipated, FIRE\n3\n(a) Deepseek-Code-v2-Lite-Instruct\n(b) Qwen2-7B-Instruct\nFigure 1: Curves for pass rate and number of effective answers with different numbers of samples on GSM8K.\nDataset\nModel\nPPO\nPPO+FIRE\nDeepseek\n80.64\n82.16\nGSM8K\nQwen2\n80.16\n82.02\nGemma\n40.39\n42.91\nGemma-2\n58.07\n61.20\nMATH\nQwen2\n53.50\n55.07\nTable 4: Pass@1 on GSM8K and Math for Different\nmodels trained with PPO with different sampling.\n1st-line\n2nd-line\n3rd-line\nPRM-line\nRegular\n46.07\n74.36\n74.77\n75.73\nFIRE\n64.59\n74.96\n75.92\n78.21\nTable 5: Pass@10 Results from Qwen2-7B-Instruct on\nthe training set of MATH dataset for FIRE variants with\ndifferent sampling points, compared to regular sampling\nmethod that does not change the temperature.\ndoes not improve Pass@1 performance due to its\nfocus on promoting diversity. However, it consis-\ntently delivers improvements when more samples\nare considered.\nIs FIRE helpful when integrated into train-\ning?\nHaving established that our method im-\nproves pass@n by improving diversity, we directly\napply FIRE to boost language model training. To\ntest this, we use Proximal Policy Optimization\n(PPO) (Schulman et al., 2017) to finetune several\nmodels using the GSM8K and MATH datasets, and\nassess their performance through the final pass rate\nfor single samples (Pass@1). As shown in Ta-\nble 4, integrating FIRE into the training process\nleads to an improvement in Pass@1. Notably, even\nthough each data point is sampled only once during\nPPO training following common practice (Ouyang\net al., 2022; Sheng et al., 2024), our method still\nyields improvements. The results also show that\nthe improvements are consistent for different mod-\nels. Furthermore, after our RL training, the model\nstill exhibits diversity and continues to benefit\nfrom inference-time pass rate improvements, as\nevidenced by Qwen2-RL in Table 1. Consequently,\nFIRE can be applied iteratively to refine the model,\nleading to an even bigger improvement margin.\nCan FIRE sampling work in mid-sequence?\nFinally, we explore the effect of applying FIRE\nsampling midway through a response. We first\nconstruct a dataset that ensures the correctness of\nthe initial sentences, by utilizing a Process Re-\nward Model (PRM) to identify the first sentences\nat which the response becomes incorrect. We then\nevaluate the effect of applying FIRE sampling at\nthe beginning of different sentences (1st, 2nd, and\n3rd-line) or at the first token deemed incorrect by\nthe PRM (\"PRM-line\"). We refer the reader to the\nappendix for a more detailed description of the con-\nstruction of this dataset. As shown in Table 5, while",
    "the PRM (\"PRM-line\"). We refer the reader to the\nappendix for a more detailed description of the con-\nstruction of this dataset. As shown in Table 5, while\nFIRE sampling offers benefits throughout different\nsettings, its advantages diminish for tokens beyond\nthe initial ones, despite an overall increase in accu-\nracy due to the prefix guaranteed to be correct.\n4\nConclusion\nIn this paper, we introduced a novel sampling\nmethod called Flaming-hot Initiation with Regular\nExecution (FIRE). Through empirical analysis, we\ndemonstrated that FIRE enhances both inference-\ntime performance and reinforcement learning, par-\nticularly when a chain of thought is integrated into\nthe prompt. We showed that FIRE improves gen-\neration diversity, and we believe that this diversity\ncontributes to its overall effectiveness. Addition-\nally, we explored several variants of FIRE that mod-\nify the sampling process not only immediately after\nthe question but also during the middle of the gen-\neration, further showcasing its versatility.\n4\n5\nLimitations\nWhile this work focuses on improving the effi-\nciency of LLM training through better sampling\nmethods, there are two limitations. First, our ap-\nproach lacks a strong theoretical guarantee, mean-\ning that there is a possibility that future models,\nespecially ones that are with different model archi-\ntectures, may not benefit from it. Second, although\nour method is designed for training LLMs, the\ninference-time algorithm could potentially bypass\nsafety measures by sampling out-of-distribution\ndata. However, we argue that this concern can be\ninherently mitigated in models trained with our\nproposed sampling technique.\nReferences\nJacob Austin, Augustus Odena, Maxwell Nye, Maarten\nBosma, Henryk Michalewski, David Dohan, Ellen\nJiang, Carrie Cai, Michael Terry, Quoc Le, et al. 2021.\nProgram synthesis with large language models. arXiv\npreprint arXiv:2108.07732.\nYuntao Bai, Andy Jones, Kamal Ndousse, Amanda\nAskell, Anna Chen, Nova DasSarma, Dawn Drain,\nStanislav Fort, Deep Ganguli, Tom Henighan, et al.\n2022. Training a helpful and harmless assistant with\nreinforcement learning from human feedback. arXiv\npreprint arXiv:2204.05862.\nWeizhe Chen, Sven Koenig, and Bistra Dilkina. 2024a.\nReprompt: Planning by automatic prompt engineer-\ning for large language models agents. arXiv preprint\narXiv:2406.11132.\nWeizhe Chen, Sven Koenig, and Bistra Dilkina. 2024b.\nWhy solving multi-agent path finding with large lan-\nguage model has not succeeded yet. arXiv preprint\narXiv:2401.03630.\nXinyun Chen, Maxwell Lin, Nathanael Sch\u00e4rli, and\nDenny Zhou. 2023. Teaching large language models\nto self-debug. arXiv preprint arXiv:2304.05128.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian,\nMark Chen, Heewoo Jun, Lukasz Kaiser, Matthias\nPlappert, Jerry Tworek, Jacob Hilton, Reiichiro\nNakano, Christopher Hesse, and John Schulman.\n2021. Training verifiers to solve math word prob-\nlems. arXiv preprint arXiv:2110.14168.\nXidong Feng, Ziyu Wan, Muning Wen, Ying Wen,\nWeinan Zhang, and Jun Wang. 2023. Alphazero-like\ntree-search can guide large language model decoding\nand training. arXiv preprint arXiv:2309.17179.\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul\nArora, Steven Basart, Eric Tang, Dawn Song, and\nJacob Steinhardt. 2021. Measuring mathematical\nproblem solving with the math dataset. NeurIPS.\nJames Y Huang, Sailik Sengupta, Daniele Bonadiman,\nYi-an Lai, Arshit Gupta, Nikolaos Pappas, Saab Man-\nsour, Katrin Kirchoff, and Dan Roth. 2024. Deal:\nDecoding-time alignment for large language models.\narXiv preprint arXiv:2402.06147.\nHuggingface.\n2023.\nNew\nsampling\nstrategy\ndropped\nin\ntransformers\n\u2013\nmin\np\nsampling.\nhttps://huggingface.co/posts/joaogante/\n319451541682734. Accessed: 2024-10-15.\nWoosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying\nSheng, Lianmin Zheng, Cody Hao Yu, Joseph E.\nGonzalez, Hao Zhang, and Ion Stoica. 2023. Effi-\ncient memory management for large language model\nserving with pagedattention. In Proceedings of the\nACM SIGOPS 29th Symposium on Operating Systems\nPrinciples.\nGuohao Li, Hasan Hammoud, Hani Itani, Dmitrii\nKhizbullin, and Bernard Ghanem. 2023. CAMEL:\ncommunicative agents for \"mind\" exploration of large\nlanguage model society. In Advances in Neural In-\nformation Processing Systems (NeurIPS).\nJiacheng Liu, Andrew Cohen, Ramakanth Pasunuru,\nYejin Choi, Hannaneh Hajishirzi, and Asli Celikyil-\nmaz. 2023a. Making ppo even better: Value-guided\nmonte-carlo tree search decoding. arXiv preprint\narXiv:2309.15028.\nJiate Liu, Yiqin Zhu, Kaiwen Xiao, Qiang Fu, Xiao Han,\nWei Yang, and Deheng Ye. 2023b. Rltf: Reinforce-\nment learning from unit test feedback. arXiv preprint\narXiv:2307.04349.\nJiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Ling-\nming Zhang. 2023c. Is your code generated by chat-\nGPT really correct? rigorous evaluation of large lan-\nguage models for code generation. In Thirty-seventh\nConference on Neural Information Processing Sys-\ntems.\nAman Madaan, Niket Tandon, Prakhar Gupta, Skyler\nHallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon,",
    "tems.\nAman Madaan, Niket Tandon, Prakhar Gupta, Skyler\nHallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon,\nNouha Dziri, Shrimai Prabhumoye, Yiming Yang,\net al. 2023. Self-refine: Iterative refinement with\nself-feedback. In Advances in Neural Information\nProcessing Systems (NeurIPS).\nJiageng Mao*, Junjie Ye*, Yuxi Qian, Marco Pavone,\nand Yue Wang. 2023.\nA language agent for au-\ntonomous driving. arXiv preprint arXiv:2311.10813.\nSidharth Mudgal,\nJong Lee,\nHarish Ganapathy,\nYaGuang Li, Tao Wang, Yanping Huang, Zhifeng\nChen, Heng-Tze Cheng, Michael Collins, Trevor\nStrohman, et al. 2023. Controlled decoding from\nlanguage models. arXiv preprint arXiv:2310.17022.\nOpenAI. 2022. Introducing chatgpt.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n5\n2022. Training language models to follow instruc-\ntions with human feedback. Advances in neural in-\nformation processing systems, 35:27730\u201327744.\nBaptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten\nSootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi,\nJingyu Liu, Tal Remez, J\u00e9r\u00e9my Rapin, et al. 2023.\nCode llama: Open foundation models for code. arXiv\npreprint arXiv:2308.12950.\nJohn Schulman, Filip Wolski, Prafulla Dhariwal,\nAlec Radford, and Oleg Klimov. 2017.\nProxi-\nmal policy optimization algorithms. arXiv preprint\narXiv:1707.06347.\nGuangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin\nWu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin\nLin, and Chuan Wu. 2024.\nHybridflow: A flex-\nible and efficient rlhf framework. arXiv preprint\narXiv:2409.19256.\nNoah Shinn, Federico Cassano, Ashwin Gopinath,\nKarthik Narasimhan, and Shunyu Yao. 2023. Re-\nflexion: language agents with verbal reinforcement\nlearning. In Advances in Neural Information Pro-\ncessing Systems (NeurIPS).\nJiankai Sun, Chuanyang Zheng, Enze Xie, Zhengying\nLiu, Ruihang Chu, Jianing Qiu, Jiaqi Xu, Mingyu\nDing, Hongyang Li, Mengzhe Geng, et al. 2023. A\nsurvey of reasoning with foundation models. arXiv\npreprint arXiv:2312.11562.\nGemma Team, Morgane Riviere, Shreya Pathak,\nPier Giuseppe Sessa, Cassidy Hardin, Surya Bhupati-\nraju, L\u00e9onard Hussenot, Thomas Mesnard, Bobak\nShahriari, Alexandre Ram\u00e9, et al. 2024. Gemma 2:\nImproving open language models at a practical size.\narXiv preprint arXiv:2408.00118.\nKarthik Valmeekam, Matthew Marquez, Alberto Olmo,\nSarath Sreedharan, and Subbarao Kambhampati.\n2023. Planbench: An extensible benchmark for eval-\nuating large language models on planning and reason-\ning about change. Advances in Neural Information\nProcessing Systems (NeurIPS).\nLei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao\nYang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang,\nXu Chen, Yankai Lin, et al. 2024. A survey on large\nlanguage model based autonomous agents. Frontiers\nof Computer Science, 18(6):186345.\nXuezhi Wang and Denny Zhou. 2024.\nChain-of-\nthought reasoning without prompting. arXiv preprint\narXiv:2402.10200.\nYen-Jen Wang, Bike Zhang, Jianyu Chen, and Koushil\nSreenath. 2023a. Prompt a robot to walk with large\nlanguage models. arXiv preprint arXiv:2309.09969.\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa\nLiu, Noah A. Smith, Daniel Khashabi, and Hannaneh\nHajishirzi. 2023b. Self-instruct: Aligning language\nmodels with self-generated instructions. In Annual\nMeeting of the Association for Computational Lin-\nguistics (ACL), pages 13484\u201313508. Association for\nComputational Linguistics.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le,\nand Denny Zhou. 2022. Chain-of-thought prompt-\ning elicits reasoning in large language models. In\nNeurIPS.\nZhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen\nDing, Boyang Hong, Ming Zhang, Junzhe Wang,\nSenjie Jin, Enyu Zhou, et al. 2023. The rise and\npotential of large language model based agents: A\nsurvey. arXiv preprint arXiv:2309.07864.\nGuangxuan Xiao, Yuandong Tian, Beidi Chen, Song\nHan, and Mike Lewis. 2023. Efficient streaming\nlanguage models with attention sinks. arXiv preprint\narXiv:2309.17453.\nAn Yang, Baosong Yang, Binyuan Hui, Bo Zheng,\nBowen Yu, Chang Zhou, Chengpeng Li, Chengyuan\nLi, Dayiheng Liu, Fei Huang, et al. 2024. Qwen2\ntechnical report. arXiv preprint arXiv:2407.10671.\nShunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran,",
    "technical report. arXiv preprint arXiv:2407.10671.\nShunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran,\nTom Griffiths, Yuan Cao, and Karthik Narasimhan.\n2023a. Tree of thoughts: Deliberate problem solving\nwith large language models. Advances in Neural\nInformation Processing Systems.\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak\nShafran, Karthik R. Narasimhan, and Yuan Cao.\n2023b. ReAct: Synergizing reasoning and acting\nin language models. In International Conference on\nLearning Representations (ICLR).\nLonghui Yu, Weisen Jiang, Han Shi, Jincheng Yu,\nZhengying Liu, Yu Zhang, James T Kwok, Zhenguo\nLi, Adrian Weller, and Weiyang Liu. 2024. Meta-\nmath: Bootstrap your own mathematical questions\nfor large language models. In International Confer-\nence on Learning Representations, (ICLR).\nQihao Zhu, Daya Guo, Zhihong Shao, Dejian Yang,\nPeiyi Wang, Runxin Xu, Y Wu, Yukun Li, Huazuo\nGao, Shirong Ma, et al. 2024. Deepseek-coder-v2:\nBreaking the barrier of closed-source models in code\nintelligence. arXiv preprint arXiv:2406.11931.\nA\nImplementation Details\nIn the paper, we proposed FIRE sampling, which is\nsimilar to CoT-decoding, and removed the need to\ncalculate the confidence score. One of the biggest\nbenefits of simplifying the method is getting an\nextremely easy implementation.\nFor inference,\nwe use vLLM (Kwon et al., 2023) and do a two-\nstage sampling, with the first stage sampling only\none token with high temperature and the second\nstage continuing the sampling with regular sam-\npling. For training, we implement based on Hybrid-\nFlow(Sheng et al., 2024), a newly released RLHF\n6\ncode base, which supports sampling with vLLM.\nThus, we only changed the sampling part of the\ncode in the RLHF framework. As shared in all\nexperiments, the temperature used for the initial\ntoken is set at 30.\nIn our experiment, we enumerate the parame-\nters of top-p sampling, top-k sampling, and min-\np sampling. We list all the parameters we have\ntried in the next section.\nDue to computation\ncosts, some of the models are not enumerated in\nthe same number as others. However, our con-\nclusion that FIRE outperforms regular sampling\nis consistent, as we will show later. Specifically\nfor MBPP(+), and for Qwen-RL, the model after\nour fine-tuning, we test on a single hyperparame-\nter combination of top \u2212p = 0.9, top \u2212k = 16,\nwhich follows the best configuration from previous\ntrials. For Qwen2.5-72b-Instruct, we follow the\nrecommended hyperparameters of top \u2212p = 0.8,\ntop \u2212k = 16. For reinforcement learning prob-\nlems, we use the default parameters in HybridFlow,\nspecifically, top \u2212k = 16, top \u2212p = 1.0. For\ntraining with FIRE sampling, to enable PPO to ac-\ncept the relatively out-of-distribution samples, we\nchange the clipping ratio for PPO from 0.2 to 0.5.\nWe observe that for PPO+FIRE to use the original\nclip rate, it will generally match the original per-\nformance, while pure PPO with a higher clip ratio\nwill lead the training to a failure and converge to a\npass rate close to 0.\nIn the paper, we use three different datasets:\nGSM8K, MATH, and MBPP(+). GSM8K is a\ndataset with 8.5K total instances of math prob-\nlem, of which 7.5K is in the training set and 1.3K\nis in the test set. MATH is a math dataset that\nis slightly more difficult and more comprehen-\nsive than GSM8K, with 7.5K training data and\n5K test data. MBPP is a benchmark consisting\nof around 1,000 crowd-sourced Python program-\nming problems, and MBPP+ is a benchmark that\nenlarges MBPP with some harder problems, reach-\ning around 35K total test problems. While MBPP+\nis still under regular update, we use version 0.1.0\nin our paper.\nFor the final part of the experiment about gen-\neration in the middle sequence, we use a dataset\nthat guarantees a certain number of sentences of\nprefixes to be correct. Here, the sentences are de-\nfined based on \u2019.\u2019 in the answer. This dataset is\ngenerated on the training set of the MATH dataset,\nfor which we first use Qwen2-7b-Instruct to sample\n10 responses for each question. Then, for each re-\nsponse, we enumerate the sentences and sample 20\ntimes using different numbers of sentences as the\nprefix. Thus, we obtained an approximation of the\npoint at which the original samples became wrong.\nSpecifically, if one response is wrong before the\nnumber of lines we enumerate in Table 5, we use\nall the prefix up to the point that is still correct\nfor that response, i.e., if for a specific sample, the\ncorrect sentences are less than 2, 3rd-line pass rate\nwill be calculated in the same way as PRM-line.\nB\nExtra Experiment Results\nWe provide our full inference experiment table in\nTable 6, Table 7, and Table 8. We observe that\namong all hyperparameter combinations, FIRE\nstably outperforms regular sampling, starting at\nPass@10 to Pass@20 and Pass@40. In most set-\ntings, FIRE is superior to regular sampling at\nPass@5, and for certain settings in the MATH\ndataset, FIRE could even show an advantage in\nPass@1.\n7\nDataset\ntop-p\ntop-k\nmin-p\nSampling\nPass@1\nPass@5\nPass@10\nPass@20\nPass@30\nPass@40\nEA@40\nGSM8K\n0.7\n16\n0.01\nReg\n87.19\n93.40\n95.07\n95.53\n95.98\n96.29\n1.96\n0.7\n16\n0.01\nFIRE\n85.75\n94.62\n96.21\n97.42",
    "95.07\n95.53\n95.98\n96.29\n1.96\n0.7\n16\n0.01\nFIRE\n85.75\n94.62\n96.21\n97.42\n97.95\n98.18\n2.58\n0.7\n16\n0.0\nReg\n87.19\n93.40\n95.07\n95.53\n95.98\n96.29\n1.96\n0.7\n16\n0.0\nFIRE\n85.75\n94.62\n96.21\n97.42\n97.95\n98.18\n2.58\n0.7\n32\n0.01\nReg\n87.72\n93.63\n94.77\n95.45\n96.13\n96.36\n1.97\n0.7\n32\n0.01\nFIRE\n85.06\n95.07\n96.74\n97.73\n98.18\n98.26\n2.64\n0.7\n32\n0.0\nReg\n86.58\n93.40\n94.84\n96.06\n96.36\n96.82\n1.97\n0.7\n32\n0.0\nFIRE\n85.67\n94.84\n96.66\n97.57\n98.10\n98.26\n2.63\n0.9\n16\n0.0\nReg\n87.41\n94.16\n95.68\n96.66\n97.35\n97.57\n2.26\n0.9\n16\n0.0\nFIRE\n84.46\n95.83\n96.89\n97.88\n98.26\n98.71\n2.76\n0.9\n32\n0.01\nReg\n86.05\n94.31\n95.83\n96.74\n97.04\n97.27\n2.28\n0.9\n32\n0.01\nFIRE\n84.76\n94.84\n96.59\n97.88\n98.33\n98.41\n2.86\n0.9\n32\n0.0\nReg\n86.43\n94.01\n95.53\n96.66\n97.19\n97.35\n2.29\n0.9\n32\n0.0\nFIRE\n84.76\n94.84\n96.59\n97.88\n98.33\n98.41\n2.86\nMATH\n0.7\n16\n0.01\nReg\n51.04\n64.66\n69.20\n72.36\n73.86\n74.82\n5.08\n0.7\n16\n0.01\nFIRE\n49.68\n64.94\n70.16\n73.52\n75.58\n76.68\n6.33\n0.7\n16\n0.0\nReg\n51.04\n64.56\n68.56\n71.84\n73.44\n74.62\n5.07\n0.7\n16\n0.0\nFIRE\n49.58\n65.48\n70.22\n74.34\n76.02\n77.16\n6.34\n0.7\n32\n0.01\nReg\n51.00\n64.36\n68.42\n71.74\n73.46\n74.38\n5.06\n0.7\n32\n0.01\nFIRE\n49.08\n65.64\n70.22\n73.92\n76.10\n77.06\n6.90\n0.7\n32\n0.0\nReg\n51.00\n64.36\n68.42\n71.74\n73.46\n74.38\n5.06\n0.7\n32\n0.0\nFIRE\n49.08\n65.64\n70.22\n73.92\n76.10\n77.06\n6.90\n0.9\n16\n0.01\nReg\n50.42\n64.98\n69.44\n72.98\n75.00\n76.08\n5.66\n0.9\n16\n0.01\nFIRE\n48.96\n65.34\n70.36\n74.12\n76.16\n77.64\n7.29\n0.9\n16\n0.0\nReg\n50.82\n65.36\n69.62\n73.12\n75.06\n76.16\n5.64\n0.9\n16\n0.0\nFIRE\n48.26\n65.00\n69.98\n74.42\n76.18\n77.64\n7.26\n0.9\n32\n0.01\nReg\n50.00\n65.40\n69.32\n72.88\n74.72\n75.98\n5.65\n0.9\n32\n0.01\nFIRE\n47.66\n65.48\n70.48\n74.58\n76.86\n78.16\n7.90\n0.9\n32\n0.0\nReg\n50.00\n65.40\n69.32\n72.88\n74.72\n75.98\n5.65\n0.9\n32\n0.0\nFIRE\n47.66\n65.48\n70.48\n74.58\n76.86\n78.16\n7.90\nTable 6: Deepseek-coder-v2-Instruct on different datasets with regular sampling (Reg) and FIRE (ours). We show\nthe pass rate with different number of samples (Pass@n), and the effective answers (EA) of the total 40 samples.\nDataset\ntop-p\ntop-k\nmin-p\nSampling\nPass@1\nPass@5\nPass@10\nPass@20\nPass@30\nPass@40\nEA@40\nMATH\n0.7\n16\n0.01\nReg\n15.90\n29.94\n36.40\n42.36\n45.74\n48.14\n8.44\n0.7\n16\n0.01\nFIRE\n17.20\n32.28\n39.22\n45.30\n48.52\n51.18\n9.82\n0.7\n16\n0.0\nReg\n15.90\n29.94\n36.40\n42.36\n45.74\n48.14\n8.44\n0.7\n16\n0.0\nFIRE\n17.20\n32.28\n39.22\n45.30\n48.52\n51.18\n9.82\n0.7\n32\n0.01\nReg\n15.78\n29.84\n36.16\n41.70\n45.20",
    "45.30\n48.52\n51.18\n9.82\n0.7\n32\n0.01\nReg\n15.78\n29.84\n36.16\n41.70\n45.20\n47.70\n8.40\n0.7\n32\n0.01\nFIRE\n16.68\n32.40\n38.80\n45.32\n48.90\n51.26\n9.76\n0.7\n32\n0.0\nReg\n15.78\n29.84\n36.16\n41.70\n45.20\n47.70\n8.40\n0.7\n32\n0.0\nFIRE\n16.68\n32.40\n38.80\n45.32\n48.90\n51.26\n9.76\n0.9\n16\n0.01\nReg\n14.74\n30.46\n37.02\n43.30\n46.98\n49.20\n9.23\n0.9\n16\n0.01\nFIRE\n15.12\n31.48\n38.30\n45.48\n48.90\n51.48\n10.39\n0.9\n16\n0.0\nReg\n14.74\n30.46\n37.02\n43.30\n46.98\n49.20\n9.23\n0.9\n16\n0.0\nFIRE\n15.12\n31.48\n38.30\n45.48\n48.90\n51.48\n10.39\n0.9\n32\n0.01\nReg\n14.58\n30.16\n36.20\n42.28\n45.98\n48.34\n9.17\n0.9\n32\n0.01\nFIRE\n15.04\n31.24\n37.60\n44.26\n47.84\n50.54\n10.34\n0.9\n32\n0.0\nReg\n15.02\n30.06\n36.48\n43.12\n46.52\n49.08\n9.15\n0.9\n32\n0.0\nFIRE\n14.58\n31.40\n38.36\n44.92\n48.48\n51.06\n10.35\nGSM8K\n0.7\n16\n0.01\nReg\n36.54\n66.41\n75.66\n82.34\n84.46\n86.81\n3.86\n0.7\n16\n0.01\nFIRE\n32.45\n66.57\n76.57\n83.32\n85.97\n87.26\n3.97\n0.7\n16\n0.0\nReg\n36.54\n66.41\n75.66\n82.34\n84.46\n86.81\n3.86\n0.7\n16\n0.0\nFIRE\n32.45\n66.57\n76.57\n83.32\n85.97\n87.26\n3.97\n0.7\n32\n0.01\nReg\n36.92\n67.25\n75.66\n82.11\n84.08\n85.52\n3.91\n0.7\n32\n0.01\nFIRE\n31.24\n66.79\n76.27\n82.87\n85.97\n87.57\n4.01\n0.7\n32\n0.0\nReg\n36.92\n67.25\n75.66\n82.11\n84.08\n85.52\n3.91\n0.7\n32\n0.0\nFIRE\n31.24\n66.79\n76.27\n82.87\n85.97\n87.57\n4.01\nTable 7: Gemma-2-2b-it on different datasets with regular sampling (Reg) and FIRE (ours). We show the pass rate\nwith different number of samples (Pass@n), and the effective answers (EA) of the total 40 samples.\n8\nDataset\ntop-p\ntop-k\nmin-p\nSampling\nPass@1\nPass@5\nPass@10\nPass@20\nPass@30\nPass@40\nEA@40\nGSM8K\n0.7\n16\n0.01\nReg\n66.72\n89.23\n92.80\n94.47\n95.07\n95.83\n2.61\n0.7\n16\n0.01\nFIRE\n66.49\n92.87\n94.92\n96.66\n97.19\n97.35\n3.08\n0.7\n16\n0.0\nReg\n66.72\n89.23\n92.80\n94.47\n95.07\n95.83\n2.61\n0.7\n16\n0.0\nFIRE\n66.49\n92.87\n94.92\n96.66\n97.19\n97.35\n3.08\n0.7\n32\n0.0\nReg\n67.02\n89.16\n92.27\n94.31\n95.30\n95.91\n2.58\n0.7\n32\n0.0\nFIRE\n66.34\n92.95\n95.75\n97.19\n97.88\n98.26\n3.17\n0.9\n16\n0.0\nReg\n64.52\n90.83\n94.16\n95.75\n96.89\n97.42\n2.96\n0.9\n16\n0.0\nFIRE\n64.22\n92.27\n95.07\n97.04\n97.65\n97.95\n3.33\nMATH\n0.7\n16\n0.01\nReg\n35.80\n59.40\n66.40\n71.68\n74.22\n75.76\n6.47\n0.7\n16\n0.01\nFIRE\n40.26\n63.74\n69.98\n75.08\n77.42\n78.90\n7.86\n0.7\n16\n0.0\nReg\n35.80\n59.40\n66.40\n71.68\n74.22\n75.76\n6.47\n0.7\n16\n0.0\nFIRE\n40.26\n63.74\n69.98\n75.08\n77.42\n78.90\n7.86\n0.7\n32\n0.01\nReg\n36.42\n59.42\n66.22\n71.10\n73.52\n75.26\n6.47\n0.7\n32\n0.01\nFIRE\n39.52\n63.54\n70.10\n75.06",
    "66.22\n71.10\n73.52\n75.26\n6.47\n0.7\n32\n0.01\nFIRE\n39.52\n63.54\n70.10\n75.06\n77.42\n78.92\n8.11\n0.7\n32\n0.0\nReg\n36.42\n59.42\n66.22\n71.10\n73.52\n75.26\n6.47\n0.7\n32\n0.0\nFIRE\n39.52\n63.54\n70.10\n75.06\n77.42\n78.92\n8.11\n0.9\n16\n0.01\nReg\n35.30\n59.48\n66.16\n71.86\n74.68\n76.60\n7.44\n0.9\n16\n0.01\nFIRE\n38.70\n62.44\n69.50\n74.64\n77.36\n78.86\n8.76\n0.9\n16\n0.0\nReg\n35.30\n59.48\n66.16\n71.86\n74.68\n76.60\n7.44\n0.9\n16\n0.0\nFIRE\n38.70\n62.44\n69.50\n74.64\n77.36\n78.86\n8.76\n0.9\n32\n0.01\nReg\n35.82\n59.84\n66.70\n72.02\n74.64\n76.38\n7.40\n0.9\n32\n0.01\nFIRE\n37.44\n62.72\n69.50\n75.08\n77.50\n79.08\n9.03\n0.9\n32\n0.0\nReg\n35.14\n59.84\n66.80\n72.34\n74.72\n76.40\n7.43\n0.9\n32\n0.0\nFIRE\n36.70\n62.52\n69.12\n74.54\n77.10\n79.04\n9.04\nTable 8: Qwen2-7B-Instruct on different datasets with regular sampling (Reg) and FIRE (ours). We show the pass\nrate with different number of samples (Pass@n), and the effective answers (EA) of the total 40 samples.\n9"
]