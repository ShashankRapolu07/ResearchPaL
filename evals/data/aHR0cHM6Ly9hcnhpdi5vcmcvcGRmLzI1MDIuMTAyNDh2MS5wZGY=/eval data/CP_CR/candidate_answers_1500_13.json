[
    {
        "question_id": "2502.10248v1_0",
        "answer": "The paper defines two distinct levels of video foundation models, each characterized by its capabilities and complexity:\n\n### Level-1: Translational Video Foundation Model\n\n*   This level focuses on cross-modal translation, enabling the generation of videos from various inputs such as text, visual data, or multimodal contexts.\n*   Models at this level function as sophisticated translation systems, converting input from one modality (e.g., text) into another (video).\n*   Current diffusion-based text-to-video models, including Step-Video-T2V, fall into this category. They excel at creating high-quality videos from text prompts, thus lowering the barrier for video content creation.\n\n### Level-2: Predictable Video Foundation Model\n\n*   This level involves prediction capabilities, similar to those seen in large language models (LLMs), allowing the model to forecast future events based on text, visual, or multimodal contexts.\n*   Models at this level act as prediction systems, handling advanced tasks such as reasoning with multimodal data or simulating real-world scenarios.\n*   Achieving Level-2 requires models to explicitly model the underlying causal relationships within videos, a capability that current Level-1 models often lack.\n\n### Key Differences\n\nThe primary distinction lies in the ability to understand and predict complex dynamics and causal relationships within videos:\n\n*   **Level-1 models** primarily focus on translating inputs into video outputs without necessarily understanding the underlying dynamics or causal relationships.\n*   **Level-2 models**, on the other hand, aim to predict future events and understand complex interactions within the video, requiring a deeper understanding of the underlying causal relationships and physical laws."
    },
    {
        "question_id": "2502.10248v1_1",
        "answer": "Step-Video-T2V employs a Video-VAE with specific architectural innovations and a multi-stage training process to achieve high spatial and temporal compression while maintaining video reconstruction quality. Here's a breakdown:\n\n### Architecture\n***\nThe Video-VAE uses a dual-path architecture in the later stages of the encoder and the early stages of the decoder. This design achieves 8x16x16 downscaling through 3D convolutions and optimized pixel un-shuffling operations.\n\n*   **Dual-Path Latent Fusion:** This is a key component. It maintains high-frequency details through convolutional processing and preserves low-frequency structure via channel averaging.\n    *   **Conv Path:** Combines causal 3D convolutions with pixel un-shuffling. The equation for this path is:\n\n        $H_{conv} = U^{(3)}_s(C3D(X))$\n\n        where $U^{(3)}_s$ is the 3D pixel un-shuffle operation, $C3D$ denotes the causal 3D convolution, and $X$ is the input video tensor.\n    *   **Shortcut Path:** Preserves structural semantics through grouped channel averaging. The equation for this path is:\n\n        $H_{avg} = \frac{1}{s^3} \\sum_{k=0}^{s^3 - 1} U^{(3)}_s (X) [..., kC_z : (k+1)C_z]$\n\n        where $C_z$ is the latent dimension of the next stage.\n    *   **Fusion:** The output of both paths are combined through residual summation:\n\n        $Z = H_{conv} \\oplus H_{avg}$\n*   **Causal 3D Convolutional Modules:** The encoder's early stage consists of three stages, each featuring two Causal Res3DBlocks and corresponding downsample layers. A MidBlock combines convolutional layers with attention mechanisms. Temporal causality is implemented using:\n\n    $C3D(X)_t = \begin{cases}\n    Conv3D([0, ..., X_t], \\Theta) & t = 0 \\\n    Conv3D([X_{t-k}, ..., X_t], \\Theta) & t > 0\n    \\end{cases}$\n\n    where $k$ is the temporal kernel size, ensuring frame $t$ only depends on previous frames.\n*   **Decoder Architecture:** The early stage of the decoder consists of two symmetric Dual Path architectures. The 3D pixel un-shuffle operation $U$ is replaced by a 3D pixel shuffle operator $P$, and the grouped channel averaging path is replaced by a grouped channel repeating operation. Spatial group normalization is used to prevent temporal flickering.\n\n### Training Process\n***\nThe VAE training process is designed in multiple stages:\n\n1.  **Initial Training:** A VAE with a 4x8x8 compression ratio is trained without a dual-path structure. This initial training is conducted jointly on images and videos of varying frame counts.\n2.  **Dual-Path Incorporation:** The model is enhanced by incorporating two dual-path modules in both the encoder and decoder. The dual-path modules, the mid-block, and the ResNet backbone are gradually unfrozen.\n3.  **Loss Functions:** A combination of **L1 reconstruction loss**, **Video-LPIPS**, and **KL-divergence** constrain is used to guide the model. **GAN loss** is introduced to further refine the model\u2019s performance after these losses have converged.\n\n### Key Factors for Maintaining Reconstruction Quality\n\n*   **Dual-Path Architecture:** By maintaining both high-frequency details and low-frequency structure, the dual-path architecture avoids blurring artifacts.\n*   **Multi-Stage Training:** This approach allows the model to learn low-level representations before gradually increasing the compression ratio and refining the architecture.\n*   **Loss Function Combination:** The combination of L1 reconstruction loss, Video-LPIPS, KL-divergence, and GAN loss ensures that the model balances reconstruction accuracy, perceptual quality, and statistical similarity to the original data distribution.\n*   **Spatial Group Normalization:** Replacing group normalization with spatial group normalization in the ResNet backbone prevents temporal flickering between different chunks."
    },
    {
        "question_id": "2502.10248v1_2",
        "answer": "Direct Preference Optimization (DPO) is a technique used to align a model's behavior with human preferences. In the context of Step-Video-T2V, it is employed to enhance the visual quality of the generated videos by incorporating human feedback.\n\n***\n\nHere's a breakdown of how DPO works and its impact on video quality:\n\n### Core Idea\n\nDPO aims to train a model to generate outputs that are preferred by humans over other possible outputs, given the same input conditions. It achieves this by adjusting the model's policy (i.e., its generation strategy) to favor the creation of preferred data while discouraging the generation of non-preferred data.\n\n### Implementation in Step-Video-T2V\n\n1.  **Data Collection**:\n\n    *   A diverse set of prompts is created, either by randomly selecting from the training data or by having human annotators synthesize prompts based on real-world user interaction patterns.\n    *   For each prompt, the Step-Video-T2V model generates multiple videos using different random seeds.\n    *   Human annotators then rate these generated videos, indicating their preferences. This results in pairs of preferred and non-preferred videos for each prompt.\n2.  **Training Objective**:\n\n    *   DPO uses a specific loss function to optimize the model. The loss function encourages the model to increase the likelihood of generating preferred samples and decrease the likelihood of generating non-preferred samples. A reference policy (i.e., a reference model) is introduced to stabilize training and prevent the current policy from deviating too far.\n    *   The policy objective is formulated as follows:\n\n        $L_{DPO} = -E_{(y, x_w, x_l) \\sim D} \\left[ \\log \\sigma \\left( \beta \\left[ \\log \frac{\\pi_\theta(x_w|y)}{\\pi_{ref}(x_w|y)} - \\log \frac{\\pi_\theta(x_l|y)}{\\pi_{ref}(x_l|y)} \right] \right) \right]$\n\n        where:\n\n        *   $\\pi_\theta$ is the current policy (the model being trained).\n        *   $\\pi_{ref}$ is the reference policy (a fixed reference model).\n        *   $x_w$ is the preferred sample.\n        *   $x_l$ is the non-preferred sample.\n        *   $y$ is the condition (e.g., a text prompt).\n        *   $\beta$ is a parameter controlling the strength of the preference optimization.\n3.  **Training Process**:\n\n    *   During training, the model selects a prompt and its corresponding positive (preferred) and negative (non-preferred) sample pairs.\n    *   The samples are generated by the model itself, ensuring smooth updates and improving overall training stability.\n    *   The initial noise and timestep are fixed to align the positive and negative samples, contributing to a more stable training process.\n\n### Benefits and Improvements\n\n1.  **Enhanced Visual Quality**:\n\n    *   By incorporating human feedback, DPO helps to generate videos that are more visually appealing and realistic. It reduces artifacts and inconsistencies, leading to smoother and more plausible video outputs.\n2.  **Better Alignment with User Prompts**:\n\n    *   DPO ensures that the generated videos are more relevant and accurate with respect to the given prompts. The model learns to interpret and follow instructions more effectively, resulting in videos that better reflect user intent.\n3.  **Efficiency and Stability**:\n\n    *   DPO is designed to be intuitive and easy to implement, making it a practical choice for incorporating human feedback.\n    *   The training process is stabilized by using samples generated by the model itself and by aligning positive and negative samples, which contributes to more consistent and reliable updates.\n4.  **Faster Convergence**:\n\n    *   The paper mentions adjustments to the $\beta$ parameter and learning rate to achieve faster convergence compared to other methods like DiffusionDPO.\n\n### Addressing Limitations\n\nThe paper also discusses some limitations and potential improvements for future work:\n\n1.  **Underutilized Dynamic Conditions**:\n\n    *   The trajectory from initial noise to timestep-specific latents acts as implicit dynamic conditions beyond text prompts, which are currently underutilized due to computational limitations.\n2.  **Tradeoff in Feedback Precision**:\n\n    *   There is a tradeoff between sparse and imprecise feedback, especially in video diffusion models where only a few pixels may be problematic in videos with millions of pixels.\n3.  **Optimization Efficiency**:\n\n    *   Diffusion models rely on regression, which may be less efficient for preference optimization compared to token-level softmax used in LLMs.\n\n### Summary\n\nDPO significantly improves the visual quality and alignment of generated videos in Step-Video-T2V by incorporating human feedback into the training process. It adjusts the model's policy to favor preferred outputs, leading to more realistic, coherent, and user-aligned video generation."
    },
    {
        "question_id": "2502.10248v1_3",
        "answer": "Here are the key advantages of using a **diffusion transformer (DiT)** with **3D full attention** in Step-Video-T2V, structured for clarity:\n\n***\n\n### Superior Modeling of Spatial and Temporal Information\n\n*   **3D full attention** captures both spatial and temporal information in a unified attention process. This approach has a higher theoretical upper bound for modeling video data compared to spatial-temporal attention mechanisms.\n*   The unified attention process allows the model to generate videos with smooth and consistent motion.\n\n***\n\n### Architectural Foundation and Performance\n\n*   Step-Video-T2V builds upon the **DiT** architecture, leveraging its inherent strengths for image and video generation.\n*   Experiments show that a **3D full attention**-based model outperforms spatial-temporal attention models, particularly in generating videos with high motion dynamics.\n\n***\n\n### Text Prompt Integration\n\n*   A cross-attention layer is introduced between the self-attention and feed-forward network (**FFN**) in each transformer block to incorporate text prompts.\n*   The model attends to textual information while processing visual features, enabling the generation of videos conditioned on the input prompt.\n*   The combined embeddings from two distinct bilingual text encoders (**Hunyuan-CLIP** and **Step-LLM**) are injected into the cross-attention layer.\n\n***\n\n### Adaptive Layer Normalization\n\n*   The model uses Adaptive Layer Normalization (**AdaLN**) with optimized computation, removing class labels from **AdaLN** since the text-to-video task does not require them.\n*   The **AdaLN-Single** structure reduces the computational overhead of traditional **AdaLN** operations, improving overall model efficiency.\n\n***\n\n### Positional Encoding\n\n*   **RoPE-3D**, an extension of Rotation-based Positional Encoding (**RoPE**), handles video data by accounting for temporal (frame) and spatial (height and width) dimensions.\n*   **RoPE-3D** enables the model to process videos with different frame counts and resolutions, improving generalization across diverse video data.\n\n***\n\n### Stabilized Self-Attention\n\n*   Query-Key Normalization (**QK-Norm**) stabilizes the self-attention mechanism by normalizing the dot product between the query (**Q**) and key (**K**) vectors.\n*   **QK-Norm** addresses numerical instability, ensures stable attention during training, accelerates convergence, and improves efficiency."
    },
    {
        "question_id": "2502.10248v1_4",
        "answer": "Step-Video-T2V employs a dual approach to process bilingual text prompts, utilizing two distinct text encoders: Hunyuan-CLIP and Step-LLM.\n\n***\n\n### Hunyuan-CLIP\n*   It is a bidirectional text encoder from an open-source bilingual CLIP model.\n*   It excels in producing text representations that align well with the visual space due to the training mechanism of the CLIP model.\n*   It is limited to processing a maximum input length of 77 tokens, posing challenges with longer user prompts.\n\n***\n\n### Step-LLM\n*   It is an in-house, unidirectional bilingual text encoder pre-trained using the next-token prediction task.\n*   It incorporates a redesigned Alibi-Positional Embedding to improve both efficiency and accuracy in sequence processing.\n*   It has no input length restriction, making it effective for handling lengthy and complex text sequences.\n\n***\n\n### Advantages of Using Two Separate Text Encoders\nThe combination of these two text encoders allows Step-Video-T2V to handle user prompts of varying lengths and complexities. Hunyuan-CLIP ensures the generated text representations are well-aligned with the visual space, while Step-LLM handles longer and more complex text sequences without input length restrictions. This dual approach results in robust text representations that effectively guide the model in the latent space, improving the overall quality and coherence of the generated videos."
    },
    {
        "question_id": "2502.10248v1_5",
        "answer": "Current diffusion-based text-to-video models face several significant challenges, which Step-Video-T2V attempts to address through its architecture, training strategies, and optimization techniques. Here's a breakdown of these challenges and how Step-Video-T2V tackles them:\n\n### Challenges and Solutions\n\n*   **High-Quality Labeled Data:**\n\n    *   **Challenge:** Existing video captioning models often struggle with hallucination issues, making it difficult to obtain accurate captions. Human annotation is costly and hard to scale.\n    *   **Step-Video-T2V's Approach:** The paper utilizes both automated and manual filtering techniques to curate a high-quality video dataset. This involves:\n        *   Filtering by video assessment scores and heuristic rules to improve overall quality.\n        *   Filtering by video categories using \"Distance to Centroid\" values to maintain diversity.\n        *   Labeling by human annotators to refine captions for accuracy, including details about camera movements, subjects, actions, backgrounds, and lighting.\n*   **Instruction Following:**\n\n    *   **Challenge:** Generating videos that involve complex action sequences or incorporate multiple concepts with low occurrence in the training data remains difficult.\n    *   **Step-Video-T2V's Approach:** The model uses two bilingual text encoders (Hunyuan-CLIP and Step-LLM) to process user text prompts. By combining these encoders, Step-Video-T2V can handle user prompts of varying lengths, generating robust text representations that effectively guide the model in the latent space.\n*   **Adherence to the Laws of Physics:**\n\n    *   **Challenge:** Current models struggle to accurately simulate the real world and generate videos that adhere to the laws of physics.\n    *   **Step-Video-T2V's Approach:** The paper plans to develop more advanced model paradigms in future work, such as combining autoregressive and diffusion models within a unified framework, to better adhere to the laws of physics and more accurately simulate realistic interactions.\n*   **Computational Cost:**\n\n    *   **Challenge:** Training and generating long-duration, high-resolution videos still face significant computational cost hurdles.\n    *   **Step-Video-T2V's Approach:** A specially designed deep compression Variational Auto-encoder (**VAE**) achieves 16x16 spatial and 8x temporal compression ratios, significantly reducing the computational complexity of large-scale video generation training.\n\n***\n\n### Additional Strategies Employed by Step-Video-T2V\n\n*   **Cascaded Training Pipeline:**\n\n    *   The model employs a cascaded training pipeline, including text-to-image pre-training, text-to-video pre-training, supervised fine-tuning (**SFT**), and direct preference optimization (**DPO**), to accelerate model convergence and fully leverage video datasets of varying quality.\n*   **Video-Based DPO:**\n\n    *   A video-based **DPO** approach is applied to reduce artifacts and improve the visual quality of the generated videos.\n\n***\n\n### Key Components of Step-Video-T2V\n\n*   **Video-VAE:**\n\n    *   A high-compression **Video-VAE** achieves 16x16 spatial and 8x temporal compression ratios, reducing computational complexity.\n*   **Bilingual Text Encoders:**\n\n    *   Two bilingual text encoders enable Step-Video-T2V to directly understand Chinese or English prompts.\n*   **DiT with 3D Full Attention:**\n\n    *   A **DiT** with 3D full attention is trained using Flow Matching and is employed to denoise input noise into latent frames.\n*   **Step-Video-T2V-Eval:**\n\n    *   A new benchmark dataset called **Step-Video-T2V-Eval** is created for text-to-video generation, which includes 128 diverse prompts across 11 categories, alongside video generation results from several top text-to-video open-source and commercial engines for comparison.\n\n***\n\n### Insights Gained\n\n*   Text-to-image pre-training is essential for the video generation model to acquire rich visual knowledge.\n*   Text-to-video pre-training at low resolution is critical for the model to learn motion dynamics.\n*   Using high-quality videos with accurate captions and desired styles in **SFT** is crucial to the stability of the model and the style of the generated videos.\n*   Video-based **DPO** can further enhance the visual quality by reducing artifacts, ensuring smoother and more realistic video outputs."
    },
    {
        "question_id": "2502.10248v1_6",
        "answer": "Step-Video-T2V employs **Flow Matching** as a training objective to generate videos. Here's how it works and why it's beneficial:\n\n***\n\n### Flow Matching Training Process\n\n1.  **Noise Sampling**: The process starts by sampling Gaussian noise, denoted as $X_0$, from a standard normal distribution $N(0, 1)$.\n\n2.  **Timestep Sampling**: A random timestep $t$ is sampled from a uniform distribution between 0 and 1 (i.e., $t \u2208 [0, 1]$).\n\n3.  **Linear Interpolation**: Model input $X_t$ is constructed through linear interpolation between the noise $X_0$ and the target sample $X_1$ (noise-free input):\n\n    $X_t = (1 - t) \\cdot X_0 + t \\cdot X_1$\n\n4.  **Velocity Calculation**: Ground truth velocity $V_t$ is calculated, representing the rate of change of $X_t$ with respect to the timestep $t$:\n\n    $V_t = \frac{dX_t}{dt} = X_1 - X_0$\n\n5.  **Model Training**: The model is trained to predict the velocity $u(X_t, y, t; \u03b8)$ at timestep $t$, given input $X_t$, optional conditioning input $y$ (e.g., text prompt), and model parameters $\u03b8$. The training minimizes the Mean Squared Error (MSE) loss between the predicted velocity and the true velocity:\n\n    $loss = E_{t, X_0, X_1, y} [\\|u(X_t, y, t; \u03b8) - V_t\\|^2]$\n\n    The expectation is taken over all training samples.\n\n***\n\n### Inference Process\n\n1.  **Initial Noise**: Inference starts by sampling random noise $X_0 \\sim N(0, 1)$.\n2.  **Iterative Denoising**: The goal is to recover the denoised sample $X_1$ by iteratively refining the noise through an ODE-based method. A sequence of timesteps $\\{t_0, t_1, ..., t_n\\}$ is defined where $t_0 = 0$, $t_n = 1$, and $t_0 < t_1 < ... < t_n$.\n3.  **Denoising**: The denoised sample $X_1$ is expressed as:\n\n    $X_1 = \\sum_{i=0}^{n-1} u(X_{t_i}, y, t_i; \u03b8) \\cdot (t_{i+1} - t_i)$\n\n    Here, $u(X_{t_i}, y, t_i; \u03b8)$ represents the predicted velocity at timestep $t_i$, given the noisy sample $X_{t_i}$ and conditioning input $y$.\n\n***\n\n### Benefits of Flow Matching for Video Generation\n\n*   **Stable Training**: Flow Matching provides a more stable training process compared to other diffusion-based methods because it directly models the velocity field between noise and data, making the training objective clearer and more direct.\n*   **High-Quality Samples**: By learning to predict the instantaneous rate of change, the model can generate high-quality video frames with improved temporal consistency.\n*   **Flexibility**: Flow Matching does not impose strict requirements on the input data distribution, allowing the model to be more adaptable to different video datasets and resolutions.\n*   **Efficient Inference**: The ODE-based inference method allows for efficient denoising, reducing the number of steps needed to generate high-quality videos."
    },
    {
        "question_id": "2502.10248v1_7",
        "answer": "The Step-Video-T2V employs a hierarchical data filtering approach to refine the video data used for pre-training and post-training. This process involves applying a series of filters with progressively stricter thresholds, creating multiple subsets of data with increasing quality.\n\n***\n\n### Data Filtering Stages\n\n1.  **Pre-training Subsets**: For the T2VI (Text-to-Video/Image) pre-training phase, the filtering process creates six subsets of data.\n2.  **Final SFT Dataset**: The Supervised Fine-Tuning (SFT) dataset is constructed through manual filtering.\n\n***\n\n### Filtering Techniques\n\n*   **Video Assessment Scores**: Automated filtering using video assessment scores and heuristic rules reduces the dataset to a subset of 30M videos, enhancing the dataset's overall quality.\n*   **Video Categories**: Videos are clustered, and those that are distant from their cluster's centroid are removed based on a predefined threshold.\n*   **Human Annotation**: Human evaluators assess each video for clarity, aesthetics, motion, scene transitions, and the absence of watermarks or subtitles. Captions are manually refined for accuracy, including details about camera movements, subjects, actions, backgrounds, and lighting.\n\n***\n\n### Importance of Hierarchical Data Filtering\n\n*   **Improved Data Quality**: Filtering enhances the quality of the training data, leading to better model performance.\n*   **Reduced Artifacts**: High-quality data in SFT helps mitigate issues like artifacts and stylistic inconsistencies in generated videos.\n*   **Enhanced Visual Quality**: High-quality datasets, especially in post-training, improve the visual quality of the generated videos.\n*   **Better Instruction Following**: Accurate captions and high-quality videos improve the model's ability to follow text prompts.\n*   **Efficient Training**: By focusing on high-quality data, the model can generalize better across different prompts and styles.\n*   **Emulating Human Cognition**: Adjusting the training data to reflect human preferences results in a significant reduction in training loss, suggesting the model's learning process emulates human cognitive patterns."
    },
    {
        "question_id": "2502.10248v1_8",
        "answer": "Step-Video-T2V is evaluated against both closed-source commercial engines and open-source models, with its strengths and weaknesses varying across different aspects of video generation.\n\n***\n\n### Performance Against Commercial Engines\n\n*   **General Text Prompts**: Step-Video-T2V delivers comparable performance to commercial video generation engines for general text prompts.\n*   **Specific Domains**: It surpasses commercial engines in specific domains, such as generating videos with high motion dynamics or text content.\n*   **Motion Dynamics**: Step-Video-T2V demonstrates the strongest motion dynamics modeling and generation capabilities among all commercial engines.\n\n***\n\n### Key Factors Influencing Comparisons\n\n*   **Aesthetic Appeal**: Commercial models like T2VTopA and T2VTopB often have a higher aesthetic appeal due to higher resolutions (720P to 1080P) and high-quality aesthetic data used during post-training stages. Step-Video-T2V generates videos at 540P.\n*   **Instruction Following**: T2VTopA shows better instruction-following capabilities, contributing to superior performance in categories like combined concepts, surreal, and cinematography. This is attributed to better video captioning models and greater human effort in labeling post-training data.\n*   **Training Data**: Commercial engines use significantly more high-quality data in the post-training phase compared to Step-Video-T2V.\n*   **Video Length**: Step-Video-T2V generates videos up to 204 frames, nearly twice the length of T2VTopA and T2VTopB, making its training more challenging.\n\n***\n\n### Specific Comparisons\n\n*   **T2VTopA and T2VTopB**:\n    *   Overall ranking: T2VTopA > Step-Video-T2V > T2VTopB.\n    *   Step-Video-T2V consistently outperforms T2VTopA and T2VTopB in the Sports category.\n    *   T2VTopA and T2VTopB are rated as having higher aesthetic appeal by annotators due to higher resolutions and better aesthetic quality.\n*   **Movie Gen Video**:\n    *   Achieves comparable performance to Step-Video-T2V.\n    *   Movie Gen Video was trained on 73.8M videos during its high-resolution pre-training phase, while Step-Video-T2V was trained on only 27.3M videos.\n    *   Movie Gen Video can generate 720P videos, which are visually more appealing than the 540P resolution produced by Step-Video-T2V.\n*   **HunyuanVideo**:\n    *   Step-Video-T2V achieves significant improvements across all categories compared to HunyuanVideo.\n    *   Video-VAE achieves compression ratios of 16x16 spatial and 8x temporal, compared to HunyuanVideo\u2019s 8x8 spatial and 4x temporal compression.\n\n***\n\n### Limitations and Future Improvements\n\n*   **Training**: Insufficient training in the final stage of pre-training with 540P videos, having only seen 25.3M samples.\n*   **Data Quality**: Lacks enough high-quality labeled data at this stage to effectively refine the visual style and quality of the generated results.\n*   **Resolution**: Lower resolution (540P) compared to some commercial models affects aesthetic appeal.\n*   **Text Generation**: The accuracy of text generation remains far from ideal, especially for Chinese characters."
    },
    {
        "question_id": "2502.10248v1_9",
        "answer": "Human feedback plays a crucial role in refining **Step-Video-T2V's** video generation by improving visual quality and ensuring better alignment with user prompts. This is achieved through a pipeline that incorporates **Direct Preference Optimization (DPO)**.\n\n***\n\nHere\u2019s how human feedback is implemented in the training pipeline:\n\n1.  **Data Collection**:\n    *   A diverse set of prompts is constructed, including random prompts from the training data and prompts synthesized by human annotators based on real-world user interaction patterns.\n    *   For each prompt, **Step-Video-T2V** generates multiple videos using different seeds.\n    *   Human annotators then rate the preference of these samples, creating a set of preference and non-preference data.\n\n2.  **Preference Optimization with DPO**:\n    *   **DPO** is used to adjust the model to align with the generation of preferred data while avoiding non-preferred data.\n    *   A reference policy (reference model) is introduced to prevent the current policy from deviating too far.\n    *   The policy objective is formulated as:\n\n        $L_{DPO} = -E_{(y, x_w, x_l) \\sim D} \\left[ \\log \\sigma \\left( \beta \\left( \\log \frac{\\pi_\theta(x_w|y)}{\\pi_{ref}(x_w|y)} - \\log \frac{\\pi_\theta(x_l|y)}{\\pi_{ref}(x_l|y)} \right) \right) \right]$\n\n        where:\n\n        *   $\\pi_\theta$ and $\\pi_{ref}$ refer to the current policy and reference policy, respectively.\n        *   $x_w$ and $x_l$ are the preferred and non-preferred samples, respectively.\n        *   $y$ denotes the condition (prompt).\n\n3.  **Training Process**:\n\n    *   At each step, a prompt and its corresponding positive and negative sample pairs are selected.\n    *   Each sample is generated by the model itself to ensure smooth updates and improve training stability.\n    *   The positive and negative samples are aligned by fixing the initial noise and timestep, contributing to a more stable training process.\n    *   The training objective is based on the **DiffusionDPO** method but modified for the **Flow Matching** framework.\n\n4.  **Addressing Limitations**:\n\n    *   To address the saturation of improvements when the model easily distinguishes between positive and negative samples, a reward model is trained using human-annotated feedback data.\n    *   This reward model dynamically evaluates the quality of newly generated samples during training.\n    *   The reward model is periodically fine-tuned with newly annotated human feedback to maintain alignment with the evolving policy.\n    *   By integrating the reward model, the training data is scored and ranked on-the-fly (on-policy), improving data efficiency.\n\n***"
    }
]