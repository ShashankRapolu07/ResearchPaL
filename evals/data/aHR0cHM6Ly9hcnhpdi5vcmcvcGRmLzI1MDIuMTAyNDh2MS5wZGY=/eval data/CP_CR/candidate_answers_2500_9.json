[
    {
        "question_id": "2502.10248v1_0",
        "answer": "The paper outlines two distinct levels of video foundation models, each characterized by its capabilities and potential applications:\n\n### Level-1: Translational Video Foundation Model\n\n*   A model at this level operates as a cross-modal translation system.\n*   It focuses on generating videos from various input modalities such as text, visual data, or a combination of both (multimodal context).\n*   Current diffusion-based text-to-video models like Step-Video-T2V fall into this category.\n\n### Level-2: Predictable Video Foundation Model\n\n*   This level represents a more advanced prediction system, analogous to large language models (LLMs).\n*   It can forecast future events based on text, visual, or multimodal context.\n*   It is designed to handle more complex tasks, including reasoning with multimodal data and simulating real-world scenarios.\n\n***\n\n### Key Differences\n\nThe primary distinction lies in their core functionality:\n\n*   **Level-1 models** translate inputs into video, focusing on cross-modal generation.\n*   **Level-2 models** predict future events and perform reasoning, going beyond mere translation to simulate and understand dynamic processes.\n\nIn essence, Level-1 models are about *generating* videos from existing data, while Level-2 models aim to *understand* and *predict* video content based on context."
    },
    {
        "question_id": "2502.10248v1_1",
        "answer": "Step-Video-T2V's **Video-VAE** achieves high spatial and temporal compression while maintaining video reconstruction quality through a combination of architectural innovations and a multi-stage training process. Here's a breakdown:\n\n***\n\n### 1. Architecture\n\n*   **Dual-Path Latent Fusion**: The **Video-VAE** employs a dual-path architecture in the later stages of the encoder and the early stages of the decoder. This architecture allows the model to maintain high-frequency details through convolutional processing while preserving low-frequency structure via channel averaging.\n    *   **Conv Path**: Combines causal 3D convolutions with pixel unshuffling. This can be represented mathematically as:\n\n        $H_{conv} = U^{(3)}_s(C3D(X))$\n\n        where $U^{(3)}_s$ is the 3D pixel unshuffle operation, $C3D$ is the causal 3D convolution, and $X$ is the input video tensor.\n    *   **Shortcut Path**: Preserves structural semantics through grouped channel averaging. This can be represented as:\n\n        $H_{avg} = \frac{1}{s^3} \\sum_{k=0}^{s^3-1} U^{(3)}_s (X) [..., kC_z:(k+1)C_z]$\n\n        where $C_z$ is the latent dimension of the next stage.\n    *   **Fusion**: The output of the two paths are combined through residual summation:\n\n        $Z = H_{conv} \\oplus H_{avg}$\n\n*   **Causal 3D Convolutional Modules**: The encoder uses causal **Res3DBlock** and downsample layers, followed by a **MidBlock** that combines convolutional layers with attention mechanisms. Temporal causality is implemented using:\n\n    $C3D(X)_t = \begin{cases} Conv3D([0, ..., X_t], \\Theta) & t = 0 \\ Conv3D([X_{t-k}, ..., X_t], \\Theta) & t > 0 \\end{cases}$\n\n    where $k$ is the temporal kernel size.\n*   **Pixel Shuffling**: The decoder uses 3D pixel shuffle operations to unfold compressed information into spatial-temporal dimensions.\n*   **Spatial Group Normalization**: The group normalization layers are replaced with spatial group normalization to avoid temporal flickering between different chunks.\n\n***\n\n### 2. Training Process\n\n*   **Multi-Stage Training**: The **VAE** training is divided into multiple stages.\n    *   **Stage 1**: A **VAE** with a 4x8x8 compression ratio is trained jointly on images and videos without a dual-path structure.\n    *   **Stage 2**: Two dual-path modules are incorporated in both the encoder and decoder, replacing the latter part after the **mid-block**. The dual-path modules, the **mid-block**, and the **ResNet** backbone are gradually unfrozen during this phase.\n*   **Loss Functions**: A combination of **L1 reconstruction loss**, **Video-LPIPS**, and **KL-divergence** constraint is used to guide the model. **GAN loss** is introduced to further refine the model\u2019s performance after the initial losses have converged.\n\n***\n\n### 3. Key Benefits\n\n*   **High Compression Ratios**: Achieves 16x16 spatial and 8x temporal compression ratios.\n*   **Efficient Parameter Usage**: The dual-path architecture allows the network to use its parameters more efficiently, reducing blurring artifacts.\n*   **Joint Image and Video Modeling**: The unified structure is adept at handling both image and video data.\n*   **State-of-the-Art Reconstruction Quality**: Maintains high **SSIM** and **PSNR** values, and low **rFVD** scores, even with higher compression ratios."
    },
    {
        "question_id": "2502.10248v1_2",
        "answer": "Direct Preference Optimization (DPO) is a method used to improve the visual quality of videos generated by Step-Video-T2V. Instead of relying on traditional reinforcement learning techniques, DPO directly optimizes the model based on human feedback. Here's a breakdown of how it works and its impact:\n\n### Core Idea of DPO\n\nDPO aims to align the model's behavior with human preferences by adjusting the model to favor generations that humans prefer, while discouraging generations that humans dislike. This is achieved by using a dataset of preference pairs, where each pair consists of two videos generated from the same prompt, with one video being preferred over the other.\n\n### How DPO Works in Step-Video-T2V\n\n1.  **Data Collection**:\n    *   A diverse set of prompts is created.\n    *   For each prompt, multiple videos are generated using the Step-Video-T2V model.\n    *   Human annotators evaluate these videos and indicate their preferences, creating pairs of preferred and non-preferred videos.\n\n2.  **Objective Function**:\n    *   DPO uses an objective function that directly compares the log probabilities of the preferred and non-preferred samples.\n    *   The goal is to maximize the probability of the preferred samples while minimizing the probability of the non-preferred samples, relative to a reference policy.\n    *   The DPO loss function can be represented as:\n\n        $L_{DPO} = -E_{(y, x_w, x_l) \\sim D} \\left[ \\log \\sigma \\left( \beta \\left( \\log \frac{\\pi_\theta(x_w|y)}{\\pi_{ref}(x_w|y)} - \\log \frac{\\pi_\theta(x_l|y)}{\\pi_{ref}(x_l|y)} \right) \right) \right]$\n\n        Where:\n\n        *   $y$ is the condition (prompt).\n        *   $x_w$ is the preferred sample.\n        *   $x_l$ is the non-preferred sample.\n        *   $\\pi_\theta$ is the current policy (model).\n        *   $\\pi_{ref}$ is the reference policy (model).\n        *   $\beta$ is a hyperparameter controlling the strength of the preference.\n        *   $\\sigma$ is the sigmoid function.\n\n3.  **Training Process**:\n    *   The model is trained using the DPO loss, adjusting its parameters to better align with human preferences.\n    *   A reference policy (a previous version of the model) is used to stabilize training and prevent the model from deviating too far from reasonable outputs.\n    *   The training process involves selecting a prompt and its corresponding positive and negative sample pairs.\n    *   The samples are generated by the model itself, ensuring smooth updates and improving overall training stability.\n    *   The initial noise and timestep are fixed to align the positive and negative samples, contributing to a more stable training process.\n\n### Impact on Visual Quality\n\nBy incorporating human feedback through DPO, Step-Video-T2V achieves several improvements in visual quality:\n\n*   **Reduced Artifacts**: DPO helps to reduce visual artifacts and distortions in the generated videos, making them more visually appealing.\n*   **Smoother Motion**: The generated videos exhibit smoother and more realistic motion, enhancing the overall quality of the video.\n*   **Better Alignment with User Prompts**: DPO enhances the alignment with the given prompts, resulting in more accurate and relevant video generation.\n*   **Improved Realism and Consistency**: Videos generated with DPO show greater realism and improved consistency, leading to more plausible and coherent content.\n\n***\n\nIn summary, DPO is a crucial component in Step-Video-T2V that leverages human feedback to directly optimize the model, resulting in higher-quality, more visually appealing, and more accurate video generations."
    },
    {
        "question_id": "2502.10248v1_3",
        "answer": "The Step-Video-T2V model leverages a Diffusion Transformer (DiT) architecture with 3D full attention, offering several key advantages:\n\n*   **Effective Spatial and Temporal Modeling**: 3D full attention allows the model to capture both spatial and temporal information within videos in a unified attention process.\n\n*   **Smooth and Consistent Motion**: The paper observes that 3D full attention leads to superior generation of videos with smooth and consistent motion, which is crucial for realistic video generation.\n\n*   **Handling Varying Lengths and Resolutions**: By using RoPE-3D (Rotation-based Positional Encoding), the model can process videos with different frame counts and resolutions without being restricted by fixed positional encoding lengths. This improves generalization across diverse video data.\n\n*   **Stable Training**: The model uses Query-Key Normalization (**QK-Norm**) to stabilize the self-attention mechanism, addressing numerical instability and improving training efficiency."
    },
    {
        "question_id": "2502.10248v1_4",
        "answer": "Step-Video-T2V leverages a dual approach to handle bilingual text prompts, employing two distinct text encoders: Hunyuan-CLIP and Step-LLM. Each encoder brings unique strengths to the system, enabling it to effectively process and understand both English and Chinese prompts.\n\n***\n\n### Bilingual Text Prompt Handling in Step-Video-T2V\n\n1.  **Hunyuan-CLIP**: This is a bidirectional text encoder derived from an open-source bilingual CLIP model. Its primary advantage lies in producing text representations that are well-aligned with the visual space, a crucial factor for generating visually coherent videos from text prompts. However, it has a limitation of a maximum input length of 77 tokens, which can be restrictive when dealing with longer, more complex prompts.\n2.  **Step-LLM**: This is a unidirectional bilingual text encoder pre-trained using a next-token prediction task. It incorporates a redesigned Alibi-Positional Embedding to enhance efficiency and accuracy in sequence processing. Unlike Hunyuan-CLIP, Step-LLM does not have input length restrictions, making it suitable for handling lengthy and complex text sequences.\n\n***\n\n### Advantages of Using Two Separate Text Encoders\n\n*   **Handling Varying Prompt Lengths**: By using two text encoders, Step-Video-T2V can effectively handle both short and long user prompts. Hunyuan-CLIP is used for shorter prompts, while Step-LLM is used for longer prompts.\n*   **Robust Text Representations**: The outputs from both encoders are concatenated to form the final text embedding sequence. This combined embedding captures a more comprehensive representation of the input text, allowing the model to generate videos conditioned on the input prompt.\n*   **Improved Visual-Text Alignment**: Hunyuan-CLIP's ability to produce text representations well-aligned with the visual space ensures that the generated videos are visually coherent and consistent with the input text.\n*   **Effective Guidance in Latent Space**: The text encoders play a crucial role in guiding the model in the latent space, ensuring that the generated videos accurately reflect the content and style described in the text prompts."
    },
    {
        "question_id": "2502.10248v1_5",
        "answer": "The paper \"Step-Video-T2V Technical Report: The Practice, Challenges, and Future of Video Foundation Model\" identifies several key challenges facing current diffusion-based text-to-video models and details how Step-Video-T2V attempts to address these limitations. Let's explore these challenges and Step-Video-T2V's solutions in a structured way:\n\n***\n\n### 1. Challenges in Existing Text-to-Video Models\n\n*   **High Computational Cost**: Generating high-resolution, long-duration videos requires significant computational resources due to the quadratic scaling of attention mechanisms with the number of tokens.\n*   **Difficulty in Modeling Complex Action Sequences and Physics**: Current models struggle to generate videos that involve complex action sequences or adhere to the laws of physics. This limitation arises because these models primarily learn mappings between text prompts and videos without explicitly modeling underlying causal relationships.\n*   **Instruction Following Issues**: Models often fail to accurately interpret and follow detailed instructions, especially when generating videos that require composing multiple concepts with low occurrence in the training data. This can result in missing objects, incorrect details, or incomplete action sequences.\n*   **Hallucination Issues in Video Captioning**: Current video captioning models face hallucination issues, leading to unstable training and poor instruction-following performance.\n*   **Data Quality and Availability**: High-quality labeled video data is scarce and expensive to obtain. Existing video captioning models often produce inaccurate captions, further exacerbating the data quality problem.\n\n***\n\n### 2. How Step-Video-T2V Addresses These Challenges\n\n*   **Deep Compression Video-VAE**: To reduce the computational complexity of large-scale video generation, Step-Video-T2V employs a deep compression **Video-VAE** that achieves 16x16 spatial and 8x temporal compression ratios. This reduces the number of tokens that the diffusion transformer needs to process, thereby accelerating training and inference.\n*   **3D Full Attention Mechanism**: Step-Video-T2V utilizes a **DiT** with a 3D full attention mechanism to capture both spatial and temporal information in videos more effectively. This helps in generating videos with smoother and more consistent motion.\n*   **Bilingual Text Encoders**: To improve instruction following, Step-Video-T2V uses two bilingual text encoders (**Hunyuan-CLIP** and **Step-LLM**) to process user text prompts in both English and Chinese. This allows the model to handle user prompts of varying lengths and complexity, generating robust text representations that guide the model in the latent space.\n*   **Video-Based DPO Approach**: Step-Video-T2V applies a video-based direct preference optimization (**Video-DPO**) approach to enhance the visual quality of generated videos by reducing artifacts and ensuring smoother, more realistic video outputs. This approach incorporates human feedback to align the model's output with user preferences.\n*   **Cascaded Training Pipeline**: Step-Video-T2V employs a cascaded training pipeline that includes text-to-image pre-training, text-to-video pre-training, supervised fine-tuning (SFT), and direct preference optimization (DPO). This pipeline helps accelerate model convergence and fully leverage video datasets of varying quality.\n*   **Data Filtering and Processing**: The paper details the process of pre-processing large-scale videos as training data, including filtering and utilizing these videos at different stages of training. This ensures that the model is trained on high-quality data, which is crucial for generating realistic and coherent videos.\n*   **Training Objective using Flow Matching**: Step-Video-T2V uses Flow Matching as the training objective, which involves predicting the velocity field that transforms noise into a coherent video.\n\n***\n\n### 3. Additional Strategies and Insights\n\n*   **Text-to-Image Pre-training**: The paper emphasizes the importance of text-to-image pre-training for the video generation model to acquire rich visual knowledge.\n*   **Low-Resolution Text-to-Video Pre-training**: Text-to-video pre-training at low resolution is critical for the model to learn motion dynamics.\n*   **High-Quality SFT Data**: Using high-quality videos with accurate captions and desired styles in SFT is crucial to the stability of the model and the style of the generated videos.\n\n***\n\nBy implementing these strategies, Step-Video-T2V aims to mitigate the challenges associated with diffusion-based text-to-video models and achieve state-of-the-art performance in terms of video quality, motion dynamics, and instruction following."
    },
    {
        "question_id": "2502.10248v1_6",
        "answer": "Step-Video-T2V employs Flow Matching as a core training objective to guide the video generation process. Here's a breakdown of how it works and its benefits:\n\n***\n\n### Flow Matching in Step-Video-T2V\n\n1.  **Noise Sampling:** The process begins by sampling random Gaussian noise, denoted as $X_0$, from a standard normal distribution.\n\n2.  **Timestep Selection:** A random timestep, $t$, is selected from a uniform distribution between 0 and 1 (inclusive).\n\n3.  **Linear Interpolation:** Model input $X_t$ is constructed using linear interpolation between $X_0$ and target sample $X_1$, where $X_1$ represents the noise-free target video or image. The formula for this interpolation is:\n\n    $X_t = (1 - t) \\cdot X_0 + t \\cdot X_1$\n\n4.  **Velocity Calculation:** The ground truth velocity, $V_t$, which represents the rate of change of $X_t$ with respect to $t$, is calculated as the difference between $X_1$ and $X_0$:\n\n    $V_t = \frac{dX_t}{dt} = X_1 - X_0$\n\n5.  **Model Training:** The model is trained to predict the velocity field $u(X_t, y, t; \theta)$ at timestep $t$, given input $X_t$, optional conditioning input $y$ (such as text prompts), and model parameters $\theta$. The training objective minimizes the Mean Squared Error (MSE) between the predicted velocity and the ground truth velocity:\n\n    $loss = E_{t, X_0, X_1, y} [||u(X_t, y, t; \theta) - V_t||^2]$\n\n    The expectation is taken over all training samples, with $t$, $X_0$, $X_1$, and $y$ drawn from the dataset.\n\n6.  **Inference:** During inference, the process starts by sampling random noise $X_0$ from a standard normal distribution. The goal is to recover the denoised sample $X_1$ by iteratively refining the noise through an ODE-based method. A sequence of timesteps $\\{t_0, t_1, ..., t_n\\}$ is defined, where $t_0 = 0$, $t_n = 1$, and $t_0 < t_1 < ... < t_n$. The denoising process is then carried out by integrating over these timesteps. The denoised sample $X_1$ can be expressed as:\n\n    $X_1 = \\sum_{i=0}^{n-1} u(X_{t_i}, y, t_i; \theta) \\cdot (t_{i+1} - t_i)$\n\n    where $u(X_{t_i}, y, t_i; \theta)$ represents the predicted velocity at timestep $t_i$, given the noisy sample $X_{t_i}$ and an optional conditioning input $y$.\n\n***\n\n### Benefits of Flow Matching for Video Generation\n\n1.  **Stable Training:** Flow Matching offers a more stable training process compared to other diffusion-based approaches. By directly predicting the velocity field, the model learns a continuous mapping from noise to data, avoiding issues like mode collapse or vanishing gradients.\n\n2.  **High-Quality Samples:** The method produces high-quality video samples with consistent motion dynamics and fine details. The continuous nature of Flow Matching allows for smoother transitions between frames, resulting in more realistic and visually appealing videos.\n\n3.  **Flexibility:** Flow Matching can be easily conditioned on various inputs, such as text prompts, enabling precise control over the generated video content. The model can effectively incorporate textual information to guide the video generation process.\n\n4.  **Efficiency:** The method can achieve competitive results with fewer sampling steps during inference, reducing the computational cost of video generation. The ODE-based sampling allows for efficient refinement of the initial noise, leading to faster convergence to the final video sample."
    },
    {
        "question_id": "2502.10248v1_7",
        "answer": "The Step-Video-T2V model employs a hierarchical data filtering approach to refine its training data progressively. This involves applying a series of filters with increasing thresholds to create multiple pre-training subsets. The final SFT (Supervised Fine-Tuning) dataset is then constructed through manual filtering.\n\n***\n\nHere's a breakdown of the key aspects and importance of this approach:\n\n### Key Filters Applied\n\nThe hierarchical data filtering uses several key filters at each stage. These filters can be broadly categorized as:\n\n*   **Video Quality Assessment:**\n    *   **Aesthetic Score:** Uses a CLIP-based aesthetic predictor to assess the visual appeal of video frames.\n    *   **NSFW Score:** Detects content inappropriate for safe work environments using a CLIP-based NSFW detector.\n    *   **Watermark Detection:** Identifies the presence of watermarks using an EfficientNet image classification model.\n    *   **Subtitle Detection:** Recognizes and localizes text within video frames using PaddleOCR to identify clips with excessive on-screen text or captions.\n    *   **Saturation Score:** Assesses color saturation by converting video frames to HSV color space and extracting the saturation channel.\n    *   **Blur Score:** Detects blurriness by applying the variance of the Laplacian method to measure the sharpness of each frame.\n    *   **Black Border Detection:** Detects black borders in frames using FFmpeg to facilitate cropping and ensure the model trains on content free of distracting edges.\n*   **Video Motion Assessment:**\n    *   **Motion\\_Mean:** Indicates the average motion magnitude across all frames in the clip.\n    *   **Motion\\_Max:** Highlights instances of extreme motion or motion distortion.\n    *   **Motion\\_Min:** Identifies clips with minimal motion.\n*   **Video Captioning:**\n    *   **Short Caption:** Provides a concise description, focusing solely on the main subject and action.\n    *   **Dense Caption:** Integrates key elements, emphasizing the main subject, events, environmental and visual aspects, video type and style, as well as camera shots and movements.\n    *   **Original Title:** Incorporates a portion of the original titles from the raw videos, adding diversity to the captions.\n*   **Video Concept Balancing:**\n    *   **Cluster\\_Cnt:** The total number of clips in the cluster to which the clip belongs.\n    *   **Center\\_Sim:** The cosine distance between the clip\u2019s embedding and the cluster center.\n*   **Video-Text Alignment:**\n    *   **CLIP Score:** Measures how well the captions align with the visual content of the video clips.\n\n### Importance for Training High-Quality Video Generation Models\n\n1.  **Improved Data Quality**: By filtering out low-quality videos, the model trains on cleaner data, leading to better video generation quality.\n2.  **Enhanced Motion Dynamics**: Assessing and filtering videos based on motion-related metrics ensures that the model learns from content with appropriate and realistic motion.\n3.  **Better Text Alignment**: Using the CLIP score to filter videos ensures that the textual descriptions accurately reflect the video content, improving the model's ability to generate videos that match the given prompts.\n4.  **Curriculum Learning**: Progressively increasing the thresholds of the filters allows the model to learn from easier examples first and then gradually move to more complex ones, which can improve convergence and overall performance.\n5.  **Human Cognitive Alignment**: Adjusting the training data to reflect what is considered higher-quality by humans results in a significant reduction in training loss, suggesting that the model's learning process may emulate human cognitive patterns.\n\n***\n\nIn summary, the hierarchical data filtering approach in Step-Video-T2V is crucial for ensuring that the model is trained on high-quality, well-aligned data, which ultimately leads to improved video generation performance. By systematically assessing and filtering videos based on various criteria, the model can learn to generate videos with strong motion dynamics, high aesthetics, and consistent content."
    },
    {
        "question_id": "2502.10248v1_8",
        "answer": "Step-Video-T2V is positioned as a competitive alternative to commercial video generation models, such as Sora and Veo. Here's a breakdown:\n\n*   **General Text Prompt Performance**: Step-Video-T2V delivers comparable performance.\n*   **Specific Domain Performance**: Step-Video-T2V surpasses commercial models in generating videos with high motion dynamics or text content.\n*   **Transparency**: Unlike closed-source commercial engines, Step-Video-T2V offers greater transparency in its implementation.\n*   **Key Features**: Step-Video-T2V is the largest open-source model, uses a high-compression **VAE** for videos, supports bilingual text prompts, implements a video-based **DPO** approach, and provides comprehensive training documentation."
    },
    {
        "question_id": "2502.10248v1_9",
        "answer": "Human feedback plays a crucial role in refining Step-Video-T2V's video generation by improving the visual quality of the generated videos and ensuring better alignment with user prompts. It is implemented in the training pipeline through a process that involves collecting preference data, training a reward model, and using Direct Preference Optimization (**DPO**).\n\n***\n\nHere's a breakdown of the key components and implementation:\n\n### 1. Preference Data Collection\n\n*   **Prompt Set Construction**: A diverse set of prompts is created to ensure diversity and reflect real-world user interaction patterns.\n*   **Video Generation**: For each prompt, Step-Video-T2V generates multiple videos using different seeds.\n*   **Human Annotation**: Human annotators rate the preference of these samples, and the annotation process is monitored for accuracy and consistency.\n*   **Preference Data**: The process results in a set of preference and non-preference data, which serves as the foundation for model training.\n\n### 2. Direct Preference Optimization (DPO)\n\n*   **Objective**: The goal is to adjust the current policy (i.e., the model) to be more aligned with the generation of preferred data while avoiding the generation of non-preferred data.\n*   **Reference Policy**: A reference policy (i.e., the reference model) is introduced to prevent the current policy from deviating too far.\n*   **Loss Function**: The policy objective is formulated as:\n\n    $L_{DPO} = -E_{(y, x_w, x_l) \\sim D} \\left[ log \\ \\sigma \\left( \beta \\left( log \\ \frac{\\pi_\theta(x_w|y)}{\\pi_{ref}(x_w|y)} - log \\ \frac{\\pi_\theta(x_l|y)}{\\pi_{ref}(x_l|y)} \right) \right) \right]$\n\n    where:\n\n    *   $\\pi_\theta$ and $\\pi_{ref}$ refer to the current policy and reference policy, respectively.\n    *   $x_w$ and $x_l$ are the preferred sample and non-preferred sample, respectively.\n    *   $y$ denotes the condition (prompt).\n    *   $\beta$ is a hyperparameter controlling the strength of the preference optimization.\n\n### 3. Reward Model Training\n\n*   **Purpose**: To dynamically evaluate the quality of newly generated samples during training and maintain alignment with the evolving policy.\n*   **Fine-tuning**: The reward model is periodically fine-tuned with newly annotated human feedback.\n*   **On-Policy Scoring**: The reward model scores and ranks training data on-the-fly (on-policy), improving data efficiency.\n\n### 4. Addressing Saturation\n\n*   **Problem**: Improvements from human feedback saturate when the model can easily distinguish between positive and negative samples because the training data is generated by earlier versions of the model.\n*   **Solution**: The reward model is trained using human-annotated feedback data to evaluate the quality of newly generated samples dynamically.\n\n***\n\nIn summary, human feedback is integral to Step-Video-T2V's training, guiding the model to produce visually appealing and contextually relevant videos through preference data collection, **DPO**, and reward model training."
    }
]