[
    {
        "question_id": "2502.10248v1_0",
        "answer": "The paper defines two levels of video foundation models, each representing a different stage of capability and complexity:\n\n1.  **Level-1: Translational Video Foundation Model**.\n2.  **Level-2: Predictable Video Foundation Model**.\n\n***\n\n### Key Differences:\n\n*   **Level-1** models primarily function as cross-modal translation systems. They can generate videos from various inputs such as text, visual data, or multimodal contexts. These models excel at creating high-quality videos from text prompts, which lowers the barrier for content creation.\n\n*   **Level-2** models act as prediction systems, akin to large language models (LLMs). They forecast future events based on text, visual, or multimodal contexts and handle more advanced tasks. These tasks include reasoning with multimodal data or simulating real-world scenarios.\n\nIn essence:\n\n*   **Level-1** focuses on converting inputs into video.\n*   **Level-2** focuses on understanding and predicting video content and its implications.\n\nCurrent diffusion-based text-to-video models like Step-Video-T2V belong to Level-1. These models still need to model causal relationships within videos to perform tasks requiring complex action sequences or adherence to the laws of physics, which are goals for Level-2 models."
    },
    {
        "question_id": "2502.10248v1_1",
        "answer": "Step-Video-T2V uses a deep compression **Video-VAE** to reduce the computational demands of large-scale video generation training. It achieves **16x16 spatial** and **8x temporal compression ratios** while maintaining good video reconstruction quality by using a novel architecture and a multi-stage training process.\n\n***\n\nHere's a breakdown of how it works:\n\n### 1. Architecture\n*   **Dual-Path Architecture**: The Video-VAE uses a dual-path architecture in the later stages of the encoder and the early stages of the decoder. This design enables unified spatial-temporal compression.\n*   **Unified Spatial-Temporal Compression**: Achieves **8\u00d716\u00d716** downscaling using 3D convolutions and optimized pixel shuffling operations.\n\n### 2. Encoder Details\n*   For an input video tensor $X \u2208 R^{B\u00d7C\u00d7T\u00d7H\u00d7W}$, the encoder $E$ produces a latent representation $Z = E(X) \u2208 R^{B\u00d7C_z\u00d7\u2308T/8\u2309\u00d7\u2308H/16\u2309\u00d7\u2308W/16\u2309}$.\n*   **Causal 3D Convolutional Modules**: The early stage of the encoder consists of three stages, each featuring two **Causal Res3DBlock** and corresponding downsample layers. A **MidBlock** combines convolutional layers with attention mechanisms. Temporal causality is implemented through:\n\n    $C3D(X)_t = \begin{cases}\n    Conv3D([0, ..., X_t], \u0398) & t = 0 \\\n    Conv3D([X_{t-k}, ..., X_t], \u0398) & t > 0\n    \\end{cases}$\n\n    where $k$ is the temporal kernel size.\n*   **Dual-Path Latent Fusion**: Maintains high-frequency details through convolutional processing and preserves low-frequency structure via channel averaging.\n    1.  **Conv Path**: Combines causal 3D convolutions with pixel unshuffling:\n\n        $H_{conv} = U^{(3)}_s(C3D(X))$\n\n        where $U^{(3)}_s: R^{B\u00d7C\u00d7T_{st} \u00d7 H_{ss} \u00d7 W_{ss}} \u2192 R^{B\u00d7C\u22c5s^3\u00d7 T_{st} \u00d7 H_{ss} \u00d7 W_{ss}}$ with spatial stride $s_s = 2$, temporal stride $s_t = 2$, and $C3D$ denoting the causal 3D convolution.\n    2.  **Shortcut Path**: Preserves structural semantics through grouped channel averaging:\n\n        $H_{avg} = \frac{1}{s^3} \\sum_{k=0}^{s^3-1} U^{(3)}_s (X)[..., kC_z:(k+1)C_z]$\n\n        where $U^{(3)}_s$ implements 3D pixel unshuffle with spatial-temporal blocking, and $C_z$ is the latent dim of the next stage.\n    3.  **Output Fusion**: Combines both paths through residual summation:\n\n        $Z = H_{conv} \u2295 H_{avg}$\n\n### 3. Decoder Details\n*   The early stage of the decoder consists of two symmetric Dual Path architectures.\n*   The 3D pixel unshuffle operation $U$ is replaced by a 3D pixel shuffle operator $P$.\n*   The grouped channel averaging path is replaced by a grouped channel repeating operation.\n*   All groupnorm layers are replaced with spatial groupnorm to prevent temporal flickering between different chunks.\n\n### 4. Training Process\n*   **Multiple Stages**: The VAE training is done in multiple stages.\n    1.  **Initial Training**: A VAE with a **4x8x8 compression ratio** is trained without a dual-path structure, jointly on images and videos.\n    2.  **Dual-Path Enhancement**: Two dual-path modules are incorporated in both the encoder and decoder, gradually unfreezing the dual-path modules, the mid-block, and the ResNet backbone.\n*   **Loss Functions**: A combination of **L1 reconstruction loss**, **Video-LPIPS**, and **KL-divergence** is used. **GAN loss** is introduced to further refine the model\u2019s performance once these losses have converged."
    },
    {
        "question_id": "2502.10248v1_2",
        "answer": "Direct Preference Optimization (DPO) is a technique used to align a model's behavior with human preferences by directly optimizing the model based on preference data, without using reinforcement learning. It is used in Step-Video-T2V to enhance the visual quality of the generated videos.\n\n***\n\nHere's how DPO works and how it's applied in Step-Video-T2V:\n\n### Core Idea of DPO\n\nDPO aims to train a model to generate outputs that are preferred by humans, given a set of preference data. This data consists of pairs of outputs for the same input, where one output is preferred over the other. The goal is to adjust the model to generate more of the preferred outputs and less of the non-preferred outputs.\n\n### DPO Formulation\n\n1.  **Objective**: The DPO objective can be formulated as:\n\n    $L_{DPO} = -E_{(y, x_w, x_l) \\sim D} \\left[ \\log \\sigma \\left( \beta \\left( \\log \frac{\\pi_\theta(x_w|y)}{\\pi_{ref}(x_w|y)} - \\log \frac{\\pi_\theta(x_l|y)}{\\pi_{ref}(x_l|y)} \right) \right) \right]$\n\n    Where:\n\n    *   $\\pi_\theta$ is the current policy (i.e., the model being trained).\n    *   $\\pi_{ref}$ is the reference policy (i.e., the reference model).\n    *   $x_w$ is the preferred sample.\n    *   $x_l$ is the non-preferred sample.\n    *   $y$ denotes the condition (e.g., a text prompt).\n    *   $\beta$ is a hyperparameter controlling the strength of the preference.\n    *   $\\sigma$ is the sigmoid function.\n2.  **Intuition**: The goal is to maximize the probability of the preferred sample ($x_w$) while minimizing the probability of the non-preferred sample ($x_l$), relative to the reference policy.\n\n***\n\n### Implementation Details in Step-Video-T2V\n\n1.  **Data Collection**:\n    *   A diverse prompt set is constructed by randomly selecting prompts from the training data and synthesizing prompts based on real-world user interaction patterns.\n    *   For each prompt, Step-Video-T2V generates multiple videos using different seeds.\n    *   Human annotators rate the preference of these samples, and the annotation process is monitored for accuracy and consistency.\n2.  **Training Process**:\n    *   At each training step, a prompt and its corresponding positive (preferred) and negative (non-preferred) sample pairs are selected.\n    *   Each sample is generated by the model itself to ensure smooth updates and improve training stability.\n    *   The positive and negative samples are aligned by fixing the initial noise and timestep to maintain consistency in the training data.\n3.  **Addressing Gradient Issues**:\n    *   The training objective is based on DiffusionDPO and DPO but with slight modifications to extend it to the Flow Matching framework.\n    *   The gradient of the DPO loss is given by:\n\n        $\frac{\\partial L_{DPO}}{\\partial \theta} \\propto -\beta (1 - \\sigma(\beta z)) \\cdot \frac{\\partial z}{\\partial \theta}$\n\n        Where $z$ represents the policy-related terms in the DPO objective.\n    *   To avoid gradient explosion, the hyperparameter $\beta$ is reduced, and the learning rate is increased to ensure faster convergence.\n4.  **Reward Model Integration**:\n    *   To address the issue of improvements saturating as the model distinguishes between positive and negative samples, a reward model is trained using human-annotated feedback data.\n    *   This reward model evaluates the quality of newly generated samples during training and is periodically fine-tuned with newly annotated human feedback to maintain alignment with the evolving policy.\n    *   The reward model scores and ranks training data on-the-fly (on-policy), improving data efficiency.\n\n***\n\n### Impact on Visual Quality in Step-Video-T2V\n\n1.  **Enhanced Plausibility and Consistency**: Human feedback enhances the plausibility and consistency of generated videos.\n2.  **Improved Alignment with Prompts**: DPO improves the alignment with the given prompts, resulting in more accurate and relevant video generation.\n3.  **Artifact Reduction**: By incorporating human preferences, DPO helps in reducing artifacts and generating smoother, more realistic video outputs."
    },
    {
        "question_id": "2502.10248v1_3",
        "answer": "Here's a breakdown of the key advantages of using a **DiT** with **3D full attention** in Step-Video-T2V, based on the provided document:\n\n*   **Superior Modeling of Spatial and Temporal Information:**\n\n    *   **3D full attention** is theoretically better at capturing both spatial and temporal information in videos compared to spatial-temporal attention. The unified attention process allows for a more holistic understanding of the video content.\n*   **Generation of Smooth and Consistent Motion:**\n\n    *   Large-scale experiments have shown that **3D full attention** leads to the generation of videos with smoother and more consistent motion. This is a crucial aspect of video quality, as it contributes to a more realistic and visually appealing output.\n*   **Effective Incorporation of Text Prompts:**\n\n    *   The architecture uses a cross-attention layer between the self-attention and feed-forward network (**FFN**) in each transformer block. This allows the model to attend to textual information while processing visual features, enabling the generation of videos conditioned on the input prompt.\n*   **Optimized Computational Efficiency:**\n\n    *   The model employs Adaptive Layer Normalization (**AdaLN**) with optimized computation. By removing class labels from **AdaLN** (since the text-to-video task doesn't require them) and adopting the **AdaLN-Single** structure, the computational overhead of traditional **AdaLN** operations is reduced, improving overall model efficiency.\n*   **Handling of Video Data with Varying Lengths and Resolutions:**\n\n    *   The use of **RoPE-3D**, an extension of the traditional Rotation-based Positional Encoding (**RoPE**), enables the model to handle video data by accounting for temporal (frame) and spatial (height and width) dimensions. This allows the model to process videos with different frame counts and resolutions without being restricted by fixed positional encoding lengths.\n\nIn summary, the choice of a **DiT** architecture with **3D full attention** in Step-Video-T2V offers a strong balance between modeling capability, generation quality, and computational efficiency, making it well-suited for the task of text-to-video generation."
    },
    {
        "question_id": "2502.10248v1_4",
        "answer": "Step-Video-T2V uses two **bilingual text encoders** to process user text prompts in both English and Chinese.\n\n***\n\n### Text Encoders Used\n\n1.  Hunyuan-CLIP\n2.  Step-LLM\n\n***\n\n### Advantages of Using Two Separate Text Encoders\n\n*   **Handling Varying Prompt Lengths**:\n    *   Hunyuan-CLIP is effective for shorter prompts due to its alignment with the visual space. However, it is limited to a maximum input length of 77 tokens.\n    *   Step-LLM, on the other hand, has no input length restriction, making it suitable for longer and more complex text sequences.\n*   **Robust Text Representations**:\n    *   By combining these two text encoders, Step-Video-T2V can handle user prompts of varying lengths, generating robust text representations that effectively guide the model in the latent space.\n    *   Hunyuan-CLIP produces text representations well-aligned with the visual space due to the training mechanism of the CLIP model.\n    *   Step-LLM incorporates a redesigned **Alibi-Positional Embedding** to improve both efficiency and accuracy in sequence processing."
    },
    {
        "question_id": "2502.10248v1_5",
        "answer": "The paper identifies several key challenges facing current diffusion-based text-to-video models and explains how Step-Video-T2V attempts to address these challenges. Let's explore these challenges and the corresponding approaches taken by Step-Video-T2V:\n\n***\n\n### 1. High-Quality Labeled Data\n\n**Challenge:**\n\n*   Existing video captioning models often struggle with hallucination issues, leading to unstable training.\n*   Human annotations are expensive and difficult to scale.\n\n**Step-Video-T2V's Approach:**\n\n*   **Data Curation:** Step-Video-T2V uses both automated and manual filtering techniques to curate a high-quality video dataset. This involves using video assessment scores and heuristic rules to filter the dataset, ensuring clarity, aesthetics, appropriate motion, and smooth scene transitions.\n*   **Human Annotation Refinement:** Captions are manually refined to ensure accuracy and include essential details such as camera movements, subjects, actions, backgrounds, and lighting.\n\n***\n\n### 2. Instruction Following\n\n**Challenge:**\n\n*   Generating videos based on detailed descriptions and handling complex action sequences is difficult.\n*   Composing multiple concepts with low occurrence in the training data within a single generated video remains a challenge.\n\n**Step-Video-T2V's Approach:**\n\n*   **Bilingual Text Encoders:** Step-Video-T2V employs two bilingual text encoders, Hunyuan-CLIP and Step-LLM, to process user text prompts. Hunyuan-CLIP produces text representations well-aligned with the visual space but has limited input length. Step-LLM has no input length restriction, making it effective for handling lengthy and complex text sequences.\n*   **Cross-Attention Mechanism:** A cross-attention layer is introduced between the self-attention and feed-forward network in each transformer block to incorporate text prompts. This enables the model to attend to textual information while processing visual features.\n\n***\n\n### 3. Laws of Physics Following\n\n**Challenge:**\n\n*   Current models struggle to accurately simulate the real world and generate videos that adhere to the laws of physics.\n\n**Step-Video-T2V's Approach:**\n\n*   The paper acknowledges this as a key limitation of diffusion-based models.\n*   The paper suggests developing more advanced model paradigms in future work, such as combining autoregressive and diffusion models within a unified framework, to better adhere to the laws of physics and more accurately simulate realistic interactions.\n\n***\n\n### 4. Computational Cost\n\n**Challenge:**\n\n*   Training and generating long-duration, high-resolution videos still face significant computational cost hurdles.\n\n**Step-Video-T2V's Approach:**\n\n*   **Deep Compression Video-VAE:** A specially designed deep compression Variational Auto-encoder (VAE) achieves 16x16 spatial and 8x temporal compression ratios, significantly reducing the computational complexity of large-scale video generation training.\n\n***\n\n### 5. Post-Training Improvements\n\n**Challenge:**\n\n*   Optimizing video generation models through reinforcement learning (RL) is challenging because, unlike language tasks, defining clear tasks with well-defined answers in the video generation domain is difficult.\n\n**Step-Video-T2V's Approach:**\n\n*   **Video-based DPO:** A video-based DPO approach is applied to reduce artifacts and improve the visual quality of the generated videos.\n*   **Reward Model Training:** The paper proposes training a reward model using human-annotated feedback data to dynamically evaluate the quality of newly generated samples during training.\n\n***\n\n### 6. Model Architecture\n\n**Challenge:**\n\n*   Choosing the right model architecture.\n\n**Step-Video-T2V's Approach:**\n\n*   **DiT with 3D Full Attention:** Step-Video-T2V is built on the DiT architecture with 3D full attention. The 3D full attention mechanism combines both spatial and temporal information in a unified attention process, offering higher performance potential.\n\n***\n\n### 7. Training Strategy\n\n**Challenge:**\n\n*   Accelerating model convergence and fully leveraging video datasets of varying quality.\n\n**Step-Video-T2V's Approach:**\n\n*   **Cascaded Training Pipeline:** A cascaded training pipeline, including text-to-image pre-training, text-to-video pre-training, supervised fine-tuning (SFT), and direct preference optimization (DPO), is introduced to accelerate model convergence and fully leverage video datasets of varying quality.\n\n***\n\nIn summary, Step-Video-T2V addresses the challenges of high-quality data, instruction following, computational cost, and model architecture through data curation, bilingual text encoders, a deep compression Video-VAE, and a DiT architecture with 3D full attention. It tackles post-training improvements with a video-based DPO approach and proposes training a reward model. The challenge of adhering to the laws of physics is acknowledged as a limitation, with future work planned to address it."
    },
    {
        "question_id": "2502.10248v1_6",
        "answer": "The paper \"Step-Video-T2V Technical Report: The Practice, Challenges, and Future of Video Foundation Model\" details how Step-Video-T2V employs Flow Matching as a training objective. Here's a breakdown:\n\n***\n\n### Flow Matching in Step-Video-T2V\n\n1.  **Noise Sampling:** The process begins by sampling Gaussian noise, denoted as $X_0 \\sim N(0,1)$.\n\n2.  **Timestep Sampling:** A random timestep $t$ is sampled from a uniform distribution between 0 and 1 (i.e., $t \\in [0, 1]$).\n\n3.  **Input Construction:** The model input $X_t$ is constructed as a linear interpolation between the noise $X_0$ and the target sample $X_1$ (noise-free input):\n    $$\n    X_t = (1 - t) \\cdot X_0 + t \\cdot X_1\n    $$\n\n4.  **Velocity Calculation:** The ground truth velocity $V_t$ is defined as the rate of change of $X_t$ with respect to the timestep $t$:\n    $$\n    V_t = \frac{dX_t}{dt} = X_1 - X_0\n    $$\n    $V_t$ represents the direction and magnitude of change from the initial noise $X_0$ to the target data $X_1$.\n\n5.  **Loss Minimization:** The model is trained to predict the velocity $u(X_t, y, t; \theta)$ at timestep $t$, given the input $X_t$, an optional conditioning input $y$ (e.g., a bilingual sentence), and model parameters $\theta$. The training loss is the Mean Squared Error (MSE) between the predicted velocity and the true velocity:\n    $$\n    \text{loss} = E_{t, X_0, X_1, y} \\left[ \\| u(X_t, y, t; \theta) - V_t \\|^2 \right]\n    $$\n    The expectation is taken over all training samples, with $t$, $X_0$, $X_1$, and $y$ drawn from the dataset.\n\n### Benefits of Flow Matching for Video Generation\n\n*   **Stable Training:** Flow Matching offers inherent stability compared to other diffusion training methods. By directly modeling the velocity field between noise and data, it avoids issues like mode collapse and vanishing gradients, leading to more reliable convergence during training.\n\n*   **High-Quality Samples:** Flow Matching produces high-quality video samples. The method ensures that the generated frames are coherent and realistic by learning the continuous transformation from noise to data.\n\n*   **Flexibility:** Flow Matching is flexible and can be adapted to various architectures and data types. It does not impose strict constraints on the model architecture or the data distribution, making it suitable for complex video generation tasks.\n\n*   **Efficient Inference:** Flow Matching supports efficient inference through **ODE (Ordinary Differential Equation)** solvers. This allows for faster generation of videos with fewer steps, reducing computational costs while maintaining high quality.\n\n*   **Conditional Generation:** Flow Matching easily incorporates conditional inputs (e.g., text prompts) by conditioning the velocity field on these inputs. This enables the model to generate videos that align with specific textual descriptions or other conditioning signals."
    },
    {
        "question_id": "2502.10248v1_7",
        "answer": "Step-Video-T2V employs a hierarchical data filtering approach that progressively increases the stringency of its filters to create multiple pre-training subsets. This approach is crucial for training high-quality video generation models because it systematically refines the dataset, ensuring that only the best data is used for training. This process involves both automated and manual filtering techniques.\n\n***\n\nHere's a breakdown of the key steps and their importance:\n\n### 1. Automated Filtering\n\n*   **Filtering by Video Assessment Scores:**\n    *   Videos are initially filtered using video assessment scores and heuristic rules.\n    *   This step significantly improves the overall quality of the dataset by removing low-quality videos.\n*   **Filtering by Video Categories:**\n    *   Videos within the same cluster are filtered based on their \"Distance to Centroid\" values.\n    *   This ensures that the video subset contains a sufficient number of videos for each cluster while maintaining diversity within the subset.\n\n### 2. Manual Filtering\n\n*   **Labeling by Human Annotators:**\n    *   Human evaluators assess each video for clarity, aesthetics, appropriate motion, smooth scene transitions, and the absence of watermarks or subtitles.\n    *   Captions are manually refined to ensure accuracy and include essential details such as camera movements, subjects, actions, backgrounds, and lighting.\n\n***\n\n### Importance of Hierarchical Data Filtering\n\n1.  **Improved Data Quality:**\n    *   By systematically filtering the data, the model is trained on videos that are clear, aesthetically pleasing, and free of artifacts like watermarks and subtitles.\n2.  **Enhanced Model Stability:**\n    *   High-quality data leads to more stable training, reducing the likelihood of the model generating videos with distortions or inconsistencies.\n3.  **Better Generalization:**\n    *   A diverse and well-balanced dataset ensures that the model generalizes well across different types of scenes, actions, and subjects.\n4.  **Increased Training Efficiency:**\n    *   Training on a refined dataset allows the model to converge faster and achieve better results with fewer resources.\n5.  **Mitigation of Hallucination Issues:**\n    *   Accurate and detailed captions help the model better understand the content of the videos, reducing hallucination issues and improving instruction-following performance.\n6.  **Style Consistency:**\n    *   Curating videos with desired styles in the SFT stage ensures that the generated videos adhere to those styles, leading to more visually appealing results."
    },
    {
        "question_id": "2502.10248v1_8",
        "answer": "Step-Video-T2V is positioned as a competitive alternative to commercial video generation models, such as Sora and Veo, with certain advantages and limitations. Here's a breakdown:\n\n### General Performance and Capabilities\n\n*   **Comparable Performance:** Step-Video-T2V delivers similar performance to closed-source engines like Sora, Gen-3, Kling, and Hailuo for general text prompts.\n*   **Specific Domain Superiority:** It surpasses commercial models in specific domains, particularly in generating videos with high motion dynamics or text content.\n*   **Transparency and Accessibility:** Unlike the longer, more complex video generation pipelines of closed-source engines that involve extensive pre- and post-processing, Step-Video-T2V offers greater transparency in its implementation, making it more accessible to researchers and content creators.\n\n***\n\n### Key Differentiators of Step-Video-T2V\n\nCompared to commercial models like Movie Gen Video, Step-Video-T2V has several unique features:\n\n1.  **High Compression VAE**: Incorporates a more powerful high compression **VAE** for large-scale video generation training.\n2.  **Bilingual Text Prompt Understanding**: Supports bilingual text prompt understanding in both English and Chinese.\n3.  **Direct Preference Optimization (DPO)**: Adds an additional **DPO** stage to the training process, reducing artifacts and improving the visual quality of the generated videos.\n4.  **Open Source**: Open-source and provides state-of-the-art video generation quality compared with both open-source and commercial engines.\n\n***\n\n### Limitations and Future Improvements\n\nDespite its competitive performance, Step-Video-T2V has limitations when compared to some commercial models:\n\n*   **Training Data**: Step-Video-T2V has been trained on significantly fewer samples compared to models like Movie Gen Video (27.3M videos vs. 73.8M videos).\n*   **High-Quality Labeled Data**: Lacks enough high-quality labeled data to effectively refine the visual style and quality of the generated results.\n*   **Video Resolution**: Generates videos at a lower resolution (540P) compared to Movie Gen Video (720P), which can impact human perception of video quality.\n\nTo address these limitations, the report suggests several areas for future improvement:\n\n1.  **More Extensive Pre-training**: Conduct more extensive pre-training to improve overall performance.\n2.  **High-Quality Labeled Data**: Increase the amount of high-quality labeled data to refine visual style and quality.\n3.  **Higher Resolution Output**: Increase the output video resolution to enhance visual appeal."
    },
    {
        "question_id": "2502.10248v1_9",
        "answer": "Human feedback plays a crucial role in refining **Step-Video-T2V's** video generation by improving visual quality and ensuring better alignment with user prompts. Here's how it's implemented in the training pipeline:\n\n***\n\n### Incorporation of Human Feedback\n\n1.  **Direct Preference Optimization (DPO):**\n\n    *   **Selection of DPO:** **Step-Video-T2V** uses **DPO** to incorporate human feedback, which has been effective in various generation tasks due to its simplicity and intuitiveness.\n    *   **Objective:** The goal is to align the model with the generation of preferred data while avoiding non-preferred data, based on human preference data.\n    *   **Reference Policy:** A reference policy (reference model) is used to stabilize training and prevent the current policy from deviating too far.\n    *   The policy objective is formulated as:\n\n        $L_{DPO} = -E_{(y, x_w, x_l) \\sim D} \\left[ log \\sigma \\left( \beta \\left( log \frac{\\pi_\theta (x_w|y)}{\\pi_{ref}(x_w|y)} - log \frac{\\pi_\theta (x_l|y)}{\\pi_{ref}(x_l|y)} \right) \right) \right]$\n\n        where:\n\n        *   $\\pi_\theta$ and $\\pi_{ref}$ refer to the current policy and reference policy, respectively.\n        *   $x_w$ and $x_l$ are the preferred and non-preferred samples.\n        *   $y$ denotes the condition.\n\n2.  **Data Collection for Training:**\n\n    *   **Prompt Set Construction:** A diverse prompt set is created by randomly selecting prompts from training data and synthesizing prompts based on real-world user interaction patterns.\n    *   **Video Generation and Annotation:** For each prompt, **Step-Video-T2V** generates multiple videos using different seeds. Human annotators then rate the preference of these samples.\n    *   **Quality Control:** The annotation process is monitored to ensure accuracy and consistency, resulting in preference and non-preference data for model training.\n    *   **Sample Alignment:** Positive and negative samples are aligned by fixing the initial noise and timestep to maintain consistency and improve training stability.\n\n3.  **Training Objective:**\n\n    *   The training objective is based on the **DiffusionDPO** method and **DPO**, modified for the **Flow Matching** framework.\n    *   The gradient of the **DPO** loss is:\n\n        $\frac{\\partial L_{DPO}}{\\partial \theta} \\propto -\beta(1 - \\sigma(\beta z)) \\cdot \frac{\\partial z}{\\partial \theta}$\n\n        where $z$ represents policy-related terms.\n    *   To address potential gradient explosion issues with large $\beta$, the value of $\beta$ is reduced and the learning rate is increased, leading to faster convergence.\n\n4.  **Reward Model Integration:**\n\n    *   **Addressing Saturation:** To address the saturation of improvements when the model easily distinguishes between positive and negative samples, a reward model is trained using human-annotated feedback data.\n    *   **Dynamic Quality Evaluation:** The reward model dynamically evaluates the quality of newly generated samples during training.\n    *   **Periodic Fine-Tuning:** The reward model is periodically fine-tuned with newly annotated human feedback to maintain alignment with the evolving policy.\n    *   **On-Policy Scoring:** By integrating the reward model, training data is scored and ranked on-the-fly (on-policy), improving data efficiency.\n\n***"
    }
]