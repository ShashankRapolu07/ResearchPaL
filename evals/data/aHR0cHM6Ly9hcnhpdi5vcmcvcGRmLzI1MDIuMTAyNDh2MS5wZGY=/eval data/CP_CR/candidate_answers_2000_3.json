[
    {
        "question_id": "2502.10248v1_0",
        "answer": "The paper defines two distinct levels for video foundation models, each characterized by its capabilities and potential applications.\n\n***\n\n### Level 1: Translational Video Foundation Model\n\n*   Focus: Cross-modal translation.\n*   Functionality: Generates videos from various inputs such as text, images, or multimodal contexts.\n*   Capabilities: Acts as a system that converts one type of input (e.g., text) into a video output.\n*   Examples: Current diffusion-based text-to-video models like Sora, Veo, and Step-Video-T2V.\n\n### Level 2: Predictable Video Foundation Model\n\n*   Focus: Prediction and reasoning.\n*   Functionality: Forecasts future events based on text, visual, or multimodal inputs.\n*   Capabilities:\n    *   Goes beyond simple translation to understand and predict dynamic scenarios.\n    *   Handles advanced tasks like reasoning with multimodal data.\n    *   Simulates real-world scenarios.\n*   Analogy: Similar to large language models (LLMs) but for video.\n*   Limitations of Level 1 Models:\n    *   Struggle with complex action sequences (e.g., gymnastics).\n    *   Difficulty adhering to the laws of physics (e.g., bouncing ball).\n    *   Inability to perform causal or logical tasks.\n\n### Key Differences\n\n*   **Function:** Level 1 models primarily translate inputs into video, while Level 2 models predict future events and simulate scenarios.\n*   **Complexity:** Level 2 requires a deeper understanding of causality and physics compared to Level 1.\n*   **Capabilities:** Level 2 models can perform reasoning and handle more advanced tasks than Level 1.\n*   **Causal Modeling:** Level 2 models incorporate causal modeling mechanisms, unlike many Level 1 models that only map inputs to outputs."
    },
    {
        "question_id": "2502.10248v1_1",
        "answer": "Step-Video-T2V uses a specialized **Video-VAE** architecture to achieve high spatial and temporal compression while maintaining video reconstruction quality. Here's a breakdown of the key components and techniques:\n\n***\n\n### Key Components and Techniques\n\n1.  **High Compression Ratios**: The Video-VAE achieves a **16x16 spatial and 8x temporal compression ratio**. This means it significantly reduces the data volume of the video, making subsequent processing more efficient.\n\n2.  **Dual-Path Architecture**:\n    *   The architecture incorporates a novel dual-path design in the later stages of the encoder and the early stages of the decoder. This design facilitates unified spatial-temporal compression.\n    *   One path focuses on preserving high-frequency details through convolutional processing.\n    *   The other path maintains low-frequency structure via channel averaging.\n\n3.  **Causal 3D Convolutional Modules**:\n    *   The encoder's early stages use causal 3D convolutional modules, ensuring that each frame's processing depends only on previous frames, thus maintaining temporal coherence.\n    *   The temporal causality is implemented using the following equation:\n\n        $C3D(X)_t = \begin{cases}\n        Conv3D([0, ..., X_t], \\Theta) & t = 0 \\\n        Conv3D([X_{t-k}, ..., X_t], \\Theta) & t > 0\n        \\end{cases}$\n\n        where $k$ is the temporal kernel size, ensuring frame $t$ only depends on previous frames.\n\n4.  **Dual-Path Latent Fusion**:\n    *   **Conv Path**: Combines causal 3D convolutions with pixel unshuffling:\n\n        $H_{conv} = U^{(3)}_s(C3D(X))$\n\n        where $U^{(3)}_s : R^{B \times C \times T_{st} \times H_{ss} \times W_{ss}} \rightarrow R^{B \times C \\cdot s^3 \times \frac{T}{s_t} \times \frac{H}{s_s} \times \frac{W}{s_s}}$ with spatial stride $s_s = 2$, temporal stride $s_t = 2$, and $C3D$ denoting causal 3D convolution.\n\n    *   **Shortcut Path**: Preserves structural semantics through grouped channel averaging:\n\n        $H_{avg} = \frac{1}{s^3} \\sum_{k=0}^{s^3 - 1} U^{(3)}_s (X)[\\dots, kC_z : (k+1)C_z]$\n\n        where $U^{(3)}_s$ implements 3D pixel unshuffle with spatial-temporal blocking, and $C_z$ is the latent dimension of the next stage.\n\n    *   The output of fusion combines both paths through residual summation:\n\n        $Z = H_{conv} \\oplus H_{avg}$\n\n5.  **Decoder Architecture**:\n    *   The decoder mirrors the encoder's dual-path architecture, using 3D pixel shuffle operations to unfold compressed information into spatial-temporal dimensions.\n    *   Spatial group normalization replaces group normalization to prevent temporal flickering between different chunks.\n\n6.  **Multi-Stage Training Process**:\n    *   **Stage 1**: Trains a VAE with a 4x8x8 compression ratio without a dual-path structure, focusing on learning low-level representations jointly on images and videos.\n    *   **Stage 2**: Incorporates dual-path modules in both the encoder and decoder, gradually unfreezing these modules along with the mid-block and ResNet backbone for refined training.\n\n7.  **Loss Functions**:\n    *   A combination of **L1 reconstruction loss**, **Video-LPIPS**, and **KL-divergence** is used to guide the model.\n    *   **GAN loss** is introduced after these losses converge to further refine the model\u2019s performance.\n\n***"
    },
    {
        "question_id": "2502.10248v1_2",
        "answer": "Direct Preference Optimization (DPO) is a technique used to align a model's behavior with human preferences, particularly in generation tasks. It's designed to simplify the process of incorporating human feedback into model training, making it more efficient and stable compared to traditional methods like Reinforcement Learning from Human Feedback (**RLHF**).\n\n***\n\nHere\u2019s how **DPO** works and how it's applied in **Step-Video-T2V** to enhance visual quality:\n\n### Core Idea of DPO\n\n**DPO** aims to directly optimize the model's policy (i.e., its generation behavior) based on preference data. Instead of using a reward model to predict human preferences and then training the model to maximize this reward, **DPO** directly learns from comparisons between preferred and non-preferred samples.\n\n1.  **Preference Data**: The process begins with a dataset of preference pairs. For a given input (e.g., a text prompt), there are two generated outputs: one that humans prefer and another that they don't.\n2.  **Objective Function**: **DPO** formulates an objective function that encourages the model to assign higher likelihood to the preferred outputs and lower likelihood to the non-preferred outputs. This objective is derived from the Bradley-Terry model of pairwise comparisons, which relates the probability of preferring one sample over another to the difference in their \"reward\" or \"value\".\n3.  **Training Process**: The model is trained to minimize the **DPO** loss, which effectively adjusts the model's parameters to align with human preferences. A reference policy is often used to prevent the updated policy from deviating too far from the initial model, ensuring training stability.\n\n***\n\n### Application in Step-Video-T2V\n\nIn **Step-Video-T2V**, **DPO** is used to improve the visual quality of the generated videos by incorporating human feedback.\n\n1.  **Data Collection**: A diverse set of prompts is created, and for each prompt, the **Step-Video-T2V** model generates multiple videos. Human annotators then rate these videos, indicating their preferences. The annotation process is monitored to ensure quality and consistency, resulting in a dataset of preferred and non-preferred video pairs.\n2.  **Training with DPO**: The **Step-Video-T2V** model is trained using the **DPO** loss function, with the preferred videos serving as positive examples and the non-preferred videos as negative examples. The model adjusts its parameters to generate videos that are more aligned with human preferences.\n3.  **Maintaining Consistency**: To ensure stable training, the positive and negative video samples are aligned by fixing the initial noise and timestep. This helps to maintain consistency in the training data and improve overall training stability.\n4.  **Addressing Limitations**: The paper identifies that improvements from human feedback tend to diminish when the model becomes proficient at differentiating between positive and negative samples. This is because the training data is generated by earlier versions of the model, which may no longer align with the current policy. To address this, the paper proposes training a reward model using human-annotated feedback data. This reward model dynamically evaluates the quality of newly generated samples during training, improving data efficiency by scoring and ranking training data on-the-fly.\n\n***\n\n### Benefits and Results\n\nThe integration of **DPO** in **Step-Video-T2V** leads to several benefits:\n\n*   **Improved Visual Quality**: Videos generated with **DPO** are more visually appealing and coherent, aligning better with human expectations.\n*   **Enhanced Prompt Alignment**: The generated videos are more accurate and relevant to the given text prompts.\n*   **Increased User Preference**: Evaluations show that videos generated with **DPO** are preferred by users compared to those generated by the baseline model. In the paper's experiments, the baseline model with **DPO** achieved a preference score of 55%, outperforming the baseline model (45%).\n\nBy directly optimizing for human preferences, **DPO** enables **Step-Video-T2V** to produce higher-quality videos that are more aligned with user expectations, enhancing the overall performance and usability of the model."
    },
    {
        "question_id": "2502.10248v1_3",
        "answer": "The Step-Video-T2V model leverages a **Diffusion Transformer (DiT)** architecture with **3D full attention**, offering several key advantages for video generation:\n\n*   **Superior Modeling of Spatial and Temporal Information**: **3D full attention** combines spatial and temporal information in a unified attention process. This allows the model to capture intricate relationships between objects and their movements across frames, leading to more coherent and realistic video generation.\n*   **High Motion Dynamics**: The **3D full attention** mechanism facilitates the generation of videos with complex motion. By jointly considering spatial and temporal dimensions, the model can better represent and synthesize dynamic scenes.\n*   **Text Prompt Integration**: The model incorporates a cross-attention layer between the self-attention and feed-forward network (FFN) in each transformer block to incorporate text prompts. The outputs from two distinct bilingual text encoders, Hunyuan-CLIP and Step-LLM, are concatenated along the sequence dimension, creating the final text embedding sequence, allowing the model to generate videos conditioned on the input prompt.\n*   **Adaptive Layer Normalization (AdaLN)**: By using AdaLN, the model can adaptively normalize the features based on timestep information. This helps in stabilizing the diffusion process and improving the overall quality of the generated videos. The use of AdaLN-Single further reduces the computational overhead of traditional AdaLN operations and improve overall model efficiency.\n*   **Positional Encoding**: By using RoPE-3D, the model can effectively handle video inputs with varying lengths and resolutions. It improves generalization across diverse video data and effectively captures both spatial and temporal relationships within the video."
    },
    {
        "question_id": "2502.10248v1_4",
        "answer": "Step-Video-T2V uses two bilingual text encoders, Hunyuan-CLIP and Step-LLM, to process user text prompts in both English and Chinese.\n\n***\n\n### Functionality of Each Encoder\n\n*   **Hunyuan-CLIP**: This is a bidirectional text encoder that is part of an open-source bilingual CLIP model. It produces text representations that align well with the visual space due to the training mechanism of the CLIP model. However, it is limited to processing a maximum of 77 tokens, which poses challenges when handling longer user prompts.\n*   **Step-LLM**: This is an in-house, unidirectional bilingual text encoder pre-trained using the next-token prediction task. It incorporates a redesigned Alibi-Positional Embedding to improve efficiency and accuracy in sequence processing. Unlike Hunyuan-CLIP, Step-LLM has no input length restriction, making it effective for handling lengthy and complex text sequences.\n\n***\n\n### Advantages of Using Two Separate Text Encoders\n\n1.  **Handling Varying Prompt Lengths**: By combining these two text encoders, Step-Video-T2V can handle user prompts of varying lengths. Hunyuan-CLIP is suitable for shorter prompts, while Step-LLM is used for longer, more complex text sequences.\n2.  **Robust Text Representations**: The two encoders generate robust text representations that effectively guide the model in the latent space, allowing the model to generate videos conditioned on the input prompt.\n3.  **Improved Efficiency and Accuracy**: Step-LLM's redesigned Alibi-Positional Embedding improves both efficiency and accuracy in sequence processing, making it particularly effective for handling lengthy and complex text sequences."
    },
    {
        "question_id": "2502.10248v1_5",
        "answer": "Here's a breakdown of the challenges faced by current diffusion-based text-to-video models and how Step-Video-T2V attempts to address them:\n\n***\n\n### Challenges of Current Diffusion-Based Text-to-Video Models\n\n1.  **Instruction Following**:\n\n    *   Difficulty in generating videos that involve complex action sequences.\n    *   Struggles in incorporating multiple concepts with low occurrence in the training data within a single generated video.\n    *   Attention distribution is occasionally highly concentrated, leading to missing objects, wrong details, or incomplete action sequences.\n\n2.  **Adherence to the Laws of Physics**:\n\n    *   Inability to accurately simulate the real world and generate videos that adhere to the laws of physics (e.g., a ball bouncing on the floor).\n    *   Successes are often due to the model over-fitting to specific annotations and cannot generalize well.\n\n3.  **Data Quality and Annotation**:\n\n    *   Existing **video captioning models** often struggle with **hallucination issues**.\n    *   High-quality labeled data is difficult and expensive to obtain.\n\n4.  **Computational Cost**:\n\n    *   Training and generating long-duration, high-resolution videos still face significant computational cost hurdles.\n\n***\n\n### How Step-Video-T2V Attempts to Address These Challenges\n\n1.  **High-Compression Video-VAE**:\n\n    *   Employs a deep compression **Video-VAE**, achieving **16x16 spatial** and **8x temporal compression ratios**.\n    *   Reduces the computational complexity of large-scale video generation training.\n\n2.  **Bilingual Text Encoders**:\n\n    *   Uses two bilingual text encoders to handle both English and Chinese prompts.\n    *   Enables the model to directly understand prompts in either language.\n\n3.  **Cascaded Training Pipeline**:\n\n    *   Introduces a cascaded training pipeline, including **text-to-image pre-training**, **text-to-video pre-training**, **supervised fine-tuning (SFT)**, and **direct preference optimization (DPO)**.\n    *   Aims to accelerate model convergence and fully leverage video datasets of varying quality.\n\n4.  **Video-Based DPO**:\n\n    *   Applies a video-based **DPO** approach to reduce artifacts and improve the visual quality of the generated videos.\n\n5.  **Text-to-Image Pre-training**:\n\n    *   Uses text-to-image pre-training to help the video generation model acquire rich visual knowledge, including concepts, scenes, and their spatial relationships.\n    *   Provides a solid foundation for the subsequent text-to-video pre-training stages.\n\n6.  **Text-to-Video Pre-training at Low Resolution**:\n\n    *   Performs text-to-video pre-training at low resolution to enable the model to learn motion dynamics.\n\n***"
    },
    {
        "question_id": "2502.10248v1_6",
        "answer": "The Step-Video-T2V model employs **Flow Matching** as a training objective, which involves predicting the velocity field that transforms noise into a data sample. Here's a breakdown:\n\n***\n\n### Flow Matching in Step-Video-T2V:\n\n1.  **Noise Sampling**: The process starts by sampling a Gaussian noise, denoted as $X_0$, from a standard normal distribution $N(0, 1)$.\n2.  **Timestep Sampling**: A random timestep $t$ is sampled from a uniform distribution between 0 and 1 (i.e., $t \\in [0, 1]$).\n3.  **Interpolation**: The model input $X_t$ is constructed as a linear interpolation between the initial noise $X_0$ and the target sample $X_1$ (the noise-free input):\n    $$\n    X_t = (1 - t) \\cdot X_0 + t \\cdot X_1\n    $$\n4.  **Velocity Calculation**: The ground truth velocity $V_t$ is defined as the rate of change of $X_t$ with respect to the timestep $t$:\n    $$\n    V_t = \frac{dX_t}{dt} = X_1 - X_0\n    $$\n    This represents the direction and magnitude of change needed to transform the noise $X_0$ into the data sample $X_1$.\n5.  **Loss Minimization**: The model is trained to predict the velocity field $u(X_t, y, t; \theta)$ at timestep $t$, given the input $X_t$, an optional conditioning input $y$ (e.g., text prompt), and model parameters $\theta$. The training objective is to minimize the **mean squared error (MSE)** loss between the predicted velocity and the ground truth velocity:\n    $$\n    \text{loss} = E_{t, X_0, X_1, y} \\left[ \\| u(X_t, y, t; \theta) - V_t \\|^2 \right]\n    $$\n\n***\n\n### Benefits of Flow Matching for Video Generation:\n\n1.  **Stable Training**: Flow matching provides a more stable training process compared to other generative modeling techniques. By directly learning the velocity field, the model avoids issues associated with adversarial training or complex sampling procedures.\n2.  **High-Quality Samples**: Models trained with flow matching can generate high-quality and diverse video samples. The continuous nature of the flow allows for smoother transitions and more realistic motion dynamics in the generated videos.\n3.  **Flexibility**: Flow matching is flexible and can be adapted to various types of data and model architectures. In the context of Step-Video-T2V, it is integrated with a **diffusion transformer (DiT)** architecture to leverage the strengths of both approaches.\n4.  **Efficiency**: The method facilitates efficient inference. By using an **ODE-based method** during inference, the model can iteratively refine the noise to generate the final video frames.\n5.  **Integration with DPO**: Flow matching can be effectively combined with techniques like **Direct Preference Optimization (DPO)** to further improve the visual quality and alignment with user preferences."
    },
    {
        "question_id": "2502.10248v1_7",
        "answer": "The Step-Video-T2V model employs a hierarchical data filtering approach during its pre-training and post-training stages to ensure the use of high-quality data, which is crucial for achieving superior video generation performance. This process involves progressively applying filters with increasing thresholds to create subsets of the pre-training data. A final, manually filtered dataset is used for supervised fine-tuning (SFT).\n\n***\n\nHere's a breakdown:\n\n1.  **Pre-training Data Filtering**:\n    *   The paper mentions the creation of six pre-training subsets during the Step-2: T2VI pre-training phase. This indicates multiple layers of automated filtering.\n    *   These filters likely involve metrics such as **CLIP Score** (to ensure video-text alignment) and video assessment scores (measuring aesthetics and quality).\n    *   The thresholds for these filters are progressively increased, creating subsets with increasingly higher quality.\n\n***\n\n2.  **Post-training Data Filtering (SFT)**:\n\n    *   **Automated Filtering**: Utilizes video assessment scores and heuristic rules to filter the dataset, significantly improving its overall quality. It also filters videos within the same cluster based on their \"Distance to Centroid\" values, removing outliers to maintain diversity within the subset.\n    *   **Manual Filtering**: Human evaluators assess each video for clarity, aesthetics, motion, scene transitions, and the absence of watermarks or subtitles. Captions are manually refined for accuracy, including details about camera movements, subjects, actions, backgrounds, and lighting.\n\n***\n\nImportance of Hierarchical Data Filtering:\n\n*   **Improved Data Quality**: By filtering data through multiple stages, the model is trained on progressively cleaner and more relevant data. This reduces noise and inconsistencies, allowing the model to learn more effectively.\n*   **Enhanced Model Stability**: High-quality data leads to more stable training. The model is less likely to be thrown off by outliers or poorly aligned video-text pairs.\n*   **Better Generalization**: Training on a diverse yet high-quality dataset allows the model to generalize better to unseen data, improving its ability to generate realistic and coherent videos from various text prompts.\n*   **Reduced Artifacts and Improved Visual Quality**: The DPO stage, combined with high-quality data, reduces artifacts and ensures smoother, more realistic video outputs.\n*   **Efficient Resource Allocation**: By identifying and filtering out low-quality data early on, computational resources are focused on training with the most valuable data, leading to faster convergence and better results.\n\n***"
    },
    {
        "question_id": "2502.10248v1_8",
        "answer": "Based on the provided document, Step-Video-T2V is benchmarked against two leading commercial text-to-video engines, T2VTopA and T2VTopB, but not directly against Sora or Veo. However, we can infer some comparisons based on the information given.\n\n***\n\n### Performance Evaluation\n\n*   **Metrics Used**: The evaluation uses two primary metrics:\n\n    *   **Metric-1**: Win/Tie/Loss rating by human annotators comparing Step-Video-T2V against target models.\n    *   **Metric-2**: Score-based evaluation across four dimensions: instruction following, motion smoothness, physical plausibility, and aesthetic appeal.\n*   **Comparison with T2VTopA and T2VTopB**:\n\n    *   In overall ranking using **Metric-1**, Step-Video-T2V is outperformed by T2VTopA but performs better than T2VTopB.\n    *   T2VTopA and T2VTopB are rated higher in aesthetic appeal, likely due to their higher video resolutions (720P and 1080P, respectively) and high-quality aesthetic data used during post-training. Step-Video-T2V has a resolution of 540P.\n    *   Step-Video-T2V outperforms both T2VTopA and T2VTopB in the Sports category, demonstrating strong capabilities in modeling and generating videos with high-motion dynamics.\n    *   T2VTopA exhibits better instruction-following capability, contributing to its superior performance in categories such as Combined Concepts, Surreal, and Cinematography. This is attributed to a better video captioning model and greater human effort in labeling the post-training data.\n\n***\n\n### Key Differentiators and Advantages of Step-Video-T2V\n\n*   **Motion Dynamics**: Step-Video-T2V excels in generating videos with high-motion dynamics, outperforming the compared commercial engines in this aspect.\n*   **Video Length**: Step-Video-T2V can generate videos up to 204 frames, nearly twice the length of T2VTopA and T2VTopB, which makes the training more challenging.\n*   **Compression Rate**: The model uses a deep compression **Video-VAE**, achieving 16x16 spatial and 8x temporal compression ratios, which is better than HunyuanVideo\u2019s 8x8 spatial and 4x temporal compression.\n\n***\n\n### Limitations and Challenges\n\n*   **Aesthetic Appeal**: Step-Video-T2V lags behind T2VTopA and T2VTopB in aesthetic appeal, primarily due to lower video resolution and less high-quality data in the post-training phase.\n*   **Instruction Following**: While strong, the instruction-following capability is not as robust as T2VTopA, impacting performance in complex conceptual and cinematic categories.\n*   **Training Data**: The model still requires more training with high-resolution videos and high-quality data in the post-training phase.\n*   **Generalization**: Like other **DiT**-based models, Step-Video-T2V struggles to generalize well when generating videos involving complex action sequences or requiring adherence to the laws of physics.\n\n***\n\n### Inferences About Sora and Veo\n\nWhile direct comparisons aren't available, we can make some inferences:\n\n*   **Sora and Veo's Strengths**: Given that T2VTopA and T2VTopB emphasize aesthetic appeal and instruction following, it's likely that Sora and Veo, as leading commercial models, also prioritize these aspects. Their high visual fidelity and contextual understanding are probably superior to Step-Video-T2V.\n*   **Motion Dynamics**: Step-Video-T2V's strength in motion dynamics suggests it could be competitive in specific applications requiring realistic movement, but overall, Sora and Veo likely offer more comprehensive motion handling.\n*   **Generalization**: The paper mentions that Step-Video-T2V struggles with complex physics and action sequences, a limitation likely shared by many current models, including potentially earlier versions of Sora and Veo. However, newer iterations of Sora and Veo may have made advancements in these areas.\n\n***\n\n### Summary\n\nStep-Video-T2V stands out as a strong open-source model with excellent motion dynamics and the ability to generate longer videos. However, it lags behind leading commercial engines like T2VTopA and T2VTopB (and, by extension, likely Sora and Veo) in aesthetic appeal and instruction following. The ongoing challenges highlight the need for more high-quality training data, better video captioning models, and advancements in handling complex physics and action sequences."
    },
    {
        "question_id": "2502.10248v1_9",
        "answer": "Human feedback plays a crucial role in refining the video generation quality of Step-Video-T2V by aligning the model's output with human preferences. This is achieved through a carefully designed pipeline that incorporates **Direct Preference Optimization (DPO)**.\n\n***\n\nHere\u2019s a breakdown of how human feedback is implemented in the training pipeline:\n\n1.  **Data Collection with Diverse Prompts**:\n    *   A diverse set of prompts is constructed by randomly selecting from the training data and inviting human annotators to create new prompts based on real-world user interaction patterns.\n    *   The model generates multiple videos for each prompt using different seeds.\n2.  **Human Annotation**:\n    *   Human annotators rate the generated videos, indicating their preferences.\n    *   Quality control personnel monitor the annotation process to ensure accuracy and consistency.\n    *   This results in a dataset of preference and non-preference data.\n3.  **DPO Training**:\n    *   **DPO** is used to adjust the model to align with the generation of preferred data while avoiding non-preferred data.\n    *   A reference policy (reference model) is introduced to stabilize training and prevent the current policy from deviating too far.\n    *   The training objective can be formulated as:\n\n        $L_{DPO} = -E_{(y, x_w, x_l) \\sim D} \\left[ log \\ \\sigma \\left( \beta \\left( log \\ \frac{\\pi_\theta(x_w|y)}{\\pi_{ref}(x_w|y)} - log \\ \frac{\\pi_\theta(x_l|y)}{\\pi_{ref}(x_l|y)} \right) \right) \right]$\n\n        where $\\pi_\theta$ and $\\pi_{ref}$ refer to the current and reference policies, respectively, $x_w$ and $x_l$ are the preferred and non-preferred samples, and $y$ denotes the condition.\n4.  **On-Policy Reward Model Training**:\n    *   A reward model is trained using human-annotated feedback data to evaluate the quality of newly generated samples dynamically.\n    *   The reward model is periodically fine-tuned with newly annotated human feedback to maintain alignment with the evolving policy.\n    *   This model scores and ranks training data on-the-fly (on-policy), improving data efficiency.\n5.  **Training Objective Modification**:\n    *   The training objective is based on the **DiffusionDPO** method and **DPO**, with slight modifications to extend it to the **Flow Matching** framework.\n    *   The derivative of the **DPO** loss with respect to the model parameters is:\n\n        $\frac{\\partial L_{DPO}}{\\partial \theta} \\propto -\beta(1 - \\sigma(\beta z)) \\cdot \frac{\\partial z}{\\partial \theta}$\n\n        where $z$ represents the policy-related terms.\n    *   To address potential gradient explosion issues, the value of $\beta$ is reduced, and the learning rate is increased to achieve faster convergence."
    }
]