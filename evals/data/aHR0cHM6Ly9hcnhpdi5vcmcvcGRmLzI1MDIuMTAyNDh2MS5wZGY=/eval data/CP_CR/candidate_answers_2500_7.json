[
    {
        "question_id": "2502.10248v1_0",
        "answer": "The paper defines two levels of video foundation models, each representing a different stage of capability and complexity.\n\n***\n\n### Level 1: Translational Video Foundation Model\n\n*   This type of model primarily functions as a cross-modal translation system.\n*   It can generate videos from various inputs such as text, visual data, or a combination of both (multimodal context).\n*   Models at this level are designed to map input data from one modality (e.g., text) to another (video).\n*   Current diffusion-based text-to-video models, including Step-Video-T2V and others like Sora, Veo, Kling, and Hailuo, fall into this category.\n*   These models excel at producing high-quality videos from text prompts, which simplifies video content creation.\n\n***\n\n### Level 2: Predictable Video Foundation Model\n\n*   This model acts as a prediction system, similar to large language models (LLMs).\n*   It can forecast future events based on text, visual, or multimodal inputs.\n*   Level 2 models are designed to handle advanced tasks that require reasoning with multimodal data and simulating real-world scenarios.\n*   These models aim to understand and predict causal relationships within videos, enabling them to perform tasks that require logical reasoning and an understanding of physics.\n*   These models can forecast future events based on text, visual, or multimodal context.\n\n***\n\n### Key Differences\n\nThe primary distinction between the two levels lies in their ability to understand and predict underlying relationships within videos, rather than merely translating inputs into video outputs. Level 1 models focus on generating videos from given prompts, while Level 2 models aim to forecast and reason about video content, similar to how LLMs handle language.\n\n***\n"
    },
    {
        "question_id": "2502.10248v1_1",
        "answer": "Step-Video-T2V's **Video-VAE** achieves high spatial and temporal compression while maintaining video reconstruction quality through a combination of architectural innovations and a multi-stage training process.\n\n***\n\n### Architectural Innovations\n\n1.  **Dual-Path Architecture**:\n    *   The **Video-VAE** incorporates a novel dual-path architecture in the later stages of the encoder and the early stages of the decoder. This design facilitates unified spatial-temporal compression.\n    *   One path (**Conv Path**) uses causal 3D convolutions with pixel unshuffling to maintain high-frequency details.\n    *   The other path (**Shortcut Path**) preserves structural semantics through grouped channel averaging.\n    *   These paths are combined through residual summation, allowing the network to efficiently use its parameters and reduce blurring artifacts.\n\n2.  **Causal 3D Convolutional Modules**:\n    *   The encoder's early stage features three stages, each with two Causal **Res3DBlock** and downsample layers. A **MidBlock** combines convolutional layers with attention mechanisms to refine compressed representations.\n    *   Temporal causality is implemented using causal 3D convolution, ensuring each frame depends only on previous frames. The causal 3D convolution is defined as:\n        $C3D(X)_t = \begin{cases}\n        Conv3D([0, ..., X_t], \\Theta) & t = 0 \\\n        Conv3D([X_{t-k}, ..., X_t], \\Theta) & t > 0\n        \\end{cases}$\n        where $k$ is the temporal kernel size.\n\n3.  **Pixel Shuffling and Unshuffling**:\n    *   The architecture uses 3D pixel unshuffle operations in the encoder and 3D pixel shuffle operations in the decoder. These operations efficiently unfold compressed information into spatial-temporal dimensions.\n\n4.  **Spatial Group Normalization**:\n    *   In the **ResNet** backbone, group normalization is replaced with spatial group normalization to avoid temporal flickering between different chunks.\n\n***\n\n### Multi-Stage Training Process\n\n1.  **Initial Training**:\n    *   A **VAE** with a 4x8x8 compression ratio is initially trained without a dual-path structure. This training is conducted jointly on images and videos with varying frame counts, adhering to a preset ratio. This stage allows the model to sufficiently learn low-level representations.\n\n2.  **Dual-Path Incorporation**:\n    *   The model is enhanced by incorporating two dual-path modules in both the encoder and decoder, replacing the latter part after the mid-block.\n    *   During this phase, the dual-path modules, the mid-block, and the **ResNet** backbone are gradually unfrozen, allowing for a refined and flexible training process.\n\n3.  **Loss Functions**:\n    *   Throughout the training, a combination of **L1** reconstruction loss, **Video-LPIPS**, and **KL-divergence** constrain are used to guide the model.\n    *   Once these losses converge, **GAN** loss is introduced to further refine the model\u2019s performance.\n\nThis staged approach ensures a robust **VAE** capable of handling complex video data efficiently, achieving high compression ratios while maintaining state-of-the-art reconstruction quality."
    },
    {
        "question_id": "2502.10248v1_2",
        "answer": "Direct Preference Optimization (DPO) is a technique used to improve the visual quality of videos generated by Step-Video-T2V. Here's a breakdown:\n\n***\n\n### What is DPO?\n\nDPO is a method for aligning a model with human preferences. Instead of using reinforcement learning, which can be complex and unstable, DPO directly optimizes the model based on which video samples humans prefer. It aims to adjust the model to generate preferred data while avoiding non-preferred data.\n\n### How DPO Works in Step-Video-T2V:\n\n1.  **Data Collection**:\n    *   Diverse prompts are created, partly from training data and partly synthesized by human annotators.\n    *   For each prompt, Step-Video-T2V generates multiple videos using different seeds.\n    *   Human annotators rate the preference between these samples, indicating preferred and non-preferred videos.\n2.  **Training Objective**:\n    *   DPO adjusts the model to align with the generation of preferred data while avoiding the generation of non-preferred data.\n    *   A reference policy (reference model) is introduced to stabilize training and prevent the current policy from deviating too far.\n    *   The policy objective is formulated as:\n        $LDPO = \u2212E(y,xw,xl)\u223cD\n        [log \u03c3[\u03b2[log \frac{\u03c0\u03b8(xw|y)}{\u03c0ref(xw|y)} \u2212log \frac{\u03c0\u03b8(xl|y)}{\u03c0ref(xl|y)}]]]$\n        where:\n        *   $\u03c0\u03b8$ and $\u03c0ref$ refer to the current policy and reference policy, respectively.\n        *   $xw$ and $xl$ are the preferred and non-preferred samples.\n        *   $y$ denotes the condition.\n3.  **Implementation Details**:\n    *   **Sample Generation**:\n        *   The model generates samples, ensuring smooth updates and improving training stability.\n        *   Positive and negative samples are aligned by fixing the initial noise and timestep to maintain consistency.\n    *   **Gradient Handling**:\n        *   To address potential gradient explosion issues, the parameter $\beta$ is reduced, and the learning rate is increased for faster convergence.\n\n### Benefits and Observations:\n\n*   **Improved Visual Quality**: Human feedback enhances the plausibility and consistency of generated videos.\n*   **Better Alignment**: DPO enhances alignment with given prompts, resulting in more accurate and relevant video generation.\n*   **Preference Score**: The baseline model with DPO achieves a higher preference score (55%) compared to the baseline model (45%).\n\n### Limitations and Future Directions:\n\n*   **Implicit Dynamic Conditions**: The trajectory from initial noise to timestep-specific latents acts as implicit dynamic conditions, which are underutilized due to computational limits.\n*   **Tradeoff in Feedback**: There is a tradeoff between sparse and imprecise feedback, especially in high-resolution videos where only a few pixels may be problematic.\n*   **Optimization Efficiency**: Unlike LLMs that use token-level softmax, diffusion models rely on regression, which may result in less efficient preference optimization."
    },
    {
        "question_id": "2502.10248v1_3",
        "answer": "Here's a breakdown of the advantages of using a **DiT** with 3D full attention in **Step-Video-T2V**, based on the information provided in the paper:\n\n***\n\n### Advantages of DiT Architecture\n\n*   **Disentanglement of Text and Video:** **DiT** naturally separates the processing of text and video information. This allows for a cleaner model design and facilitates its extension to pure video prediction tasks (without text input).\n*   **Comparable Performance:** In initial training stages, **DiT** demonstrates similar performance to **MMDiT** (another Transformer architecture) while offering the benefits of disentanglement.\n*   **Natural Extension:** **DiT** can be naturally extended to pure video prediction models without text.\n\n***\n\n### Advantages of 3D Full Attention\n\n*   **Superior Performance:** Models using 3D full attention outperform those using spatial-temporal attention, especially in generating videos with high motion dynamics.\n*   **Unified Spatial and Temporal Information Processing:** 3D full attention combines spatial and temporal information within a single attention mechanism. This is theoretically more effective for modeling video data.\n*   **Smooth and Consistent Motion:** Large-scale experiments have shown that 3D full attention leads to the generation of videos with smoother and more consistent motion.\n\n***\n\n### Key Takeaways\n\nThe **DiT** architecture provides a solid foundation for **Step-Video-T2V** due to its ability to disentangle text and video processing, while 3D full attention enhances the model's capability to generate high-quality videos with complex motion."
    },
    {
        "question_id": "2502.10248v1_4",
        "answer": "Step-Video-T2V leverages a dual bilingual text encoder system to process user-provided text prompts in both English and Chinese. This design choice enhances the model's ability to understand and generate videos from a wider range of textual inputs.\n\nHere's a breakdown of how it works and the benefits:\n\n### Bilingual Text Encoding in Step-Video-T2V\n\n1.  **Dual Text Encoders:** The model employs two distinct text encoders:\n    *   Hunyuan-CLIP\n    *   Step-LLM\n2.  **Hunyuan-CLIP:** This encoder excels at producing text representations that are well-aligned with the visual space due to the CLIP model's training mechanism. However, it has a limitation of a maximum input length of 77 tokens, which can be restrictive for longer, more complex prompts.\n3.  **Step-LLM:** This is an in-house, unidirectional bilingual text encoder pre-trained using the next-token prediction task. It incorporates a redesigned Alibi-Positional Embedding to improve efficiency and accuracy in sequence processing. Unlike Hunyuan-CLIP, Step-LLM has no input length restriction, making it effective for handling lengthy and complex text sequences.\n4.  **Concatenation of Outputs:** The outputs from these two encoders are concatenated along the sequence dimension, creating the final text embedding sequence. This combined embedding is then injected into the cross-attention layer, allowing the model to generate videos conditioned on the input prompt.\n\n### Advantages of Using Two Separate Text Encoders\n\n1.  **Handling Varying Prompt Lengths:** By combining Hunyuan-CLIP and Step-LLM, Step-Video-T2V can effectively handle user prompts of varying lengths. Hunyuan-CLIP is suitable for shorter prompts, while Step-LLM can process longer, more detailed instructions.\n2.  **Robust Text Representations:** The dual-encoder approach ensures that the model generates robust text representations that effectively guide it in the latent space. Hunyuan-CLIP provides visually aligned representations, while Step-LLM offers flexibility in handling complex text sequences.\n3.  **Improved Understanding:** The model can better understand the nuances and details of the text prompt by leveraging the strengths of both encoders, leading to more accurate and relevant video generation.\n4.  **Bilingual Capability:** Both encoders are bilingual, enabling the model to process prompts in both English and Chinese, expanding its user base and application scenarios."
    },
    {
        "question_id": "2502.10248v1_5",
        "answer": "The paper identifies several key challenges in current diffusion-based text-to-video models and details how Step-Video-T2V attempts to mitigate these issues.\n\n***\n\n### Challenges in Diffusion-Based Text-to-Video Models\n\n1.  **Complex Action Sequences and Physics Adherence**:\n    *   **Challenge**: Existing models struggle with generating videos that involve complex action sequences (e.g., gymnastic performances) or adherence to the laws of physics (e.g., a bouncing basketball).\n    *   **Reasoning**: These models primarily learn mappings between text prompts and corresponding videos without explicitly modeling underlying causal relationships.\n\n2.  **Instruction Following**:\n    *   **Challenge**: Models often fail to accurately interpret and follow detailed instructions, especially when involving multiple concepts with low occurrence in the training data (e.g., an elephant and a penguin in the same video).\n    *   **Reasoning**: The distribution of cross-attention scores can be highly concentrated, causing the model to miss objects, details, or complete action sequences.\n\n3.  **High-Quality Labeled Data**:\n    *   **Challenge**: The availability of high-quality, accurately captioned video data is limited. Video captioning models often produce hallucinations, leading to unstable training and poor instruction-following performance.\n    *   **Reasoning**: Human annotation is expensive and difficult to scale, hindering the creation of comprehensive datasets.\n\n4.  **Computational Cost**:\n    *   **Challenge**: Training and generating long-duration, high-resolution videos require significant computational resources.\n    *   **Reasoning**: The computational complexity scales quadratically with the number of tokens due to attention operations.\n\n***\n\n### How Step-Video-T2V Addresses These Challenges\n\n1.  **Model Architecture and Training**:\n    *   **3D Full Attention**: Step-Video-T2V employs a **DiT** (Diffusion Transformer) architecture with **3D full attention**, designed to better capture both spatial and temporal information in videos, which is crucial for generating videos with smooth and consistent motion.\n    *   **Flow Matching**: The model is trained using **Flow Matching**, which helps in learning the continuous transformations from noise to video frames, potentially improving the coherence of generated sequences.\n\n2.  **Video Compression**:\n    *   **Video-VAE**: A deep compression **Variational Autoencoder (VAE)** is used to achieve **16x16 spatial and 8x temporal compression ratios**. This reduces the computational complexity of large-scale video generation training.\n\n3.  **Bilingual Text Encoders**:\n    *   **Hunyuan-CLIP and Step-LLM**: Two bilingual text encoders are used to handle both English and Chinese prompts. **Hunyuan-CLIP** aligns text representations with the visual space, while **Step-LLM** handles longer and more complex text sequences without input length restrictions.\n\n4.  **Training Strategies**:\n    *   **Cascaded Training Pipeline**: A multi-stage training pipeline is used, including text-to-image pre-training, text-to-video pre-training, supervised fine-tuning (**SFT**), and direct preference optimization (**DPO**). This approach accelerates model convergence and leverages video datasets of varying quality.\n    *   **Text-to-Image Pre-training**: This stage allows the model to acquire rich visual knowledge, providing a solid foundation for subsequent text-to-video pre-training.\n    *   **Video-Based DPO**: A video-based **DPO** approach is applied to reduce artifacts and improve the visual quality of the generated videos by aligning the model with human preferences.\n\n5.  **Data Handling**:\n    *   **Hierarchical Data Filtering**: A series of filters are applied to the data, progressively increasing their thresholds to create pre-training subsets. Manual filtering is used to construct the final **SFT** dataset.\n    *   **Bucketization**: Videos are grouped into buckets based on length and aspect ratio to accommodate varying video lengths and aspect ratios during training.\n\n***"
    },
    {
        "question_id": "2502.10248v1_6",
        "answer": "Step-Video-T2V utilizes Flow Matching as its training objective. Here's a breakdown of how it works and its benefits:\n\n### Flow Matching in Step-Video-T2V\n\n1.  **Noise Sampling**: During training, the process starts by sampling a Gaussian noise, denoted as $X_0$, from a standard normal distribution.\n\n2.  **Timestep Sampling**: A random timestep $t$ is sampled from a uniform distribution between 0 and 1 (i.e., $t \\in [0, 1]$).\n\n3.  **Linear Interpolation**: The model input $X_t$ is constructed as a linear interpolation between the initial noise $X_0$ and the target sample $X_1$ (the noise-free input). This is defined as:\n\n    $X_t = (1 - t) \\cdot X_0 + t \\cdot X_1$\n\n4.  **Ground Truth Velocity**: The ground truth velocity $V_t$ represents the rate of change of $X_t$ with respect to the timestep $t$, and is defined as:\n\n    $V_t = \frac{dX_t}{dt} = X_1 - X_0$\n\n    This captures the direction and magnitude of change from the initial noise $X_0$ to the target data $X_1$.\n\n5.  **Model Training**: The model is trained to predict the velocity $u(X_t, y, t; \theta)$ at timestep $t$, given the input $X_t$, an optional conditioning input $y$ (e.g., a bilingual sentence), and model parameters $\theta$. The training loss minimizes the Mean Squared Error (MSE) between the predicted velocity and the true velocity:\n\n    $\text{loss} = E_{t, X_0, X_1, y} [\\|u(X_t, y, t; \theta) - V_t\\|^2]$\n\n    The expectation is taken over all training samples, with $t$, $X_0$, $X_1$, and $y$ drawn from the dataset.\n\n6.  **Inference**: During inference, the process starts by sampling random noise $X_0 \\sim N(0, 1)$. The goal is to recover the denoised sample $X_1$ by iteratively refining the noise through an ODE-based method. A sequence of timesteps $\\{t_0, t_1, ..., t_n\\}$ is defined, where $t_0 = 0$, $t_n = 1$, and $t_0 < t_1 < ... < t_n$. The denoising process is carried out by integrating over these timesteps. The denoised sample $X_1$ can be expressed as:\n\n    $X_1 = \\sum_{i=0}^{n-1} u(X_{t_i}, y, t_i; \theta) \\cdot (t_{i+1} - t_i)$\n\n    where $u(X_{t_i}, y, t_i; \theta)$ represents the predicted velocity at timestep $t_i$, given the noisy sample $X_{t_i}$ and an optional conditioning input $y$.\n\n***\n\n### Benefits of Flow Matching for Video Generation\n\n1.  **Simplified Training**: Flow Matching simplifies the training process by directly learning the velocity field that transforms the noise distribution into the data distribution. This contrasts with other diffusion models that require learning complex reverse diffusion processes.\n\n2.  **Improved Stability**: By directly modeling the velocity field, Flow Matching provides more stable training dynamics, reducing the risk of mode collapse and improving the overall quality of generated videos.\n\n3.  **Enhanced Efficiency**: Flow Matching can lead to more efficient sampling during inference, as it allows for the use of ordinary differential equation (ODE) solvers to generate samples in fewer steps compared to traditional diffusion models.\n\n4.  **High-Quality Samples**: The method ensures that the model learns to predict the instantaneous rate of change of noisy samples, which can be used to reverse the diffusion process and recover high-quality data samples from noise.\n\n5.  **Flexibility**: Flow Matching offers flexibility in terms of the choice of ODE solvers and timestep schedules, allowing for further optimization of the video generation process."
    },
    {
        "question_id": "2502.10248v1_7",
        "answer": "The Step-Video-T2V employs a hierarchical data filtering approach to refine the pre-training dataset into subsets of progressively higher quality. This process involves applying a series of filters with increasing thresholds, resulting in six pre-training subsets. The final SFT (Supervised Fine-Tuning) dataset is then created through manual filtering.\n\n***\n\nHere's a breakdown of the key aspects and importance of this approach:\n\n**1. Data Filtering Process**\n\n*   **Automated Filters**: The paper mentions several key filters applied at each stage. These filters are designed to remove undesirable or low-quality data based on specific criteria:\n\n    *   **Aesthetic Score**: Uses a CLIP-based aesthetic predictor to assess the visual appeal of video frames.\n    *   **NSFW Score**: Employs a CLIP-based NSFW detector to identify and remove inappropriate content.\n    *   **Watermark Detection**: Detects the presence of watermarks, which are often indicative of lower-quality or copyrighted material.\n    *   **Subtitle Detection**: Identifies and removes videos with excessive on-screen text or captions.\n    *   **Saturation Score**: Assesses color saturation to filter out videos with poor color quality.\n    *   **Blur Score**: Detects blurriness caused by camera shake or lack of clarity.\n    *   **Black Border Detection**: Identifies and removes frames with black borders.\n    *   **Motion Assessment**: Assesses motion content to identify clips with appropriate motion.\n    *   **Video Captioning**: Generates both short and dense captions for video clips using an in-house Vision Language Model (VLM).\n    *   **Video Concept Balancing**: Addresses category imbalances and facilitates deduplication in the dataset.\n    *   **Video-Text Alignment**: Computes a CLIP Score to measure video-text alignment.\n\n*   **Hierarchical Thresholds**: The thresholds for these filters are progressively increased to create subsets of higher quality. This hierarchical approach allows for a gradual refinement of the dataset.\n\n*   **Manual Filtering**: The final SFT dataset is constructed through manual filtering by human annotators, ensuring the highest possible quality.\n\n**2. Importance for Training High-Quality Video Generation Models**\n\n*   **Improved Data Quality**: By systematically filtering out low-quality data, the model is trained on a dataset that is more consistent, visually appealing, and relevant.\n\n*   **Enhanced Model Performance**: Training on high-quality data leads to improved model performance, including better visual quality, more accurate content, and smoother motion dynamics.\n\n*   **Increased Training Efficiency**: By reducing the amount of noise and irrelevant information in the training data, the model can learn more efficiently, leading to faster convergence and better generalization.\n\n*   **Better Alignment with Human Preferences**: Filters such as aesthetic score and manual filtering help align the model's learning process with human cognitive patterns, resulting in videos that are more visually pleasing and engaging.\n\n*   **Reduced Artifacts and Instabilities**: High-quality data helps mitigate issues such as artifacts, distortions, and unstable camera movements in the generated videos.\n\n***"
    },
    {
        "question_id": "2502.10248v1_8",
        "answer": "Here's a breakdown of how Step-Video-T2V stacks up against commercial models like Sora and Veo, based on the provided document.\n\n***\n\n### General Comparison\n\n*   **Level of Development:** The paper positions Step-Video-T2V as a Level-1 video foundation model. This means it primarily focuses on translating text or other inputs into video. It acknowledges that models like Sora, Veo, and others also fall into this category. These models excel at generating high-quality videos from text prompts, which simplifies video content creation.\n*   **Limitations of Level-1 Models:** The paper points out that Level-1 models often struggle with complex action sequences or accurately simulating physics. They learn mappings between prompts and videos but don't explicitly model causal relationships within the videos themselves.\n\n***\n\n### Step-Video-T2V vs. Commercial Engines\n\n*   **Strengths of Step-Video-T2V:**\n    *   Comparable performance to commercial engines for general text prompts.\n    *   Surpasses commercial engines in specific domains:\n        *   Generating videos with high motion dynamics.\n        *   Generating videos with text content.\n    *   Largest open-source model.\n    *   Utilizes a high-compression **VAE** for videos.\n    *   Supports bilingual text prompts (English and Chinese).\n    *   Implements a video-based **DPO** approach to reduce artifacts and enhance visual quality.\n*   **Weaknesses Compared to Some Commercial Models:**\n    *   Aesthetic appeal: The paper mentions that models like **T2VTopA** and **T2VTopB** have a generally higher aesthetic appeal. This is attributed to their higher video resolutions (720P and 1080P, respectively, compared to Step-Video-T2V's 540P) and the use of high-quality aesthetic data during post-training.\n    *   Instruction-following: **T2VTopA** is noted as having better instruction-following capability, which contributes to superior performance in categories like \"Combined Concepts,\" \"Surreal,\" and \"Cinematography.\" This is linked to a better video captioning model and more human effort in labeling post-training data.\n\n***\n\n### Specific Comparisons and Benchmarks\n\n*   **Movie Gen Video:** Step-Video-T2V achieves comparable performance. The paper notes that Movie Gen Video was trained on significantly more videos during its high-resolution pre-training phase (73.8M vs. 27.3M for Step-Video-T2V). Also, Movie Gen Video can generate 720P videos, which are visually more appealing than Step-Video-T2V's 540P resolution.\n*   **HunyuanVideo:** Step-Video-T2V achieves significant improvements across all categories, solidifying its position as a state-of-the-art open-source text-to-video model. Step-Video-T2V can generate videos up to 204 frames, nearly double the 129-frame maximum of HunyuanVideo.\n*   **T2VTopA & T2VTopB:** In an evaluation using **Metric-1**, the overall ranking of the three models is as follows: **T2VTopA** \\> Step-Video-T2V \\> **T2VTopB**. Step-Video-T2V consistently outperforms **T2VTopA** and **T2VTopB** in the \"Sports\" category, demonstrating its strong capability in modeling and generating videos with high-motion dynamics.\n\n***\n\n### Key Factors Influencing Performance\n\n*   **Data Quality and Quantity:** The paper emphasizes the importance of high-quality data with accurate captions and desired styles for supervised fine-tuning (SFT).\n*   **Training Strategy:** A cascaded training pipeline, including text-to-image pre-training, text-to-video pre-training, supervised fine-tuning (SFT), and direct preference optimization (DPO), accelerates model convergence and fully leverages video datasets of varying quality.\n*   **Model Architecture:** The choice of a diffusion Transformer (DiT)-based model trained using Flow Matching is a key architectural decision.\n*   **Resolution:** Higher video resolutions (like 720P or 1080P) contribute to a greater aesthetic appeal.\n\n***\n\n### Summary\n\nStep-Video-T2V is a strong open-source contender in the text-to-video space. While it may not always match the aesthetic appeal or instruction-following capabilities of some leading commercial models, it excels in motion dynamics and offers advantages in terms of accessibility, customizability, and support for bilingual prompts. The paper identifies key areas for future improvement, including increasing resolution, improving instruction-following, and ensuring adherence to the laws of physics."
    },
    {
        "question_id": "2502.10248v1_9",
        "answer": "Human feedback plays a crucial role in refining Step-Video-T2V's video generation by improving the visual quality of generated videos and ensuring better alignment with user prompts. This is achieved through a specific implementation in the training pipeline.\n\n***\n\nHere's how human feedback is incorporated:\n\n1.  **Direct Preference Optimization (DPO):**\n\n    *   DPO is selected as the method for incorporating human feedback due to its effectiveness and simplicity.\n    *   The goal is to adjust the model (current policy) to generate preferred data while avoiding non-preferred data, based on human preference data.\n    *   A reference model (reference policy) is used to stabilize training and prevent the current model from deviating too far.\n2.  **Data Collection for Training:**\n\n    *   A diverse prompt set is constructed to collect training samples.\n    *   Prompts are randomly selected from the training data to ensure diversity.\n    *   Human annotators synthesize prompts based on real-world user interaction patterns.\n    *   Step-Video-T2V generates multiple videos using different seeds for each prompt.\n    *   Human annotators rate the preference of these samples, with quality control to ensure accuracy and consistency.\n    *   This process results in preference and non-preference data used for model training.\n3.  **Training Process:**\n\n    *   At each training step, a prompt and its corresponding positive and negative sample pairs are selected.\n    *   Each sample is generated by the model itself to ensure smooth updates and improve training stability.\n    *   Positive and negative samples are aligned by fixing the initial noise and timestep to maintain consistency in training data.\n4.  **Reward Model (Proposed):**\n\n    *   A reward model is proposed to address the saturation of improvements when the model can easily distinguish between positive and negative samples.\n    *   This reward model dynamically evaluates the quality of newly generated samples during training.\n    *   The reward model is periodically fine-tuned with newly annotated human feedback to maintain alignment with the evolving model.\n    *   By integrating the reward model, training data is scored and ranked on-the-fly, improving data efficiency.\n\n***\n\nIn summary, human feedback is implemented in Step-Video-T2V through DPO, which adjusts the model to align with preferred video outputs. The process involves collecting preference data through human annotation, using this data to train the model, and proposing a reward model to dynamically evaluate and rank training data, thereby improving overall video generation quality and alignment with user preferences."
    }
]