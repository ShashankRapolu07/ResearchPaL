[
    {
        "question_id": "2502.10248v1_0",
        "answer": "## Two Levels of Video Foundation Models\n\nThe paper describes two levels of video foundation models:\n\n1. **Level-1: Translational Video Foundation Model**  \n   - Functions as a **cross-modal translation system**, capable of generating videos from text, visual, or multimodal context.  \n   - Current diffusion-based text-to-video models such as **Sora, Veo, Kling, Hailuo, and Step-Video** belong to this level.  \n   - These models focus on mapping input prompts to videos without explicitly modeling underlying causal relationships.  \n   - **Limitations**:  \n     - Struggle with generating complex action sequences (e.g., gymnastics).  \n     - Cannot enforce physical laws (e.g., a basketball bouncing realistically).  \n     - Lack reasoning abilities found in large language models (LLMs).\n\n2. **Level-2: Predictable Video Foundation Model**  \n   - Acts as a **prediction system**, similar to LLMs, by forecasting future events based on text, visual, or multimodal context.  \n   - Aims to handle **advanced tasks**, including multimodal reasoning and real-world scenario simulation.  \n   - **Potential improvements over Level-1**:  \n     - Better understanding of causal and logical relationships in videos.  \n     - More accurate generation of physics-consistent motion sequences.  \n     - Higher-level reasoning capabilities beyond simple prompt-to-video generation.\n\nCurrently, diffusion-based models are limited to **Level-1**, while **autoregressive approaches** introduce causal modeling but still underperform compared to diffusion models in text-to-video generation.\n\n(Source: Step-Video-T2V Technical Report)\n"
    },
    {
        "question_id": "2502.10248v1_1",
        "answer": "# How Step-Video-T2V\u2019s Video-VAE Achieves High Spatial and Temporal Compression While Maintaining Video Reconstruction Quality\n\n## Overview of Video-VAE Compression Strategy\n\nStep-Video-T2V\u2019s **Video-VAE** achieves an impressive **16\u00d716 spatial and 8\u00d7 temporal compression ratio** while maintaining high video reconstruction quality. This is achieved through a combination of **causal 3D convolution, dual-path latent fusion, and optimized compression techniques**.\n\n### 1. **Latent Space Compression for Video Generation**\n- Video-VAE maps **3-channel RGB inputs to 16-channel latent representations**, achieving an overall compression ratio of **1:96**.\n- To further reduce tokens, a **patchification** technique is employed, where **2\u00d72\u00d71 latent patches** are grouped into individual tokens.\n- Unlike traditional methods that use a two-stage compression and tokenization process, **Video-VAE integrates both within a unified structure**, reducing complexity and enhancing performance.\n\n### 2. **Dual-Path Latent Fusion for High-Quality Reconstruction**\n- The dual-path approach helps balance **high-frequency details and low-frequency structures**:\n  - **Conv Path:** Uses **causal 3D convolutions and pixel unshuffling** for capturing fine-grained details.\n  - **Shortcut Path:** Preserves **structural semantics** through **grouped channel averaging**, ensuring smooth video reconstruction without loss of texture.\n- This mechanism allows Video-VAE to **overcome blurring artifacts** typically seen in traditional VAEs&#8203;:contentReference[oaicite:0]{index=0}.\n\n### 3. **Efficient Temporal and Spatial Encoding**\n- **Causal 3D Convolutions** ensure that each frame is dependent only on previous frames, allowing for efficient motion modeling.\n- The **encoder applies three stages of Res3DBlock downsampling**, followed by a **mid-block with convolution and attention mechanisms** to refine compressed representations&#8203;:contentReference[oaicite:1]{index=1}.\n\n### 4. **Optimized Decoder for Reconstruction**\n- The decoder **reverses the dual-path compression process** using:\n  - **3D pixel shuffle for spatial-temporal upscaling**\n  - **Grouped channel repeating for structure preservation**\n- **Spatial group normalization** is applied instead of conventional group normalization to **prevent flickering artifacts** between video chunks&#8203;:contentReference[oaicite:2]{index=2}.\n\n### 5. **Superior Reconstruction Performance**\n- Despite using a **higher compression ratio (8\u00d716\u00d716)** than competing models (such as Cosmos-VAE's 8\u00d716\u00d716 variant), **Video-VAE outperforms in quality metrics**:\n  - **SSIM: 0.9776 (higher is better)**\n  - **PSNR: 39.37 (higher is better)**\n  - **rFVD: 3.61 (lower is better)**\n- This demonstrates that Video-VAE **maintains exceptional quality despite aggressive compression**&#8203;:contentReference[oaicite:3]{index=3}.\n\n### 6. **Training and Loss Optimization**\n- The VAE is **trained in multiple stages**:\n  - Initial training with a **4\u00d78\u00d78 compression ratio** before incorporating the **dual-path structure**.\n  - Progressive **unfreezing** of key components (ResNet backbone, mid-block, and dual-path modules) for fine-tuning.\n  - Use of **L1 reconstruction loss, Video-LPIPS, and KL-divergence constraints**, followed by **GAN loss** to further enhance perceptual quality&#8203;:contentReference[oaicite:4]{index=4}.\n\n## Conclusion\nStep-Video-T2V\u2019s **Video-VAE** effectively balances **high compression with superior reconstruction quality** by employing **causal 3D convolutions, dual-path latent fusion, and optimized training strategies**. This enables **efficient large-scale video generation** while preserving **motion dynamics, textures, and structural integrity** at an unprecedented level.\n"
    },
    {
        "question_id": "2502.10248v1_2",
        "answer": "# Direct Preference Optimization (DPO) in Step-Video-T2V\n\n## What is Direct Preference Optimization (DPO)?\n\nDirect Preference Optimization (DPO) is a training strategy that refines a model\u2019s outputs based on human preferences. Instead of using traditional Reinforcement Learning with Human Feedback (RLHF), DPO directly optimizes the model\u2019s likelihood of generating preferred outputs while avoiding non-preferred ones. This method is particularly useful in generative models where human annotations can provide feedback on the quality and coherence of outputs.\n\nDPO works by comparing two samples generated under the same conditions\u2014one preferred by human annotators and one not preferred. The model is then optimized to favor the preferred outputs while staying close to a reference policy to maintain stability. The optimization objective can be formulated as:\n\n\\[\nL_{DPO} = -E(y,x_w,x_l) \\sim D \\left[ \\log \\sigma \\left( \beta \\left( \\log \frac{\\pi_{\theta}(x_w | y)}{\\pi_{ref}(x_w | y)} - \\log \frac{\\pi_{\theta}(x_l | y)}{\\pi_{ref}(x_l | y)} \right) \right) \right]\n\\]\n\nwhere:\n- \\( \\pi_{\theta} \\) and \\( \\pi_{ref} \\) refer to the current and reference policies.\n- \\( x_w \\) and \\( x_l \\) are the preferred and non-preferred samples.\n- \\( y \\) represents the input condition (e.g., text prompt).\n- \\( \beta \\) is a scaling factor controlling the optimization strength.\n\n## How DPO Improves Visual Quality in Step-Video-T2V?\n\nIn **Step-Video-T2V**, a video-based variant of DPO (Video-DPO) is used to enhance video generation quality. The key benefits of using Video-DPO in Step-Video-T2V include:\n\n1. **Artifact Reduction** \u2013 DPO helps eliminate common video generation artifacts, such as blurriness, flickering, and unnatural transitions, by optimizing the model toward more visually appealing samples.\n\n2. **Smoother Motion and Realism** \u2013 By incorporating human preference data, the model learns to generate more fluid and realistic motion sequences, avoiding choppy or unnatural movement.\n\n3. **Better Prompt Adherence** \u2013 The model becomes better aligned with text prompts, producing outputs that more accurately reflect user intentions.\n\n4. **Efficient Optimization** \u2013 Unlike RLHF, which requires reinforcement learning policies and reward modeling, DPO provides a more stable and computationally efficient way to incorporate human feedback.\n\n5. **Faster Convergence** \u2013 Modifications in the training strategy, such as reducing the scaling factor \\( \beta \\) and adjusting the learning rate, allow for significantly faster convergence compared to traditional preference-based optimization methods.\n\n### Human Feedback Pipeline in Step-Video-T2V\nTo implement DPO, Step-Video-T2V follows a structured pipeline:\n- A **prompt pool** is created from a mix of real and synthetic prompts.\n- Multiple video samples are generated per prompt.\n- Human annotators **rank the samples** based on visual quality, realism, and motion smoothness.\n- Preferred and non-preferred samples are used in the **DPO training process**.\n- A **reward model** is periodically trained to maintain alignment with evolving preferences.\n\n### Limitations and Future Directions\nWhile Video-DPO significantly improves video generation quality, its effectiveness saturates as the model reaches a certain quality threshold. This happens because:\n- The model\u2019s policy evolves over time, making older preference data less relevant.\n- Manual annotations are time-consuming and may not scale efficiently.\n\nTo address these, **Step-Video-T2V integrates a reward model** that dynamically evaluates newly generated samples, improving on-the-fly training efficiency.\n\n### Conclusion\nDPO plays a crucial role in refining Step-Video-T2V by leveraging human preferences to systematically enhance video quality. By reducing artifacts, improving motion consistency, and refining prompt adherence, Video-DPO ensures a **more natural and visually appealing** video generation experience.\n\n---\n_Source: Step-Video-T2V Technical Report (2025)_\n"
    },
    {
        "question_id": "2502.10248v1_3",
        "answer": "# Key Advantages of Using a Diffusion Transformer (DiT) with 3D Full Attention in Step-Video-T2V\n\n## 1. Superior Spatial-Temporal Modeling\n- The **3D full-attention** mechanism in Step-Video-T2V offers an **upper bound** for capturing both **spatial and temporal** information in videos.\n- Unlike spatial-temporal attention, which separates spatial and temporal components, 3D full-attention **unifies** them, leading to a more **coherent** understanding of video dynamics&#8203;:contentReference[oaicite:0]{index=0}.\n\n## 2. Enhanced Motion Consistency\n- Large-scale experiments confirm that 3D full-attention **outperforms** other attention mechanisms in generating videos with **smooth and consistent motion**.\n- This leads to more **realistic** and **visually appealing** outputs, especially in scenarios involving high-motion dynamics&#8203;:contentReference[oaicite:1]{index=1}.\n\n## 3. Computational Efficiency\n- Despite the theoretical increase in complexity, 3D full-attention is found to be **more efficient** than spatial-temporal attention when handling long video sequences.\n- The model leverages **optimized computation strategies**, including **AdaLN-Single** for reduced overhead, ensuring better resource utilization&#8203;:contentReference[oaicite:2]{index=2}.\n\n## 4. Effective Text Prompt Conditioning\n- The **cross-attention** layer introduced between self-attention and the feed-forward network (FFN) enables the model to **effectively integrate textual prompts**.\n- The **bilingual text encoding system** (Hunyuan-CLIP & Step-LLM) enhances **text-video alignment**, leading to more accurate prompt-based video generation&#8203;:contentReference[oaicite:3]{index=3}.\n\n## 5. Improved Positional Encoding for Video Data\n- The implementation of **RoPE-3D** (Rotation-based Positional Encoding for 3D space) allows for a **flexible** and **continuous** representation of positions in sequences of varying lengths.\n- RoPE-3D accounts for **frame-level**, **height**, and **width** dimensions, significantly improving the model\u2019s ability to **generalize** across different video resolutions&#8203;:contentReference[oaicite:4]{index=4}.\n\n## 6. Stability and Training Optimizations\n- The use of **Query-Key Normalization (QK-Norm)** ensures more **stable self-attention mechanisms**, reducing training instabilities.\n- The model follows **Flow Matching** as its training objective, which enhances the **efficiency** of denoising steps in the diffusion process&#8203;:contentReference[oaicite:5]{index=5}.\n\n## 7. Higher Performance in Video Generation Tasks\n- Direct **comparison with spatial-temporal attention-based models** in Step-Video-T2V confirms that 3D full-attention leads to **better motion dynamics**.\n- This makes it a **preferred choice** for high-quality text-to-video generation models&#8203;:contentReference[oaicite:6]{index=6}.\n\n## Conclusion\nThe integration of **DiT with 3D full-attention** in Step-Video-T2V provides **significant advantages** in **video quality, motion smoothness, efficiency, and text alignment**, making it one of the **most powerful** open-source text-to-video generation models available.\n"
    },
    {
        "question_id": "2502.10248v1_4",
        "answer": "## Step-Video-T2V Handling of Bilingual Text Prompts and Dual Text Encoders\n\n### Bilingual Text Prompts Handling\nStep-Video-T2V is designed to process bilingual text prompts efficiently, supporting both English and Chinese. To achieve this, it utilizes two distinct pre-trained bilingual text encoders:\n\n1. **Hunyuan-CLIP**: A bidirectional text encoder from an open-source bilingual CLIP model. It generates text representations well-aligned with the visual space due to CLIP's training paradigm. However, it has a **77-token input limit**, making it less suitable for long and complex text prompts.\n   \n2. **Step-LLM**: An in-house unidirectional bilingual text encoder trained with a next-token prediction task. It employs a **redesigned Alibi-Positional Embedding**, enhancing its ability to process long text sequences efficiently, **without input length restrictions**.\n\n### Advantages of Using Two Separate Text Encoders\nUsing these two encoders in tandem provides several advantages:\n\n1. **Handling Prompts of Different Lengths**: \n   - Hunyuan-CLIP is effective for short prompts due to its strong alignment with visual embeddings.\n   - Step-LLM accommodates longer prompts without truncation.\n\n2. **Improved Text Representation**: \n   - Hunyuan-CLIP captures text-to-image alignment well, making it ideal for **short and structured descriptions**.\n   - Step-LLM is optimized for **long-form content**, ensuring more contextual understanding.\n\n3. **Better Integration with Video Generation**:\n   - The embeddings from both encoders are **concatenated along the sequence dimension**, forming a **comprehensive text representation**.\n   - This combined representation is used in the **cross-attention layer** of the diffusion transformer, allowing for better alignment between text and video generation.\n\n4. **Robust Multimodal Mapping**:\n   - The **CLIP-based encoder** ensures strong grounding in the visual space.\n   - The **LLM-based encoder** enhances textual comprehension, leading to **more coherent and relevant video outputs**.\n\nBy leveraging these two encoders, Step-Video-T2V achieves **higher fidelity and flexibility in video generation**, allowing it to process and understand complex bilingual prompts effectively&#8203;:contentReference[oaicite:0]{index=0}.\n"
    },
    {
        "question_id": "2502.10248v1_5",
        "answer": "# Challenges in Diffusion-Based Text-to-Video Models and Step-Video-T2V's Solutions\n\n## Challenges in Current Diffusion-Based Text-to-Video Models\n\n1. **Complex Action Sequences**  \n   - Struggle to generate videos requiring intricate action sequences (e.g., gymnastics) due to a lack of explicit causal modeling.\n\n2. **Physics Adherence**  \n   - Difficulty in generating physically plausible motion, such as a bouncing basketball, as they primarily learn mappings from text to video rather than the laws of physics.\n\n3. **Causal and Logical Reasoning**  \n   - Incapable of performing logical or causal reasoning akin to Large Language Models (LLMs), limiting their ability to generate videos requiring contextual understanding.\n\n4. **Composition of Rare Concepts**  \n   - Generating videos that successfully blend multiple uncommon concepts (e.g., an elephant and a penguin in the same scene) remains challenging due to data sparsity.\n\n5. **Video Captioning Issues**  \n   - Current video captioning models suffer from hallucination issues, leading to unstable training and poor instruction-following performance.\n\n6. **Computational Costs**  \n   - Generating long-duration, high-resolution videos demands significant computational resources, making scalability a key concern.\n\n7. **Artifact Reduction and Visual Quality**  \n   - Diffusion models often produce videos with artifacts and inconsistencies in visual quality, requiring additional post-processing.\n\n---\n\n## How Step-Video-T2V Addresses These Challenges\n\n1. **High-Compression Video-VAE**  \n   - Introduces a **deep compression Variational Autoencoder (Video-VAE)** with **16\u00d716 spatial** and **8\u00d7 temporal** compression, reducing computational costs while maintaining quality.\n\n2. **Bilingual Text Encoding**  \n   - Uses **two bilingual text encoders** (Hunyuan-CLIP and Step-LLM) to process English and Chinese text prompts effectively.\n\n3. **Diffusion Transformer with 3D Full Attention (DiT-3D)**  \n   - Employs a **DiT with 3D full attention** trained with **Flow Matching**, improving motion consistency and spatial-temporal coherence.\n\n4. **Video-Based Direct Preference Optimization (Video-DPO)**  \n   - Implements a **Video-DPO** approach to refine video quality based on human feedback, reducing artifacts and enhancing realism.\n\n5. **Hierarchical Cascaded Training Strategy**  \n   - Utilizes **multi-stage training**:\n     - **Text-to-Image Pre-training** for foundational visual understanding.\n     - **Low-resolution Text-to-Video Pre-training** for motion learning.\n     - **High-resolution Text-to-Video Pre-training** for fine details.\n     - **Supervised Fine-Tuning (SFT)** for improved stability.\n     - **Direct Preference Optimization (DPO)** for aligning videos with human expectations.\n\n6. **Improved Computational Efficiency**  \n   - Introduces optimizations in model parallelism, including **tensor parallelism (TP), sequence parallelism (SP), and zero redundancy optimizations** to maximize GPU utilization.\n\n7. **Step-Video-T2V Turbo for Faster Inference**  \n   - Reduces **ODE integration steps from 50 to 8** using **self-distillation with rectified flow**, significantly improving inference efficiency.\n\nBy implementing these advancements, Step-Video-T2V surpasses existing diffusion-based text-to-video models in **motion dynamics, visual quality, computational efficiency, and instruction following**.\n\n---\n\nFor more details, see the [Step-Video-T2V repository](https://github.com/stepfun-ai/Step-Video-T2V).\n"
    },
    {
        "question_id": "2502.10248v1_6",
        "answer": "## Step-Video-T2V and Flow Matching in Training\n\n### How Step-Video-T2V Uses Flow Matching\n\nStep-Video-T2V utilizes **Flow Matching** as part of its training strategy for video generation. Specifically, it applies Flow Matching in the training of its **Diffusion Transformer (DiT) with 3D full attention**. The key steps in this process are:\n\n1. **Sampling Noise and Timestep**  \n   - At each training step, a Gaussian noise sample \\(X_0 \\sim N(0, 1)\\) is drawn.\n   - A random timestep \\(t\\) is sampled from a uniform distribution in the range \\([0,1]\\).\n\n2. **Interpolating Between Noise and Target Sample**  \n   - The model constructs an intermediate sample \\(X_t\\) as a linear interpolation between \\(X_0\\) (pure noise) and \\(X_1\\) (the target noise-free video frame):\n     \\[\n     X_t = (1 - t) \\cdot X_0 + t \\cdot X_1\n     \\]\n   - This creates a continuous trajectory between the noisy state and the clean video data.\n\n3. **Predicting the Velocity Field**  \n   - The model learns to predict the **velocity** \\(V_t\\), which is the rate of change of \\(X_t\\) with respect to \\(t\\):\n     \\[\n     V_t = \frac{dX_t}{dt} = X_1 - X_0\n     \\]\n   - Instead of denoising in discrete steps (like traditional diffusion models), Flow Matching learns the direct path to the final sample.\n\n4. **Loss Function for Training**  \n   - The training objective minimizes the mean squared error (MSE) between the predicted velocity \\(u(X_t, y, t; \theta)\\) and the true velocity \\(V_t\\):\n     \\[\n     \text{loss} = \\mathbb{E}_{t, X_0, X_1, y} \\left[ \\| u(X_t, y, t; \theta) - V_t \\|^2 \right]\n     \\]\n\n### Benefits of Flow Matching in Video Generation\n\nUsing Flow Matching instead of conventional diffusion-based denoising approaches provides several advantages for video generation:\n\n- **Efficient Sampling**: Flow Matching allows for **ODE-based inference**, reducing the number of required sampling steps compared to standard diffusion models.\n- **Smoother Motion Dynamics**: Since the model learns continuous motion trajectories, it generates **more fluid and realistic motion** in videos.\n- **Better Generalization**: Flow Matching helps in **learning complex temporal dependencies** in videos, improving consistency across frames.\n- **Faster Convergence**: Compared to traditional noise-conditioning diffusion models, Flow Matching **stabilizes training** and accelerates model convergence.\n\nIn conclusion, Step-Video-T2V leverages Flow Matching to improve **motion consistency, training efficiency, and inference speed**, making it a powerful approach for high-quality video generation&#8203;:contentReference[oaicite:0]{index=0}.\n"
    },
    {
        "question_id": "2502.10248v1_7",
        "answer": "## Hierarchical Data Filtering in Step-Video-T2V\n\n### Approach:\nStep-Video-T2V employs a **hierarchical data filtering** approach, progressively applying increasingly strict thresholds to refine the training dataset. This method ensures that only high-quality data is used in different stages of model training.\n\n### Key Steps:\n1. **Filtering by Video Assessment Scores:**  \n   - Initial filtering is based on heuristic rules and automated video assessment scores.\n   - This step reduces the dataset size while improving overall quality.\n\n2. **Filtering by Video Categories:**  \n   - Videos are grouped into clusters using K-means clustering.\n   - Within each cluster, outlier videos (determined by \"distance to centroid\") are removed.\n   - This maintains diversity while ensuring a balanced dataset.\n\n3. **Human Annotation for Final Refinement:**  \n   - Human evaluators assess videos for motion clarity, aesthetics, smooth transitions, and absence of watermarks/subtitles.\n   - Captions are manually refined to improve accuracy, ensuring inclusion of essential details like camera movements, actions, and lighting.\n\n4. **Stepwise Dataset Creation for Pre-training:**  \n   - The dataset is progressively filtered into six pre-training subsets.\n   - As training progresses, increasingly refined datasets are used to optimize learning.\n\n### Importance:\n- **Improves Data Quality:** Reduces noise and artifacts, leading to clearer, more realistic video generations.\n- **Enhances Model Stability:** Ensures a smooth learning curve with stepwise dataset refinement.\n- **Reduces Training Loss:** A stepwise improvement in dataset quality correlates with a significant drop in training loss.\n- **Aligns with Human Perception:** Filtering criteria are designed based on human intuition (e.g., CLIP scores, aesthetic scores), which helps the model learn representations that align with human preferences.\n\nThis hierarchical data filtering strategy plays a crucial role in training high-quality text-to-video generation models by ensuring that the dataset is progressively optimized for better performance&#8203;:contentReference[oaicite:0]{index=0}.\n"
    },
    {
        "question_id": "2502.10248v1_8",
        "answer": "# Comparison of Step-Video-T2V with Commercial Video Generation Models (Sora, Veo)\n\n## Overview\n\nStep-Video-T2V is an open-source **text-to-video** (T2V) model developed by **StepFun**. It competes with commercial models such as OpenAI's **Sora** and DeepMind's **Veo**, offering high-quality video generation through a **diffusion-based** approach.\n\n## Technical Comparison\n\n| Feature               | Step-Video-T2V                            | Sora (OpenAI)                     | Veo (DeepMind)                     |\n|----------------------|--------------------------------------|--------------------------------|--------------------------------|\n| **Model Type**       | Diffusion Transformer (DiT)         | Likely Transformer-based Diffusion | Likely Transformer-based Diffusion |\n| **Parameter Count**  | **30B**                             | Unknown (~highly parameterized) | Unknown (~highly parameterized) |\n| **Max Video Length** | **204 frames (~8.5 sec @ 24 fps)**  | **Potentially 1+ min**          | **Potentially 1+ min**          |\n| **Resolution**       | **544x992 px**                      | **Likely higher (1080p+)**     | **Likely higher (1080p+)**     |\n| **Compression**      | Video-VAE (16x16 spatial, 8x temporal) | Unknown | Unknown |\n| **Prompt Language Support** | **English, Chinese**          | **English (primary)**          | **English (primary)**          |\n| **Architecture Enhancements** | 3D full-attention, RoPE-3D, Query-Key Normalization (QK-Norm) | Likely optimized Transformers | Likely optimized Transformers |\n| **Training Data** | **2B video-text pairs, 3.8B image-text pairs** | Unknown (likely massive) | Unknown (likely massive) |\n| **Inference Speed** | **Step-Video-T2V Turbo (10 NFE inference)** | Likely optimized inference | Likely optimized inference |\n| **Commercial Availability** | **Open-source** (GitHub) | **Closed-source** | **Closed-source** |\n\n## Strengths of Step-Video-T2V\n- **Open-source**: Available for research and customization.\n- **Bilingual support**: Handles **English and Chinese** prompts.\n- **Efficient Compression**: Uses **Video-VAE** to achieve **16x16 spatial and 8x temporal compression**, reducing computational costs.\n- **Video-DPO (Direct Preference Optimization)**: Improves visual quality through human feedback fine-tuning.\n- **Turbo Mode**: Accelerated inference using self-distillation, reducing steps without sacrificing quality.\n\n## Limitations Compared to Sora and Veo\n- **Shorter video length**: **204 frames (~8.5s)** is shorter than **Sora and Veo**, which can likely generate **longer sequences (30s+ or even 1+ min)**.\n- **Lower resolution**: **544x992 px** vs. the **likely higher (1080p+) resolution** of commercial models.\n- **Motion and physics limitations**: Struggles with **complex action sequences and physical accuracy** (e.g., bouncing objects, intricate human motions).\n\n## Conclusion\n\nStep-Video-T2V is a **powerful open-source competitor** in text-to-video generation, offering strong motion dynamics and aesthetic quality. However, **commercial models like Sora and Veo** appear to have **longer-duration, higher-resolution, and more advanced** motion generation capabilities. **Sora and Veo remain ahead in length and realism**, but **Step-Video-T2V is a strong alternative for researchers and developers** wanting access to an advanced, customizable video model.\n\n**\ud83d\udd17 More Details**: [GitHub - Step-Video-T2V](https://github.com/stepfun-ai/Step-Video-T2V)\n"
    },
    {
        "question_id": "2502.10248v1_9",
        "answer": "## Role of Human Feedback in Step-Video-T2V\u2019s Video Generation\n\n### Importance of Human Feedback\nHuman feedback has been widely validated in **LLMs** through methods such as **Reinforcement Learning with Human Feedback (RLHF)**. Recently, this approach has been applied to **image and video generation**, leading to significant improvements in visual quality. In **Step-Video-T2V**, human feedback is incorporated to refine the model\u2019s outputs and reduce artifacts.\n\n### Implementation of Human Feedback in Training Pipeline\nStep-Video-T2V utilizes **Direct Preference Optimization (DPO)** to integrate human feedback into the video generation process. The pipeline consists of the following steps:\n\n1. **Prompt Collection:**\n   - A diverse set of prompts is constructed.\n   - Prompts are selected from training data and human-annotated prompts are added for better real-world representation.\n\n2. **Video Generation & Preference Labeling:**\n   - The model generates multiple videos per prompt using different seeds.\n   - Human annotators **rank and label** the generated videos based on quality and adherence to the prompt.\n   - This produces **preferred (xw) and non-preferred (xl) samples**.\n\n3. **DPO Training Objective:**\n   - The model is trained to **favor preferred outputs** over non-preferred ones while preventing excessive deviations from its original policy.\n   - The **DPO loss function** is formulated as:\n     \\[\n     L_{DPO} = -E(y, x_w, x_l) \\left[ \\log \\sigma \\left( \beta \\left( \\log \frac{\\pi_{\theta}(x_w | y)}{\\pi_{ref}(x_w | y)} - \\log \frac{\\pi_{\theta}(x_l | y)}{\\pi_{ref}(x_l | y)} \right) \right) \right]\n     \\]\n   - Here, **\u03c0\u03b8** is the current policy, **\u03c0ref** is the reference policy, and **\u03b2** is a scaling factor.\n\n4. **Stable Training Adjustments:**\n   - Initial noise and timesteps are **fixed** between positive and negative samples for stability.\n   - **Gradient clipping** and an optimized learning rate are used to accelerate convergence.\n\n5. **Reward Model for Continuous Improvement:**\n   - Over time, improvements in video quality **saturate** as the model learns to distinguish good and bad outputs.\n   - A **reward model** is trained using human feedback to **dynamically evaluate** newly generated videos.\n   - This allows **on-policy scoring** and **data-efficient updates**, ensuring that training remains relevant.\n\n### Effectiveness of Human Feedback\n- **Reduces artifacts** and improves realism in generated videos.\n- Enhances alignment between text prompts and generated content.\n- Helps **iteratively refine** model performance by leveraging continuous human evaluation.\n\nBy integrating human feedback through **DPO and reward modeling**, Step-Video-T2V achieves state-of-the-art video generation quality while maintaining efficiency in training.\n\n_Source: Step-Video-T2V Technical Report_\n"
    }
]