[
    {
        "question_id": "2502.10248v1_0",
        "answer": "The paper outlines two distinct levels for video foundation models, each representing a different stage of capability and complexity:\n\n### Level 1: Translational Video Foundation Model\n\n*   This level focuses on cross-modal translation.\n*   Models at this level function as systems that can generate videos from various inputs such as text, images, or a combination of both.\n*   Current diffusion-based text-to-video models, including Step-Video-T2V, fall into this category.\n*   These models excel at creating high-quality videos from text prompts, which lowers the barrier for content creation.\n*   However, they often struggle with generating videos that require complex action sequences or adherence to physical laws, and they lack the ability to perform causal or logical reasoning.\n\n***\n\n### Level 2: Predictable Video Foundation Model\n\n*   This level aims for predictive capabilities.\n*   Models at this level should function as prediction systems, similar to large language models (LLMs).\n*   They should be able to forecast future events based on various inputs and handle advanced tasks such as reasoning with multimodal data or simulating real-world scenarios.\n*   Achieving Level 2 requires models to explicitly model the underlying causal relationships within videos, which is a limitation of current Level 1 models.\n*   Autoregression-based text-to-video models attempt to introduce causal modeling by predicting the next video token, frame, or clip, but they have not yet matched the performance of diffusion-based models in text-to-video generation."
    },
    {
        "question_id": "2502.10248v1_1",
        "answer": "Step-Video-T2V's Video-VAE achieves high spatial and temporal compression while maintaining video reconstruction quality through a combination of architectural innovations and a multi-stage training process. Here's a breakdown:\n\n***\n\n### 1. Novel Dual-Path Architecture\n\n*   **Unified Spatial-Temporal Compression:** The Video-VAE employs a dual-path architecture in the later stages of the encoder and the early stages of the decoder. This design achieves 8x16x16 downscaling using 3D convolutions and optimized pixel shuffling operations.\n\n*   **Dual-Path Latent Fusion:**\n    *   **Conv Path**: Combines causal 3D convolutions with pixel unshuffling:\n        $H_{conv} = U^{(3)}_s(C3D(X))$\n        where $U^{(3)}_s$: $R^{B \times C \times T_{st} \times H_{ss} \times W_{ss}} \rightarrow R^{B \times C \\cdot s^3 \times T_{st} \times H_{ss} \times W_{ss}}$ with spatial stride $s_s = 2$, temporal stride $s_t = 2$, and $C3D$ denoting causal 3D convolution.\n    *   **Shortcut Path**: Preserves structural semantics through grouped channel averaging:\n        $H_{avg} = \frac{1}{s^3} \\sum_{k=0}^{s^3 - 1} U^{(3)}_s (X) [..., kC_z : (k+1)C_z]$\n        where $U^{(3)}_s$ implements 3D pixel unshuffle with spatial-temporal blocking, $C_z$ is the latent dimension of the next stage.\n    *   **Residual Summation**: The output of fusion combines both paths through residual summation:\n        $Z = H_{conv} \\oplus H_{avg}$\n\nThis dual-path approach allows the network to efficiently use its parameters, mitigating blurring artifacts typically associated with traditional VAEs.\n\n*   **Causal 3D Convolutional Modules**: The encoder's early stage features three stages, each with two Causal Res3DBlocks and downsample layers. A MidBlock combines convolutional layers with attention mechanisms. Temporal causality is implemented through:\n\n    $C3D(X)_t = \begin{cases}\n    Conv3D([0, ..., X_t], \\Theta) & t = 0 \\\n    Conv3D([X_{t-k}, ..., X_t], \\Theta) & t > 0\n    \\end{cases}$\n\n    where $k$ is the temporal kernel size, ensuring frame $t$ only depends on previous frames.\n\n### 2. Multi-Stage Training Process\n\n*   **Stage 1: Initial VAE Training:** A VAE with a 4x8x8 compression ratio is trained without a dual-path structure, jointly on images and videos with varying frame counts. This stage focuses on learning low-level representations.\n\n*   **Stage 2: Dual-Path Integration:** The model is enhanced by incorporating two dual-path modules in both the encoder and decoder. These modules, along with the mid-block and ResNet backbone, are gradually unfrozen during training.\n\n*   **Loss Functions**: A combination of L1 reconstruction loss, Video-LPIPS, and KL-divergence constraint guides the model. GAN loss is introduced after these losses have converged.\n\n### 3. Architectural Details\n\n*   **High Compression Ratios**: Achieves 16x16 spatial and 8x temporal compression ratios.\n*   **Pixel Shuffling**: Utilizes pixel shuffling operations to efficiently unfold compressed information into spatial-temporal dimensions in the decoder.\n*   **Spatial Group Normalization**: Replaces group normalization with spatial group normalization in the ResNet backbone to avoid temporal flickering between different chunks.\n\n### 4. Impact\n\n*   **Efficient Video Modeling**: The high compression enables the model to extend context length and scale up the DiT model more aggressively.\n*   **State-of-the-Art Performance**: Despite a higher compression ratio, the reconstruction quality maintains state-of-the-art performance, outperforming other baselines in scenarios with high motion, text, and textures."
    },
    {
        "question_id": "2502.10248v1_2",
        "answer": "**Direct Preference Optimization (DPO)** is a technique used to align a model's behavior with human preferences, particularly in generative tasks. Instead of using reinforcement learning, which can be complex and unstable, DPO directly optimizes the model based on preference data. The core idea is to adjust the model to generate outputs that are preferred by humans while avoiding those that are not.\n\n***\n\n### How DPO Works:\n\n1.  **Preference Data:** DPO relies on a dataset of human preferences. This dataset consists of pairs of outputs generated by the model for a given input, where humans have indicated which output they prefer.\n2.  **Optimization Objective:** The DPO objective aims to maximize the likelihood of generating preferred outputs while minimizing the likelihood of generating dispreferred outputs. This is typically achieved by adjusting the model's parameters to increase the score (e.g., log-likelihood) of preferred outputs and decrease the score of dispreferred outputs.\n3.  **Reference Policy:** To stabilize training and prevent the model from deviating too far from its initial behavior, a reference policy (i.e., a reference model) is often used. The DPO objective includes a term that penalizes deviations from this reference policy.\n\n    The policy objective can be formulated as:\n\n    $L_{DPO} = -E_{(y, x_w, x_l) \\sim D} \\left[ log \\ \\sigma \\left( \beta \\left( log \\ \frac{\\pi_\theta(x_w|y)}{\\pi_{ref}(x_w|y)} - log \\ \frac{\\pi_\theta(x_l|y)}{\\pi_{ref}(x_l|y)} \right) \right) \right]$\n\n    *   $\\pi_\theta$ and $\\pi_{ref}$ refer to the current policy and reference policy, respectively.\n    *   $x_w$ and $x_l$ are the preferred (winning) sample and non-preferred (losing) sample.\n    *   $y$ denotes the condition (e.g., a text prompt).\n    *   $\\sigma$ is the sigmoid function.\n    *   $\beta$ is a hyperparameter controlling the strength of the preference signal.\n4.  **Training Process:** During training, the model is updated iteratively using the DPO objective. The gradients of the objective are computed with respect to the model's parameters, and the parameters are adjusted to improve the alignment with human preferences.\n\n***\n\n### How DPO Improves Visual Quality in Step-Video-T2V:\n\nIn the context of Step-Video-T2V, DPO is used to enhance the visual quality of the generated videos by aligning them with human aesthetic preferences.\n\n1.  **Preference Data Collection:** Human annotators are shown pairs of videos generated by Step-Video-T2V for the same text prompt. They are asked to indicate which video they prefer based on factors such as clarity, aesthetics, motion smoothness, and the absence of artifacts.\n2.  **Training with DPO:** The Step-Video-T2V model is then trained using the DPO objective, with the preference data collected from human annotators. This encourages the model to generate videos that are more visually appealing and aligned with human preferences.\n3.  **Reduction of Artifacts:** By training with DPO, Step-Video-T2V learns to avoid generating videos with common artifacts, such as distortions, blurriness, and unnatural motions. This results in smoother, more realistic video outputs.\n4.  **Improved Alignment with Prompts:** DPO also helps to improve the alignment between the generated videos and the text prompts. The model learns to generate videos that are more relevant and accurate representations of the given prompts.\n5.  **Consistency:** DPO enhances the consistency of generated videos, ensuring that objects and scenes remain coherent throughout the video.\n\n***\n\n### Addressing Gradient Issues:\n\nThe original implementation of **DiffusionDPO** used a large $\beta$ (e.g., 5,000), which could cause gradient explosions when $z < 0$, where $z$ represents policy-related terms. This required gradient clipping and a very low learning rate, leading to slow convergence. To address this, Step-Video-T2V reduces $\beta$ and increases the learning rate, achieving faster convergence."
    },
    {
        "question_id": "2502.10248v1_3",
        "answer": "The Step-Video-T2V model leverages a Diffusion Transformer (DiT) architecture with 3D full attention, offering several key advantages for video generation:\n\n*   **Superior Spatial-Temporal Modeling**: 3D full attention allows the model to capture both spatial and temporal information in a unified attention process. This is unlike spatial-temporal attention mechanisms that process spatial information within individual frames and then temporal relationships across frames in separate steps. By combining these, 3D full attention can model intricate movements and dependencies within videos more effectively.\n\n*   **Enhanced Video Quality**: The unified attention mechanism of 3D full attention results in the generation of videos with smoother and more consistent motion. This is particularly noticeable in scenes with complex dynamics, where the model can maintain coherence across frames more effectively.\n\n*   **Theoretical Upper Bound**: 3D full attention has a higher theoretical capacity for modeling video data because it directly considers all spatial and temporal relationships. This makes it a more powerful tool for capturing the complexities inherent in video content.\n\n*   **Effective Text Prompt Integration**: The DiT architecture incorporates cross-attention layers that allow the model to attend to textual information while processing visual features. By using two distinct bilingual text encoders (Hunyuan-CLIP and Step-LLM), the model can understand and integrate both English and Chinese prompts effectively. The outputs from these encoders are combined and injected into the cross-attention layer, enabling the model to generate videos conditioned on the input prompt.\n\n*   **Efficient Normalization**: Step-Video-T2V uses Adaptive Layer Normalization (AdaLN) with optimized computation. By removing class labels from AdaLN (since the text-to-video task does not require them) and adopting the AdaLN-Single structure, the model reduces computational overhead and improves overall efficiency.\n\n*   **Flexible Positional Encoding**: The model employs RoPE-3D, an extension of Rotation-based Positional Encoding (RoPE), to handle video data. RoPE-3D accounts for temporal (frame) and spatial (height and width) dimensions, allowing the model to process videos with varying lengths and resolutions without being restricted by fixed positional encoding lengths. This improves generalization across diverse video data and effectively captures spatial and temporal relationships within the video."
    },
    {
        "question_id": "2502.10248v1_4",
        "answer": "Step-Video-T2V uses two bilingual text encoders to process user text prompts in both English and Chinese. These encoders are:\n\n1.  Hunyuan-CLIP\n2.  Step-LLM\n\n***\n\n### Advantages of Using Two Separate Text Encoders\n\n*   **Handling Varying Input Lengths**:\n\n    *   Hunyuan-CLIP is effective due to its alignment with the visual space but is limited to processing prompts with a maximum of 77 tokens.\n    *   Step-LLM, on the other hand, has no input length restriction, making it suitable for longer and more complex text sequences.\n\n*   **Efficiency and Accuracy**:\n\n    *   Step-LLM incorporates a redesigned Alibi-Positional Embedding, which improves both efficiency and accuracy in sequence processing.\n\n*   **Robust Text Representations**:\n\n    *   By combining these two text encoders, Step-Video-T2V can handle user prompts of varying lengths, generating robust text representations that effectively guide the model in the latent space.\n"
    },
    {
        "question_id": "2502.10248v1_5",
        "answer": "Here's a breakdown of the challenges faced by current diffusion-based text-to-video models, and how Step-Video-T2V attempts to address them.\n\n***\n\n### Challenges of Current Diffusion-Based Text-to-Video Models\n\n1.  **Instruction Following**:\n    *   Generating videos involving complex action sequences.\n    *   Incorporating multiple concepts with low occurrence in the training data within a single generated video.\n2.  **Laws of Physics**:\n    *   Struggling to accurately simulate the real world.\n    *   Generating videos that adhere to the laws of physics (e.g., a ball bouncing on the floor).\n3.  **Data Quality**:\n    *   Video captioning models often face hallucination issues.\n    *   High-quality labeled data remains a significant hurdle.\n    *   Human annotations are expensive and difficult to scale.\n4.  **Computational Cost**:\n    *   Training and generating long-duration, high-resolution videos still face significant computational cost hurdles.\n5.  **Optimization**:\n    *   Defining similar tasks in the video generation domain remains challenging, unlike RL-focused natural language tasks.\n\n***\n\n### How Step-Video-T2V Attempts to Address These Challenges\n\n1.  **High-Quality Labeled Data for Post-training**:\n\n    *   Employs high-quality human-labeled data in **SFT** to achieve significant improvements in the overall video quality.\n    *   Focuses on the quality and diversity of the data, which outweighs its sheer scale.\n2.  **Instruction Following**:\n\n    *   Analyzes the instruction-following capability of Step-Video-T2V, focusing on how it interprets instructions involving various objects, actions, and other details.\n    *   Balances the attention for all elements in the prompt to refine the model\u2019s ability to better attend and follow all elements in the prompt.\n3.  **Laws of Physics Following**:\n\n    *   Plans to develop more advanced model paradigms in future work, such as combining autoregressive and diffusion models within a unified framework, to better adhere to the laws of physics and more accurately simulate realistic interactions.\n4.  **Video-VAE for High Compression**:\n\n    *   Introduces a deep compression **Video-VAE** for video foundation models, achieving **16x16 spatial** and **8x temporal compression ratios**, while maintaining exceptional video reconstruction quality.\n    *   Reduces the computational complexity of large-scale video generation training.\n5.  **Bilingual Text Encoders**:\n\n    *   Uses two bilingual text encoders to process user text prompts: **Hunyuan-CLIP** and **Step-LLM**.\n    *   Handles user prompts of varying lengths, generating robust text representations that effectively guide the model in the latent space.\n6.  **Video-based DPO Approach**:\n\n    *   Applies a video-based **DPO** approach to reduce artifacts and improve the visual quality of the generated videos.\n    *   Enhances the alignment with the given prompts, resulting in more accurate and relevant video generation.\n7.  **Cascaded Training Pipeline**:\n\n    *   Introduces a cascaded training pipeline, including text-to-image pre-training, text-to-video pre-training, supervised fine-tuning (**SFT**), and direct preference optimization (**DPO**), to accelerate model convergence and fully leverage video datasets of varying quality.\n8.  **Hierarchical Data Filtering**:\n\n    *   Applies a series of filters to the data, progressively increasing their thresholds to create six pre-training subsets for **Step-2: T2VI Pre-training**.\n    *   Constructs the final **SFT** dataset through manual filtering.\n9.  **RL-based Optimization Mechanism for Post-training**:\n\n    *   Explores training a reward model to automate the entire post-training process.\n    *   Identifies this as a key challenge for future research exploration."
    },
    {
        "question_id": "2502.10248v1_6",
        "answer": "Step-Video-T2V uses **Flow Matching** as its training objective, which involves predicting the velocity field that transforms noise into a coherent video frame. This approach offers several benefits for video generation.\n\n***\n\n### Flow Matching in Step-Video-T2V\n\n1.  **Training Process**:\n    *   A Gaussian noise `$X_0$` is sampled from a normal distribution `$N(0, 1)$`.\n    *   A random time step `$t$` is sampled from a uniform distribution between 0 and 1 (i.e., `$t \u2208 [0, 1]$`).\n    *   The input `$X_t$` is constructed as a linear interpolation between the noise `$X_0$` and the target sample `$X_1$`, where `$X_1$` is the noise-free input:\n\n        `$X_t = (1 - t) \\cdot X_0 + t \\cdot X_1$`\n    *   The ground truth velocity `$V_t$` is defined as the rate of change of `$X_t$` with respect to `$t$`:\n\n        `$V_t = \frac{dX_t}{dt} = X_1 - X_0$`\n    *   The model predicts velocity `$u(X_t, y, t; \u03b8)$` at time step `$t$`, given input `$X_t$` and conditional input `$y$`.\n    *   The model is trained to minimize the **Mean Squared Error (MSE)** loss between the predicted velocity and the true velocity:\n\n        `$loss = E_{t, X_0, X_1, y} [\\|u(X_t, y, t; \u03b8) - V_t\\|^2]$`\n\n2.  **Inference Process**:\n    *   Random noise `$X_0$` is sampled from a normal distribution `$N(0, 1)$`.\n    *   The denoised sample `$X_1$` is recovered by iteratively refining the noise through an **ODE-based method**.\n    *   A sequence of time steps `$\\{t_0, t_1, ..., t_n\\}$` is defined, where `$t_0 = 0$` and `$t_n = 1$`.\n    *   The denoised sample `$X_1$` is expressed as:\n\n        `$X_1 = \\sum_{i=0}^{n-1} u(X_{t_i}, y, t_i; \u03b8) \\cdot (t_{i+1} - t_i)$`\n\n        where `$u(X_{t_i}, y, t_i; \u03b8)$` represents the predicted velocity at time step `$t_i$`.\n\n***\n\n### Benefits of Flow Matching for Video Generation\n\n1.  **Stable Training**:\n    *   Flow matching provides a more stable training process compared to other diffusion-based methods. By directly predicting the velocity field, the model learns a smooth and continuous transformation from noise to data, avoiding abrupt changes that can lead to instability.\n2.  **High-Quality Samples**:\n    *   The method ensures that the generated video frames are coherent and realistic. The model learns to map noise to data in a structured way, which is essential for producing visually appealing and consistent video content.\n3.  **Flexibility**:\n    *   Flow matching can be combined with various network architectures, allowing for flexible model designs. In Step-Video-T2V, it is integrated with a **DiT** architecture, leveraging the strengths of both approaches.\n4.  **Efficiency**:\n    *   The approach simplifies the generation process by mapping noise to data through a continuous flow, making it computationally efficient. This efficiency is particularly important for video generation, which typically requires significant computational resources.\n5.  **Integration with DPO**:\n    *   Flow matching can be effectively combined with **Direct Preference Optimization (DPO)** to further refine the visual quality of the generated videos. This combination allows the model to incorporate human feedback, leading to more visually pleasing results."
    },
    {
        "question_id": "2502.10248v1_7",
        "answer": "The Step-Video-T2V model employs a multi-stage data filtering strategy to refine the training data, which is crucial for achieving high-quality video generation. This strategy involves progressively applying filters with increasing thresholds to create several pre-training subsets.\n\nHere's a breakdown:\n\n1.  **Initial Filtering**: The process begins with a broad filtering of the initial dataset. This step aims to remove low-quality or irrelevant videos, significantly reducing the dataset size while improving its overall quality.\n\n2.  **Clustering and Centroid Distance**: Videos are grouped into clusters based on their content using a **VideoCLIP model**. The \"Distance to Centroid\" value is then used to remove videos that are outliers within their respective clusters. This ensures that the remaining videos are representative of their categories, maintaining diversity while focusing on core concepts.\n\n3.  **Manual Filtering**: In the final stage, human evaluators assess each video for clarity, aesthetics, motion quality, and the absence of artifacts like watermarks or subtitles. Captions are also manually refined for accuracy, including details about camera movements, subjects, actions, backgrounds, and lighting.\n\n4.  **CLIP Score Filtering**: A **CLIP score** is computed to measure the alignment between video content and textual descriptions. This score assesses how well the captions align with the visual content of the video clips, ensuring that only videos with accurate and relevant captions are retained.\n\nThe importance of this hierarchical filtering approach lies in its ability to address several key challenges in training video generation models:\n\n*   **Data Quality**: By filtering out low-quality videos, the model is trained on a cleaner dataset, leading to more stable training and better generalization.\n*   **Diversity**: The clustering-based filtering helps maintain diversity within the dataset by ensuring that each cluster is adequately represented.\n*   **Accuracy**: Manual refinement of captions ensures that the model learns accurate associations between video content and text descriptions, which is crucial for text-to-video generation.\n*   **Efficiency**: By progressively reducing the dataset size, the filtering process reduces the computational cost of training, allowing for more efficient use of resources."
    },
    {
        "question_id": "2502.10248v1_8",
        "answer": "Step-Video-T2V is positioned as a competitive alternative to commercial video generation models, such as Sora and Veo, with a focus on specific strengths and addressing key limitations in the field. Let's break down the comparison based on the information provided in the paper:\n\n### Overall Performance\n\n*   **General Text Prompts**: Step-Video-T2V is designed to deliver comparable performance for general text prompts. This suggests that, for common video generation tasks, the model can produce results that are on par with those from commercial engines.\n*   **Specific Domains**: Step-Video-T2V surpasses commercial models in particular areas, such as generating videos with high motion dynamics or text content. This indicates a specialization that could make it preferable for certain applications.\n\n***\n\n### Key Features and Capabilities\n\n*   **Model Size and Training**: Step-Video-T2V is a large model with 30B parameters, indicating a substantial capacity for learning and generating complex video content. It is trained using a cascaded training pipeline, including text-to-image pre-training, text-to-video pre-training, supervised fine-tuning (SFT), and direct preference optimization (DPO), which helps accelerate model convergence and leverage video datasets of varying quality.\n*   **Video Quality**: The model can generate high-quality videos featuring strong motion dynamics, high aesthetics, and consistent content. It supports high-resolution video generation (544x992) up to 204 frames.\n*   **Bilingual Text Prompts**: Step-Video-T2V supports both Chinese and English text prompts, broadening its applicability and user base.\n\n***\n\n### Technical Innovations\n\n*   **Video-VAE**: A deep compression Variational Autoencoder (VAE) achieves 16x16 spatial and 8x temporal compression ratios. This significantly reduces the computational complexity of large-scale video generation training.\n*   **Diffusion Transformer (DiT)**: Like many commercial engines, Step-Video-T2V uses a DiT-based model trained using Flow Matching.\n*   **Video-based DPO**: A video-based Direct Preference Optimization (DPO) approach is applied to reduce artifacts and improve the visual quality of the generated videos.\n\n***\n\n### Limitations and Challenges\n\n*   **Instruction Following**: The model, like others, struggles with complex action sequences or generating videos that combine multiple low-occurrence concepts.\n*   **Laws of Physics**: Current models face difficulties in generating videos that adhere to the laws of physics.\n*   **Data and Training**: The model's performance is affected by the quantity and quality of training data, particularly in the post-training phase. The paper notes that Step-Video-T2V has seen less high-quality data in post-training compared to commercial engines.\n\n***\n\n### Evaluation and Benchmarking\n\n*   **Step-Video-T2V-Eval**: A new benchmark dataset is created for text-to-video generation, including 128 diverse prompts across 11 categories. This benchmark is used to compare Step-Video-T2V with other open-source and commercial engines.\n*   **Comparisons**: Evaluations show that Step-Video-T2V demonstrates state-of-the-art performance among open-source models and competitive results compared to commercial engines, particularly in motion dynamics.\n\n***\n\n### Summary\n\nStep-Video-T2V is a state-of-the-art video generation model that holds its own against commercial models like Sora and Veo. Its strengths lie in handling motion dynamics, supporting bilingual prompts, and employing efficient compression techniques. While it faces common challenges such as instruction following and adherence to the laws of physics, ongoing improvements in data quality, training strategies, and model architecture promise to further enhance its capabilities."
    },
    {
        "question_id": "2502.10248v1_9",
        "answer": "Human feedback plays a crucial role in refining Step-Video-T2V's video generation by improving the visual quality of the generated videos and ensuring better alignment with user prompts. This is achieved through a specific implementation in the training pipeline.\n\n***\n\n### Incorporation of Human Feedback\n\n1.  **Direct Preference Optimization (DPO):**\n\n    *   Step-Video-T2V uses DPO to incorporate human feedback. The goal is to adjust the model to generate preferred data while avoiding non-preferred data, based on human preference data.\n    *   A reference policy is used to stabilize training and prevent the current policy from deviating too far.\n2.  **Data Collection for Training:**\n\n    *   A diverse prompt set is constructed by randomly selecting prompts from the training data and synthesizing prompts based on real-world user interaction patterns.\n    *   Step-Video-T2V generates multiple videos for each prompt using different seeds. Human annotators then rate the preference of these samples.\n    *   Quality control personnel monitor the annotation process to ensure accuracy and consistency, resulting in a set of preference and non-preference data.\n3.  **Training Process:**\n\n    *   At each training step, a prompt and its corresponding positive and negative sample pairs are selected. Each sample is generated by the model itself to ensure smooth updates and improve training stability.\n    *   The positive and negative samples are aligned by fixing the initial noise and timestep, contributing to a more stable training process.\n4.  **Addressing Gradient Issues:**\n\n    *   The training objective is based on the DiffusionDPO method and DPO, modified to extend it to the Flow Matching framework.\n    *   To avoid gradient explosion, the parameter $\beta$ is reduced, and the learning rate is increased, leading to faster convergence.\n5.  **Reward Model Integration (Proposed):**\n\n    *   To address the saturation of improvements as the model distinguishes between positive and negative samples more easily, a reward model is proposed.\n    *   This reward model dynamically evaluates the quality of newly generated samples during training and is periodically fine-tuned with newly annotated human feedback to maintain alignment with the evolving policy.\n    *   By integrating the reward model, the training data is scored and ranked on-the-fly (on-policy), improving data efficiency.\n\n***"
    }
]