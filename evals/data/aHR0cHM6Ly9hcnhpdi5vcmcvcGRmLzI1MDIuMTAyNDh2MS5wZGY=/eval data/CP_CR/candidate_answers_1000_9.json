[
    {
        "question_id": "2502.10248v1_0",
        "answer": "The paper defines two levels of video foundation models, each representing a different stage of advancement in capabilities.\n\n***\n\n### Level 1: Translational Video Foundation Model\n\nThis level focuses on cross-modal translation, where the model generates videos from various input types such as text, images, or multimodal contexts. These models function primarily as sophisticated conversion systems, transforming one type of input into a video output. Current diffusion-based text-to-video models like Step-Video (described in the report), Sora, Veo, Kling and Hailuo fall into this category. These models excel at creating high-quality videos from text prompts, which lowers the barrier for content creators to produce video content.\n\n***\n\n### Level 2: Predictable Video Foundation Model\n\nThis level involves more advanced predictive capabilities, functioning similarly to large language models (LLMs). Models at this level can forecast future events based on text, visual, or multimodal inputs and handle more complex tasks. These tasks include reasoning with multimodal data and simulating real-world scenarios. This requires the model to understand and model underlying causal relationships within videos, enabling it to perform causal or logical tasks."
    },
    {
        "question_id": "2502.10248v1_1",
        "answer": "Step-Video-T2V employs a specialized Video-VAE architecture to achieve significant spatial and temporal compression while preserving the quality of video reconstructions. Here's a breakdown of the key components and techniques that enable this:\n\n### Architecture Overview\n\n*   **High Compression Ratios**: The Video-VAE achieves a **16x16 spatial** and **8x temporal compression ratio**.\n*   **Dual-Path Architecture**: The architecture incorporates a novel dual-path design in the later stages of the encoder and the early stages of the decoder, facilitating unified spatial-temporal compression.\n*   **3D Convolutions and Pixel Unshuffling**: It uses a combination of 3D convolutions and optimized pixel unshuffling operations to achieve **8x16x16 downscaling**.\n\n### Encoder Details\n\n*   **Causal 3D Convolutional Modules**: The encoder's early stages feature three stages, each containing two **Causal Res3DBlocks** and corresponding downsample layers. A **MidBlock** combines convolutional layers with attention mechanisms to refine the compressed representations further.\n*   **Temporal Causality**: To enable joint image and video modeling, temporal causal 3D convolution is implemented. The convolution ensures that frame $t$ only depends on previous frames:\n\n    $C3D(X)_t = \begin{cases}\n    Conv3D([0, ..., X_t], \\Theta) & t = 0 \\\n    Conv3D([X_{t-k}, ..., X_t], \\Theta) & t > 0\n    \\end{cases}$\n\n    where $k$ is the temporal kernel size.\n*   **Dual-Path Latent Fusion**:\n    *   **Conv Path**: Combines causal 3D convolutions with pixel unshuffling:\n\n        $H_{conv} = U^{(3)}_s(C3D(X))$\n\n        where $U^{(3)}_s : R^{B \times C \times T_{st} \times H_{ss} \times W_{ss}} \rightarrow R^{B \times C \\cdot s^3 \times \frac{T}{s_t} \times \frac{H}{s_s} \times \frac{W}{s_s}}$ with spatial stride $s_s = 2$, temporal stride $s_t = 2$, and $C3D$ denoting the causal 3D convolution.\n    *   **Shortcut Path**: Preserves structural semantics through grouped channel averaging:\n\n        $H_{avg} = \frac{1}{s^3} \\sum_{k=0}^{s^3-1} U^{(3)}_s (X) [..., kC_z:(k+1)C_z]$\n\n        where $U^{(3)}_s$ implements 3D pixel unshuffle with spatial-temporal blocking, and $C_z$ is the latent dimension of the next stage.\n    *   **Fusion Output**: Combines both paths through residual summation:\n\n        $Z = H_{conv} \\oplus H_{avg}$\n\n### Decoder Details\n\n*   **Symmetric Dual Path Architectures**: The decoder's early stages consist of two symmetric dual-path architectures. The 3D pixel unshuffle operation $U$ is replaced by a 3D pixel shuffle operator $P$, and the grouped channel averaging path is replaced by a grouped channel repeating operation.\n*   **Spatial Group Normalization**: All groupnorm layers in the ResNet backbone are replaced with spatial groupnorm to avoid temporal flickering between different chunks.\n\n### Training Details\n\n*   **Multi-Stage Training**: The VAE training process is divided into multiple stages.\n    *   **Initial Training**: A VAE with a **4x8x8 compression ratio** is trained without a dual-path structure, jointly on images and videos.\n    *   **Dual-Path Enhancement**: The model is enhanced by incorporating two dual-path modules in both the encoder and decoder. The dual-path modules, mid-block, and ResNet backbone are gradually unfrozen during this phase.\n*   **Loss Functions**: A combination of **L1 reconstruction loss**, **Video-LPIPS**, and **KL-divergence constrain** is used. **GAN loss** is introduced to further refine the model\u2019s performance.\n\n### Performance Metrics\n\n*   The Video-VAE maintains state-of-the-art reconstruction performance despite having a significantly larger compression ratio compared to other methods.\n*   **SSIM**: 0.9776\n*   **PSNR**: 39.37\n*   **rFVD**: 3.61\n\nBy combining these architectural innovations and training strategies, Step-Video-T2V\u2019s Video-VAE effectively balances high compression with exceptional video reconstruction quality."
    },
    {
        "question_id": "2502.10248v1_2",
        "answer": "Direct Preference Optimization (DPO) is a technique used to align a model's output with human preferences. Instead of using reinforcement learning, which can be complex and unstable, DPO directly optimizes the model based on explicit comparisons between preferred and non-preferred samples.\n\n***\n\n### How DPO Works:\n\n1.  **Preference Data:** DPO relies on a dataset of comparisons. For a given input (e.g., a text prompt), multiple outputs are generated, and human annotators rank these outputs based on which one they prefer.\n\n2.  **Objective Function:** The DPO objective aims to adjust the model (policy) so that it generates outputs that are more like the preferred samples and less like the non-preferred samples. A reference policy is used to prevent the updated policy from deviating too far. The objective function can be formulated as:\n\n    $L_{DPO} = -E_{(y, x_w, x_l) \\sim D} \\left[ log \\ \\sigma \\left( \beta \\left( log \frac{\\pi_\theta(x_w|y)}{\\pi_{ref}(x_w|y)} - log \frac{\\pi_\theta(x_l|y)}{\\pi_{ref}(x_l|y)} \right) \right) \right]$\n\n    where:\n\n    *   $\\pi_\theta$ is the current policy (model).\n    *   $\\pi_{ref}$ is the reference policy (model).\n    *   $x_w$ is the preferred sample.\n    *   $x_l$ is the non-preferred sample.\n    *   $y$ is the condition (e.g., text prompt).\n    *   $\beta$ is a hyperparameter controlling the strength of the preference.\n    *   $\\sigma$ is the sigmoid function.\n\n3.  **Gradient Optimization**: The gradient of the DPO loss is used to update the model's parameters. The equation for the gradient is:\n\n    $\frac{\\partial L_{DPO}}{\\partial \theta} \\propto -\beta(1 - \\sigma(\beta z)) \\cdot \frac{\\partial z}{\\partial \theta}$,\n\n    where $z$ represents the terms inside the log sigmoid function in the DPO loss equation.\n\n### How DPO Improves Visual Quality in Step-Video-T2V:\n\n1.  **Human Feedback Integration**: Step-Video-T2V incorporates human feedback by having annotators rate the preference of generated videos for a given prompt. This creates a dataset of preferred and non-preferred video samples.\n\n2.  **Training with Preference Data**: The model is then trained using the DPO objective to align its video generation with these human preferences. The goal is to adjust the model to generate videos more like the preferred ones and less like the non-preferred ones.\n\n3.  **Visual Quality Enhancement:** By training with DPO, Step-Video-T2V improves the visual quality of the generated videos in several ways:\n\n    *   **Plausibility and Consistency**: Human feedback enhances the plausibility and consistency of generated videos.\n    *   **Alignment with Prompts**: DPO improves the alignment with the given prompts, resulting in more accurate and relevant video generation.\n    *   **Artifact Reduction**: The DPO stage in the training process helps in reducing artifacts in the generated videos.\n\n4.  **Addressing Limitations**: The paper also notes some limitations and potential improvements for future work:\n\n    *   **Dynamic Conditions**: The trajectory from initial noise to timestep-specific latents could be better utilized.\n    *   **Feedback Precision**: Addressing the tradeoff between sparse and imprecise feedback in video diffusion models.\n    *   **Optimization Efficiency**: Exploring more efficient preference optimization methods compared to regression-based diffusion models."
    },
    {
        "question_id": "2502.10248v1_3",
        "answer": "The Step-Video-T2V model leverages a Diffusion Transformer (DiT) architecture with 3D full attention, offering several key advantages for video generation:\n\n*   **Superior Modeling of Spatial and Temporal Information**: 3D full attention allows the model to capture both spatial and temporal relationships within the video data more effectively compared to spatial-temporal attention mechanisms. This leads to a better understanding of motion dynamics and overall video structure.\n*   **High-Quality Video Generation**: The enhanced modeling capability of 3D full attention results in the generation of videos with smoother and more consistent motion. This is particularly noticeable in scenarios with high motion dynamics.\n*   **Effective Use of Text Prompts**: The DiT architecture incorporates cross-attention layers that enable the model to attend to textual information while processing visual features. By using two distinct bilingual text encoders (Hunyuan-CLIP and Step-LLM), the model can handle user prompts of varying lengths and complexities, generating videos that align well with the given text descriptions.\n*   **Scalability and Performance**: While 3D full attention is computationally demanding, the Step-Video-T2V model optimizes this aspect through techniques like Adaptive Layer Normalization (AdaLN) with optimized computation and Query-Key Normalization (QK-Norm), ensuring stable and efficient training.\n*   **High Compression VAE**: By using a specially designed deep compression Variational Auto-encoder (**VAE**) achieving 16x16 spatial and 8x temporal compression ratios the computational complexity of large-scale video generation training is significantly reduced."
    },
    {
        "question_id": "2502.10248v1_4",
        "answer": "Step-Video-T2V uses two bilingual text encoders, Hunyuan-CLIP and Step-LLM, to process user text prompts in both English and Chinese.\n\n*   **Hunyuan-CLIP:** This is a bidirectional text encoder from an open-source CLIP model. It produces text representations that align well with the visual space due to the CLIP model's training mechanism. However, it has a limitation of a maximum input length of 77 tokens, which can be a challenge when processing longer user prompts.\n*   **Step-LLM:** This is an in-house, unidirectional text encoder pre-trained using the next-token prediction task. It incorporates a redesigned Alibi-Positional Embedding to improve efficiency and accuracy in sequence processing. Unlike Hunyuan-CLIP, Step-LLM has no input length restriction, making it effective for handling lengthy and complex text sequences.\n\nThe outputs from these two encoders are concatenated along the sequence dimension, creating the final text embedding sequence, which is then injected into a cross-attention layer within the DiT model. This allows the model to generate videos conditioned on the input prompt.\n\n***\n\nAdvantages of using two separate text encoders:\n\n*   **Handling Varying Prompt Lengths:** By combining Hunyuan-CLIP and Step-LLM, Step-Video-T2V can handle user prompts of varying lengths effectively. Hunyuan-CLIP is suitable for shorter prompts due to its alignment with the visual space, while Step-LLM excels at processing longer and more complex text sequences.\n*   **Robust Text Representations:** The combination of these two encoders generates robust text representations that effectively guide the model in the latent space. This ensures that the model can understand and interpret a wide range of text prompts, leading to more accurate and relevant video generation.\n*   **Leveraging Different Strengths:** Hunyuan-CLIP's strength lies in its ability to produce text representations well-aligned with the visual space, which is beneficial for generating videos that accurately reflect the content of the text prompt. Step-LLM's strength lies in its ability to handle long and complex text sequences, which is beneficial for generating videos that capture the nuances and details of the text prompt."
    },
    {
        "question_id": "2502.10248v1_5",
        "answer": "Current diffusion-based text-to-video models face several significant challenges, and **Step-Video-T2V** attempts to address these in various ways. Here's a breakdown:\n\n***\n\n### 1. High-Quality Labeled Data\n\n*   **Challenge:** Existing video captioning models often struggle with hallucination issues, leading to inaccurate descriptions of video content. Additionally, creating human annotations is expensive and difficult to scale.\n*   **Step-Video-T2V's Approach:** The model uses a small amount of high-quality, human-labeled data during supervised fine-tuning (SFT). This approach demonstrates that the quality and diversity of the data outweigh its sheer scale. The paper also suggests building a comprehensive video knowledge base with structured labels for future work.\n\n***\n\n### 2. Instruction Following\n\n*   **Challenge:** Current models struggle with generating videos based on detailed descriptions, handling complex action sequences, and combining multiple concepts, especially those with low occurrence in the training data.\n*   **Step-Video-T2V's Approach:** The model architecture incorporates bilingual text encoders to understand both Chinese and English prompts directly. The paper also identifies the importance of ensuring all elements in the prompt receive appropriate attention, suggesting that balancing attention across different elements can improve instruction following.\n\n***\n\n### 3. Laws of Physics\n\n*   **Challenge:** Many models struggle to accurately simulate the real world and generate videos that adhere to the laws of physics.\n*   **Step-Video-T2V's Approach:** The paper acknowledges this limitation of diffusion-based models. Future work involves developing more advanced model paradigms, such as combining autoregressive and diffusion models within a unified framework, to better adhere to the laws of physics and simulate realistic interactions.\n\n***\n\n### 4. Computational Cost\n\n*   **Challenge:** Training and generating long-duration, high-resolution videos requires significant computational resources.\n*   **Step-Video-T2V's Approach:** A specially designed deep compression Variational Auto-encoder (VAE) achieves high spatial and temporal compression ratios, significantly reducing the computational complexity of large-scale video generation training.\n\n***\n\n### 5. Preference Optimization\n\n*   **Challenge:** Applying reinforcement learning (RL) based optimization mechanisms to video generation is challenging because, unlike in natural language tasks, defining well-defined problems with clear answers remains difficult.\n*   **Step-Video-T2V's Approach:** The model employs a video-based Direct Preference Optimization (DPO) approach to reduce artifacts and improve the visual quality of the generated videos. The paper also explores training a reward model to automate the entire post-training process, though this still requires human labeling efforts in the early stages."
    },
    {
        "question_id": "2502.10248v1_6",
        "answer": "Step-Video-T2V employs Flow Matching as a core training technique. This approach involves predicting the velocity field that transforms an initial noise distribution into the desired data distribution, which in this case is video frames. Here's a breakdown:\n\n1.  **Flow Matching Process:**\n    *   **Noise Sampling**: The process begins by sampling Gaussian noise, denoted as $X_0$, from a standard normal distribution.\n    *   **Timestep Sampling**: A random timestep $t$ is sampled from a uniform distribution between 0 and 1 (i.e., $t \\in [0, 1]$).\n    *   **Interpolation**: The model input $X_t$ is constructed as a linear interpolation between the initial noise $X_0$ and the target sample $X_1$ (the noise-free input). This is mathematically represented as:\n        $X_t = (1 - t) \\cdot X_0 + t \\cdot X_1$\n    *   **Velocity Calculation**: The ground truth velocity $V_t$ represents the rate of change of $X_t$ with respect to the timestep $t$, defined as:\n        $V_t = \frac{dX_t}{dt} = X_1 - X_0$\n        This essentially captures the direction and magnitude of change from the initial noise to the target data.\n    *   **Model Training**: The model is trained to predict the velocity field $u(X_t, y, t; \theta)$, where $X_t$ is the input at timestep $t$, $y$ is an optional conditioning input (e.g., text prompt), and $\theta$ represents the model parameters. The training objective is to minimize the Mean Squared Error (**MSE**) loss between the predicted velocity and the true velocity $V_t$. The loss function is:\n        $loss = E_{t, X_0, X_1, y} [\\|u(X_t, y, t; \theta) - V_t\\|^2]$\n\n***\n\n2.  **Benefits of Flow Matching for Video Generation:**\n\n    *   **Stable Training Dynamics:** Flow Matching provides more stable training dynamics compared to traditional diffusion models. By directly predicting the velocity field, the model avoids issues like vanishing gradients, which can occur when iteratively denoising an image or video frame.\n    *   **Efficient Sampling:** Flow Matching facilitates efficient sampling during inference. The denoising process is carried out by integrating over a sequence of timesteps using an Ordinary Differential Equation (**ODE**) solver. This approach allows for faster generation of high-quality videos with fewer steps.\n    *   **High-Quality Video Generation:** By learning to predict the instantaneous rate of change of the noisy sample with respect to time, the model can accurately reverse the diffusion process and recover data samples from noise. This results in the generation of videos with strong motion dynamics, high aesthetics, and consistent content.\n    *   **Flexibility and Control:** Flow Matching allows for incorporating conditioning inputs such as text prompts, enabling precise control over the generated video content. The model can effectively translate text descriptions into realistic video sequences by conditioning the velocity field on the given text embeddings."
    },
    {
        "question_id": "2502.10248v1_7",
        "answer": "The Step-Video-T2V employs a **hierarchical data filtering** approach to refine its training data. This process progressively increases the thresholds of various filters, creating multiple subsets used for the **T2VI pre-training** phase. The final **SFT dataset** is constructed through manual filtering.\n\n***\n\nHere's a breakdown of the key aspects and their importance:\n\n### Key Filters Applied:\n\n1.  **Aesthetic Score**:\n    *   Uses a **CLIP-based aesthetic predictor** to assess the visual appeal of video frames.\n    *   Importance: Ensures the training data consists of aesthetically pleasing videos, which helps the model learn to generate visually attractive content.\n2.  **NSFW Score**:\n    *   Employs a **CLIP-based NSFW detector** to identify and remove inappropriate content.\n    *   Importance: Filters out content unsuitable for safe work environments, ensuring the model does not generate offensive or explicit material.\n3.  **Watermark Detection**:\n    *   Uses an **EfficientNet image classification model** to detect the presence of watermarks.\n    *   Importance: Removes videos with watermarks, which can be distracting and negatively impact the quality of the generated content.\n4.  **Subtitle Detection**:\n    *   Utilizes **PaddleOCR** to recognize and localize text within video frames.\n    *   Importance: Identifies and removes clips with excessive on-screen text or captions, ensuring the model focuses on generating visual content rather than reproducing text-heavy videos.\n5.  **Saturation Score**:\n    *   Assesses color saturation by converting video frames to **HSV color space** and extracting the saturation channel.\n    *   Importance: Filters out videos with poor color quality, ensuring the model learns from well-saturated and visually appealing content.\n6.  **Blur Score**:\n    *   Detects blurriness by applying the **variance of the Laplacian method** to measure the sharpness of each frame.\n    *   Importance: Removes blurry videos, ensuring the model trains on clear and sharp visual data.\n7.  **Black Border Detection**:\n    *   Uses **FFmpeg** to detect black borders in frames.\n    *   Importance: Facilitates cropping to remove distracting edges, ensuring the model trains on content free of these artifacts.\n8.  **Centroid-based Filtering**:\n    *   Filters video embeddings based on distance from cluster centroids to maintain diversity.\n9.  **Human Annotation and Refinement**:\n    *   Human evaluators assess videos for clarity, aesthetics, motion, and absence of artifacts.\n    *   Captions are manually refined for accuracy.\n\n***\n\n### Why Hierarchical Data Filtering is Important:\n\n*   **Improved Data Quality**: By systematically filtering the data based on various quality metrics, the training dataset is refined to include only high-quality videos. This leads to better model performance and more visually appealing generated content.\n*   **Enhanced Model Stability**: High-quality data reduces noise and inconsistencies in the training process, leading to more stable and reliable model training.\n*   **Better Generalization**: Training on a diverse yet high-quality dataset allows the model to generalize better to various scenarios and prompts, improving its ability to generate diverse and realistic videos.\n*   **Efficient Training**: By focusing on high-quality data, the model can learn more effectively and efficiently, reducing the need for extensive computational resources and training time.\n*   **Alignment with Human Perception**: Filters like aesthetic score and human annotation ensure that the training data aligns with human preferences, resulting in generated videos that are more visually pleasing and realistic.\n\n***\n\nIn summary, **hierarchical data filtering** is a crucial step in training high-quality video generation models like Step-Video-T2V. By systematically refining the training data, the model can learn more effectively, generalize better, and generate videos that align with human preferences and expectations."
    },
    {
        "question_id": "2502.10248v1_8",
        "answer": "Okay, let's break down how Step-Video-T2V stacks up against commercial models such as Sora and Veo, based on the information provided in the paper. I will focus on performance aspects and capabilities.\n\n***\n\n### General Comparison\n\n*   **Overall Performance**: Step-Video-T2V delivers comparable performance for general text prompts and even surpasses commercial engines in specific domains, such as generating videos with high motion dynamics or text content.\n\n*   **Complexity**: Commercial video generation engines often involve longer and more complex video generation pipelines with extensive pre- and post-processing. Step-Video-T2V aims to deliver competitive performance without such complexity.\n\n***\n\n### Specific Capabilities and Features\n\n*   **Model Size and Training Data**:\n    *   Step-Video-T2V is a 30B parameter model.\n    *   It was trained using a cascaded training pipeline, including text-to-image pre-training, text-to-video pre-training, supervised fine-tuning (**SFT**), and direct preference optimization (**DPO**).\n*   **Video Resolution and Length**: Step-Video-T2V generates videos at a 544x992 resolution, up to 204 frames in length.\n*   **Compression**: Utilizes a deep compression Variational Autoencoder (**VAE**) achieving 16x16 spatial and 8x temporal compression ratios.\n*   **Bilingual Support**: Supports bilingual text prompts in both English and Chinese.\n*   **DPO**: Implements a video-based **DPO** approach to reduce artifacts and enhance visual quality.\n*   **Motion Dynamics**: Demonstrates strong capabilities in modeling and generating videos with high-motion dynamics.\n*   **Text Generation**: Outperforms other models in generating basic English text within videos, attributed to the text-to-image pre-training stage.\n*   **Limitations**: Struggles to generalize well when generating videos involving complex action sequences or requiring adherence to the laws of physics.\n\n***\n\n### Benchmarking\n\n*   **Step-Video-T2V-Eval**: The paper introduces a new benchmark dataset called **Step-Video-T2V-Eval** for text-to-video generation, which includes 128 diverse prompts across 11 categories.\n*   **Movie Gen Video Bench**: Compared to Movie Gen Video, Step-Video-T2V achieves a comparable performance.\n*   **Comparison to Commercial Models**: The paper compares Step-Video-T2V with two leading text-to-video engines, **T2VTopA** and **T2VTopB**, on **Step-Video-T2V-Eval**.\n\n***\n\n### Key Differentiators\n\n*   **Open Source**: Step-Video-T2V is open-source, providing greater transparency and accessibility compared to closed-source commercial engines.\n*   **Compression Ratio**: The **Video-VAE** achieves higher compression ratios (16x16 spatial and 8x temporal) compared to some other models like HunyuanVideo (8x8 spatial and 4x temporal).\n*   **Bilingual Support**: Supports both Chinese and English prompts.\n*   **Video-Based DPO**: Applies a video-based **DPO** approach to reduce artifacts and improve visual quality.\n\n***\n\n### Areas for Improvement\n\n*   **Pre-training**: The pre-training of Step-Video-T2V remains insufficient compared to models like Movie Gen Video.\n*   **Data Quality**: Lacks enough high-quality labeled data to effectively refine the visual style and quality of the generated results.\n*   **Resolution**: Generates 540P videos, which are visually less appealing than the higher resolutions (720P or 1080P) produced by some commercial models.\n*   **Instruction Following**: T2VTopA has better instruction-following capability, leading to superior performance in categories such as Combined Concepts, Surreal, and Cinematography.\n*   **Text Generation**: The accuracy of text generation remains far from ideal, especially for Chinese characters."
    },
    {
        "question_id": "2502.10248v1_9",
        "answer": "Human feedback is integral to refining **Step-Video-T2V**, enhancing the visual quality of generated videos and ensuring better alignment with user prompts. Here's how it's implemented:\n\n***\n\n### Pipeline Overview\n\n1.  **Prompt Set Construction**:\n\n    *   A diverse set of prompts is created by randomly selecting from training data and synthesizing new prompts based on real-world user interaction patterns.\n2.  **Video Generation**:\n\n    *   **Step-Video-T2V** generates multiple videos for each prompt using different seeds to create varied samples.\n3.  **Human Annotation**:\n\n    *   Human annotators rate the preference of these samples, with quality control to ensure accuracy and consistency.\n4.  **Preference Data**:\n\n    *   This process results in a set of preference and non-preference data, which serves as the foundation for model training.\n5.  **Model Training**:\n\n    *   At each training step, a prompt and its corresponding positive and negative sample pairs are selected.\n    *   Samples are generated by the model itself to ensure smooth updates and improve overall training stability.\n    *   Positive and negative samples are aligned by fixing the initial noise and timestep, contributing to a more stable training process.\n\n***\n\n### Direct Preference Optimization (DPO)\n\n*   **Method Selection**: Direct Preference Optimization (**DPO**) is chosen for incorporating human feedback due to its effectiveness and simplicity.\n*   **Objective**: Adjust the model to align with the generation of preferred data while avoiding non-preferred data.\n*   **Reference Policy**: A reference policy (reference model) is introduced to stabilize training and prevent the current policy from deviating too far.\n*   **Policy Objective Formulation**:\n\n    $L_{DPO} = -E_{(y, x_w, x_l) \\sim D} \\left[ \\log \\sigma \\left( \beta \\left( \\log \frac{\\pi_\theta(x_w|y)}{\\pi_{ref}(x_w|y)} - \\log \frac{\\pi_\theta(x_l|y)}{\\pi_{ref}(x_l|y)} \right) \right) \right]$\n\n    *   $\\pi_\theta$ and $\\pi_{ref}$ refer to the current and reference policies, respectively.\n    *   $x_w$ and $x_l$ are the preferred and non-preferred samples, respectively.\n    *   $y$ denotes the condition.\n\n***\n\n### Training Process\n\n1.  **Sample Selection**:\n\n    *   A prompt and its corresponding positive and negative sample pairs are selected at each training step.\n2.  **Data Consistency**:\n\n    *   Positive and negative samples are aligned by fixing the initial noise and timestep to maintain consistency.\n3.  **Objective**:\n\n    *   The training objective is based on the **DiffusionDPO** method and **DPO**, with slight modifications to extend its application to video generation.\n4.  **Visual Quality**:\n\n    *   **Video-based DPO training** is employed to enhance the visual quality of the generated videos and ensure better alignment with user prompts."
    }
]