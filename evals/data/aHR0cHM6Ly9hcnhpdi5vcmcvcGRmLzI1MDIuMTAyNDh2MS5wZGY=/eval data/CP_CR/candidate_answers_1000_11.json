[
    {
        "question_id": "2502.10248v1_0",
        "answer": "The paper defines two levels of video foundation models, each with distinct capabilities:\n\n1.  **Level-1: Translational Video Foundation Model**:\n    *   This type of model functions as a cross-modal translation system.\n    *   It can generate videos from text, visual, or multimodal contexts.\n    *   Current diffusion-based text-to-video models (like Sora, Veo, and Step-Video-T2V) fall into this category.\n    *   These models excel at generating high-quality videos from text prompts, making video creation more accessible.\n2.  **Level-2: Predictable Video Foundation Model**:\n    *   This model acts as a prediction system, similar to large language models (LLMs).\n    *   It can forecast future events based on text, visual, or multimodal context.\n    *   It is capable of handling more advanced tasks, such as reasoning with multimodal data or simulating real-world scenarios.\n\n***\n\nThe key difference lies in their capabilities: Level-1 models primarily translate inputs into videos, while Level-2 models can predict and reason about video content, similar to how LLMs handle language. Level 2 models have causal modeling mechanism by predicting the next video token, frame, or clip."
    },
    {
        "question_id": "2502.10248v1_1",
        "answer": "To achieve high **spatial and temporal compression** while maintaining **video reconstruction quality**, Step-Video-T2V's **Video-VAE** incorporates several key architectural and training innovations:\n\n***\n\n### 1. Dual-Path Architecture\n\n*   **Unified Spatial-Temporal Compression**: The Video-VAE employs a novel dual-path architecture in the later stages of the encoder and the early stages of the decoder. This design achieves 8x16x16 downscaling using 3D convolutions and optimized pixel shuffling operations.\n\n*   **Conv Path**: Combines causal 3D convolutions with pixel unshuffling to maintain high-frequency details:\n\n    $H_{conv} = U^{(3)}_s(C3D(X))$\n\n    where $U^{(3)}_s$: $R^{B \times C \times T_{st} \times H_{ss} \times W_{ss}} \rightarrow R^{B \times C \\cdot s^3 \times \frac{T}{st} \times \frac{H}{ss} \times \frac{W}{ss}}$ with spatial stride $s_s = 2$, temporal stride $s_t = 2$, and $C3D$ denoting causal 3D convolution.\n\n*   **Shortcut Path**: Preserves structural semantics through grouped channel averaging:\n\n    $H_{avg} = \frac{1}{s^3} \\sum_{k=0}^{s^3-1} U^{(3)}_s (X)[\\dots, kC_z:(k+1)C_z]$\n\n    where $U^{(3)}_s$ implements 3D pixel unshuffle with spatial-temporal blocking, and $C_z$ is the latent dimension of the next stage.\n\n*   **Fusion**: The output of fusion combines both paths through residual summation:\n\n    $Z = H_{conv} \\oplus H_{avg}$\n\n***\n\n### 2. Causal 3D Convolutional Modules\n\n*   The encoder's early stage features three stages, each with two Causal **Res3DBlock** and corresponding downsample layers. A **MidBlock** combines convolutional layers with attention mechanisms to refine the compressed representations further.\n\n*   **Temporal Causality**: Achieved through:\n\n    $C3D(X)_t = \begin{cases}\n    Conv3D([0, ..., X_t], \\Theta) & t = 0 \\\n    Conv3D([X_{t-k}, ..., X_t], \\Theta) & t > 0\n    \\end{cases}$\n\n    where $k$ is the temporal kernel size, ensuring frame $t$ only depends on previous frames.\n\n***\n\n### 3. Decoder Architecture\n\n*   The decoder's early stage consists of two symmetric Dual Path architectures. The 3D pixel unshuffle operation $U$ is replaced by a 3D pixel shuffle operator $P$, and the grouped channel averaging path is replaced by a grouped channel repeating operation.\n\n*   **Spatial GroupNorm**: Replaces all groupnorm with spatial groupnorm in the **ResNet** backbone to prevent temporal flickering between different chunks.\n\n***\n\n### 4. Multi-Stage Training Process\n\n*   **Stage 1**: Trains a **VAE** with a 4x8x8 compression ratio without a dual-path structure, jointly on images and videos to learn low-level representations.\n\n*   **Stage 2**: Enhances the model by incorporating two dual-path modules in both the encoder and decoder, gradually unfreezing the dual-path modules, the **Mid-Block**, and the **ResNet** backbone.\n\n*   **Loss Functions**: Utilizes a combination of **L1 reconstruction loss**, **Video-LPIPS**, and **KL-divergence constrain**. **GAN loss** is introduced after convergence to further refine the model\u2019s performance.\n\n***\n\n### 5. Performance Metrics\n\n*   Achieves state-of-the-art performance, maintaining high reconstruction quality despite having an 8 times larger compression ratio than most baselines. For example, it achieves **SSIM** of 0.9776 and **PSNR** of 39.37.\n\n*   Significantly outperforms other baselines in challenging video reconstruction cases, including high-motion, text, texture, and combinations thereof."
    },
    {
        "question_id": "2502.10248v1_2",
        "answer": "Direct Preference Optimization (**DPO**) is a technique used to fine-tune generative models by directly optimizing for human preferences. Instead of using reinforcement learning, which can be unstable and complex, **DPO** reframes the problem as a classification task, making it more stable and easier to implement. The core idea is to adjust the model to generate outputs that are preferred by humans while avoiding outputs they dislike, based on a set of preference data.\n\n***\n\n### How DPO Works in Step-Video-T2V\n\n1.  **Preference Data Collection**:\n    *   A diverse set of prompts is created, partly from the training data and partly synthesized by human annotators to mimic real-world user interactions.\n    *   For each prompt, multiple videos are generated using different random seeds.\n    *   Human annotators then rate these videos, indicating which ones they prefer and which ones they don't. Quality control measures are in place to ensure accuracy and consistency.\n2.  **Training Objective**:\n    *   **DPO** aims to maximize the likelihood of preferred samples while minimizing the likelihood of non-preferred samples. A reference policy (a reference model) is used to prevent the current policy from deviating too far during training.\n    *   The loss function is defined as:\n        $L_{DPO} = -E_{(y, x_w, x_l) \\sim D} \\left[ \\log \\sigma \\left( \beta \\left( \\log \frac{\\pi_\theta(x_w|y)}{\\pi_{ref}(x_w|y)} - \\log \frac{\\pi_\theta(x_l|y)}{\\pi_{ref}(x_l|y)} \right) \right) \right]$\n        *   $\\pi_\theta$ is the current policy (the model being trained).\n        *   $\\pi_{ref}$ is the reference policy (the reference model).\n        *   $x_w$ is the preferred sample.\n        *   $x_l$ is the non-preferred sample.\n        *   $y$ is the condition (prompt).\n        *   $\beta$ is a hyperparameter controlling the strength of the preference optimization.\n3.  **Training Process**:\n    *   During each training step, a prompt and its corresponding positive (preferred) and negative (non-preferred) sample pairs are selected.\n    *   These samples are generated by the model itself to ensure smooth and stable updates.\n    *   The initial noise and timestep are fixed to align the positive and negative samples, further stabilizing the training process.\n4.  **Addressing Gradient Issues**:\n    *   The paper notes that a large $\beta$ can cause gradient explosion, requiring gradient clipping and a low learning rate, which slows convergence.\n    *   To mitigate this, the authors reduce $\beta$ and increase the learning rate for faster convergence.\n\n***\n\n### How DPO Improves Visual Quality in Step-Video-T2V\n\n1.  **Enhanced Plausibility and Consistency**:\n    *   Human feedback enhances the plausibility and consistency of generated videos.\n2.  **Better Alignment with Prompts**:\n    *   **DPO** improves the alignment with given prompts, resulting in more accurate and relevant video generation.\n3.  **Addressing Limitations**:\n    *   The paper acknowledges that improvements can saturate when the model easily distinguishes between positive and negative samples because the training data is generated by earlier versions of the model.\n    *   To address this, a reward model is trained using human-annotated feedback data to dynamically evaluate the quality of newly generated samples during training, improving data efficiency.\n4.  **Preference Score**:\n    *   The baseline model with **DPO** achieves a preference score of 55%, outperforming the baseline model (45%), demonstrating the effectiveness of **Video-DPO** in generating videos more aligned with user preferences."
    },
    {
        "question_id": "2502.10248v1_3",
        "answer": "The Step-Video-T2V model leverages a Diffusion Transformer (DiT) architecture with 3D full attention, offering several key advantages:\n\n*   **Superior Modeling of Spatial and Temporal Information:** 3D full attention allows the model to capture both spatial and temporal relationships within videos more effectively than spatial-temporal attention mechanisms. This leads to better performance, particularly in generating videos with complex motion dynamics.\n*   **High-Quality Video Generation:** The enhanced modeling capability results in the generation of videos with smoother and more consistent motion, which contributes to higher overall video quality.\n*   **Effective Handling of Video Data:** By employing techniques like RoPE-3D (Rotation-based Positional Encoding), the model can process videos with varying frame counts and resolutions without being restricted by fixed positional encoding lengths. This improves generalization across diverse video data.\n*   **Text Prompt Incorporation:** The model uses cross-attention layers to incorporate text prompts, allowing it to attend to textual information while processing visual features. This enables the generation of videos conditioned on the input prompt.\n*   **Computational Efficiency:** While 3D full attention is computationally demanding, the model incorporates optimizations like Adaptive Layer Normalization (AdaLN) with optimized computation and Query-Key Normalization (QK-Norm) to improve overall model efficiency and training stability."
    },
    {
        "question_id": "2502.10248v1_4",
        "answer": "Step-Video-T2V uses a dual approach to handle bilingual text prompts, leveraging two distinct text encoders: Hunyuan-CLIP and Step-LLM. Here's a breakdown of how it works and the advantages this approach provides:\n\n### Bilingual Text Encoding in Step-Video-T2V\n\n1.  **Two Separate Text Encoders:**\n    *   **Hunyuan-CLIP:** This is a bidirectional text encoder from an open-source bilingual CLIP model. It excels at producing text representations that are well-aligned with the visual space due to the CLIP model's training mechanism.\n    *   **Step-LLM:** This is an in-house, unidirectional bilingual text encoder pre-trained using the next-token prediction task. It is designed with a redesigned Alibi-Positional Embedding to enhance efficiency and accuracy in sequence processing.\n\n2.  **Processing Text Prompts:** User text prompts, whether in English or Chinese, are processed by both Hunyuan-CLIP and Step-LLM.\n\n3.  **Concatenation of Embeddings:** The outputs from these two encoders are then concatenated along the sequence dimension. This creates a combined text embedding sequence that captures information from both encoders.\n\n4.  **Cross-Attention Mechanism:** This combined embedding is injected into a cross-attention layer within the diffusion transformer (DiT) architecture. This allows the model to attend to textual information while processing visual features, enabling the generation of videos conditioned on the input prompt.\n\n### Advantages of Using Two Separate Text Encoders\n\n1.  **Handling Varying Input Lengths:**\n    *   Hunyuan-CLIP has a limitation on the maximum input length (77 tokens), which can be a challenge when processing longer user prompts.\n    *   Step-LLM, on the other hand, has no input length restriction, making it effective for handling lengthy and complex text sequences.\n    *   By combining these two encoders, Step-Video-T2V can handle user prompts of varying lengths effectively.\n\n2.  **Robust Text Representations:** The combination of the two encoders allows Step-Video-T2V to generate robust text representations. Hunyuan-CLIP ensures good alignment with the visual space, while Step-LLM handles complex text sequences.\n\n3.  **Improved Efficiency and Accuracy:** Step-LLM incorporates a redesigned Alibi-Positional Embedding, which improves both efficiency and accuracy in sequence processing. This enhancement contributes to the overall performance of the model.\n\nIn summary, Step-Video-T2V's approach of using two bilingual text encoders allows it to effectively handle a wide range of text prompts, generate robust text representations, and guide the model in the latent space for high-quality video generation."
    },
    {
        "question_id": "2502.10248v1_5",
        "answer": "Current diffusion-based text-to-video models grapple with several significant challenges. Here's a breakdown, along with how Step-Video-T2V attempts to tackle them:\n\n### Key Challenges in Diffusion-Based Text-to-Video Models\n\n*   **High-Quality Labeled Data:** A significant hurdle is the scarcity of high-quality, accurately captioned video data. Existing video captioning models often produce inaccurate descriptions (**hallucinations**), and manual annotation is expensive and difficult to scale.\n*   **Instruction Following:** These models often struggle with complex instructions, such as generating videos based on detailed descriptions, handling intricate action sequences, or combining multiple concepts, especially those with low occurrence in training data.\n*   **Adherence to Physical Laws:** Generating videos that accurately simulate the real world and obey the laws of physics remains a challenge. Models often fail to produce realistic interactions, such as a ball bouncing naturally or water flowing realistically.\n*   **Computational Cost:** Training and generating long-duration, high-resolution videos is computationally expensive.\n\n***\n\n### How Step-Video-T2V Addresses These Challenges\n\n*   **High-Quality Labeled Data:** Step-Video-T2V emphasizes the importance of high-quality data by using a small amount of carefully curated, human-labeled data in its supervised fine-tuning (**SFT**) stage. This approach demonstrates that data quality and diversity outweigh sheer scale. The model also aims to build a comprehensive video knowledge base with structured labels in the future.\n*   **Instruction Following:** The model recognizes the instruction-following problem and attempts to address it by ensuring all elements in the prompt receive appropriate attention. The report mentions plans to refine the model's ability to better attend to and follow all elements in the prompt, although specific techniques are left for future work.\n*   **Adherence to Physical Laws:** Step-Video-T2V acknowledges the limitations of diffusion-based models in simulating realistic physics. The report suggests exploring more advanced model paradigms, such as combining autoregressive and diffusion models within a unified framework, to better adhere to the laws of physics.\n*   **Computational Cost:** Step-Video-T2V employs a deep compression **Video-VAE** (Variational Autoencoder) that achieves 16x16 spatial and 8x temporal compression ratios. This significantly reduces the computational complexity of large-scale video generation training. The model also details optimizations of model hyper-parameters, operators, and parallelism to ensure training stability and efficiency.\n\n***\n\nIn summary, Step-Video-T2V addresses these challenges through a combination of data curation strategies, architectural choices (**Video-VAE**), and training techniques (**SFT**, **DPO**). While the model doesn't completely solve all of these issues, it represents a significant step forward and provides a strong baseline for future research."
    },
    {
        "question_id": "2502.10248v1_6",
        "answer": "Step-Video-T2V employs Flow Matching as its training objective, which offers a method for training continuous normalizing flows. Here's how it works and why it's beneficial:\n\n***\n\n### Flow Matching in Step-Video-T2V\n\n1.  **Sampling Noise:**\n    -   A Gaussian noise `$X_0$` is sampled from a standard normal distribution `$N(0, 1)$`.\n    -   A random time step `$t$` is sampled from a uniform distribution between 0 and 1 (i.e., `$t \u2208 [0, 1]$`).\n2.  **Constructing Model Input:**\n    -   The model input `$X_t$` is constructed as a linear interpolation between `$X_0$` and the target sample `$X_1$`, where `$X_1$` is the noise-free input.\n    -   This is defined as:\n        ```\n        $X_t = (1 - t) \\cdot X_0 + t \\cdot X_1$\n        ```\n3.  **Ground Truth Velocity:**\n    -   The ground truth velocity `$V_t$` represents the rate of change of `$X_t$` with respect to the time step `$t$`.\n    -   It's defined as:\n        ```\n        $V_t = \frac{dX_t}{dt} = X_1 - X_0$\n        ```\n    -   `$V_t$` captures the direction and magnitude of change from the initial noise `$X_0$` to the target data `$X_1$`.\n4.  **Training the Model:**\n    -   The model is trained to predict the velocity field. The training loss minimizes the Mean Squared Error (MSE) between the predicted velocity `$u(X_t, y, t; \u03b8)$` and the true velocity `$V_t$`. Here, `$u(X_t, y, t; \u03b8)$` denotes the model\u2019s predicted velocity at time step `$t$`, given input `$X_t$` and an optional conditioning input `$y$` (e.g., a bilingual sentence).\n    -   The training loss is given by:\n        ```\n        $loss = E_{t,X_0,X_1,y} \\left[ \\|u(X_t, y, t; \u03b8) - V_t\\|^2 \right]$\n        ```\n        where the expectation is taken over all training samples. The term `$\u03b8$` represents the model parameters.\n5.  **Inference:**\n    -   During inference, random noise `$X_0$` is sampled from `$N(0, 1)$`. The goal is to recover the denoised sample `$X_1$` by iteratively refining the noise through an ODE-based method.\n    -   A sequence of time steps `$\\{t_0, t_1, ..., t_n\\}$` is defined, where `$t_0 = 0$`, `$t_n = 1$`, and `$t_0 < t_1 < ... < t_n$`.\n    -   The denoising process is carried out by integrating over these time steps. The denoised sample `$X_1$` can be expressed as:\n        ```\n        $X_1 = \\sum_{i=0}^{n-1} u(X_{t_i}, y, t_i; \u03b8) \\cdot (t_{i+1} - t_i)$\n        ```\n\n***\n\n### Benefits of Flow Matching for Video Generation\n\n1.  **Stable Training:**\n    -   Flow Matching provides a more stable training process compared to other diffusion-based methods. By directly predicting the velocity field, the model learns a smoother and more continuous transformation from noise to data.\n2.  **High-Quality Samples:**\n    -   The method ensures that the model learns to predict the instantaneous rate of change of the noisy sample with respect to `$t$`, which can later be used to reverse the diffusion process and recover data samples from noise effectively. This leads to the generation of high-quality video frames.\n3.  **Efficient Inference:**\n    -   The ODE-based inference allows for a controlled and efficient denoising process. By integrating over a sequence of time steps, the model can gradually refine the noise into a coherent video frame.\n4.  **Flexibility:**\n    -   Flow Matching is flexible and can be conditioned on various inputs, such as text prompts, allowing the model to generate videos based on textual descriptions.\n5.  **Reduced Artifacts:**\n    -   The method is designed to reduce artifacts and ensure smoother, more realistic video outputs, enhancing the visual quality of the generated videos.\n\nIn summary, Step-Video-T2V uses Flow Matching to ensure stable training, generate high-quality video samples, and provide flexibility in conditioning the video generation process, which is essential for producing realistic and coherent videos from text prompts."
    },
    {
        "question_id": "2502.10248v1_7",
        "answer": "The Step-Video-T2V model employs a hierarchical data filtering approach to refine its training data, progressively increasing the thresholds of filters to create multiple pre-training subsets. This method is crucial for training high-quality video generation models because it systematically weeds out noisy or low-quality data, allowing the model to focus on learning from the most informative examples.\n\nHere's a breakdown of the approach and its importance:\n\n1.  **Key Filters Applied:**\n\n    *   **Aesthetic Score:** Uses a CLIP-based aesthetic predictor to assess the visual appeal of video frames and remove those that do not meet a certain aesthetic standard.\n    *   **NSFW Score:** Employs a CLIP-based NSFW detector to filter out content inappropriate for safe work environments.\n    *   **Watermark Detection:** Detects and removes videos containing watermarks.\n    *   **Subtitle Detection:** Identifies and removes clips with excessive on-screen text or captions.\n    *   **Saturation Score:** Assesses color saturation to filter out videos with poor color quality.\n    *   **Blur Score:** Detects and removes blurry videos caused by camera shake or lack of clarity.\n    *   **Black Border Detection:** Detects and removes videos with black borders, ensuring the model trains on content free of distracting edges.\n    *   **Centroid Filtering:** Removes videos whose features are outliers compared to the cluster centroid, ensuring diversity within the subset.\n    *   **Human Annotation:** Human evaluators assess each video for clarity, aesthetics, appropriate motion, smooth scene transitions, and the absence of watermarks or subtitles, manually refining captions to ensure accuracy.\n2.  **Importance for Training High-Quality Video Generation Models:**\n\n    *   **Improved Data Quality:** By filtering out low-quality videos, the model is trained on a cleaner dataset, leading to better performance.\n    *   **Enhanced Learning Efficiency:** Training on high-quality data allows the model to learn more effectively, reducing the need for extensive training iterations.\n    *   **Better Generalization:** A refined dataset helps the model generalize better to unseen data, improving its ability to generate diverse and realistic videos.\n    *   **Reduced Artifacts:** Filtering out videos with watermarks, subtitles, and other artifacts ensures that the generated videos are free from these issues.\n    *   **Increased Stability:** High-quality data contributes to more stable training, preventing the model from learning undesirable patterns or behaviors."
    },
    {
        "question_id": "2502.10248v1_8",
        "answer": "Okay, let's break down how Step-Video-T2V stacks up against commercial models like Sora and Veo.\n\n***\n\n### General Comparison\n\n*   **Comparable Performance:** Step-Video-T2V delivers performance that is on par with closed-source commercial engines for general text prompts.\n*   **Specific Domain Excellence:** It even surpasses commercial models in specific areas, such as generating videos with high motion dynamics or text content.\n*   **Transparency:** Unlike the closed-source nature of Sora, Veo, and others, Step-Video-T2V offers greater transparency due to its open-source implementation.\n\n***\n\n### Key Differentiators of Step-Video-T2V\n\n*   **Model Size:** Step-Video-T2V is the largest open-source model to date.\n*   **High-Compression VAE:** It utilizes a high-compression **VAE** for videos.\n*   **Bilingual Text Prompts:** It supports both English and Chinese text prompts.\n*   **Video-Based DPO:** It implements a video-based **DPO** approach to reduce artifacts and enhance visual quality.\n*   **Comprehensive Documentation:** It provides comprehensive training and inference documentation.\n\n***\n\n### Limitations and Future Improvements\n\n*   **Training Data:** Step-Video-T2V uses significantly less high-quality data in the post-training phase compared to commercial engines.\n*   **Video Length and Resolution:** It generates videos with a length of 204 frames at 540P resolution. The paper suggests longer video generation and higher resolution can be areas of improvement.\n*   **General Domain Performance:** With comparable training resources and high-quality data, Step-Video-T2V can achieve state-of-the-art results in general domains.\n\n***\n\n### Evaluations and Metrics\n\nWhen comparing Step-Video-T2V with commercial models **T2VTopA** and **T2VTopB**, the following observations were noted:\n\n1.  **Aesthetic Appeal:** Commercial models have higher aesthetic appeal due to higher resolutions and high-quality aesthetic data used during post-training.\n2.  **Motion Dynamics:** Step-Video-T2V demonstrates strong capabilities in modeling and generating videos with high-motion dynamics.\n3.  **Instruction Following:** Commercial models exhibit better instruction-following capability due to better video captioning models and greater human effort in labeling post-training data.\n\nThe evaluation metrics used include:\n\n*   **Metric-1:** Win/Tie/Loss labels assigned by human annotators to compare Step-Video-T2V with a target model.\n*   **Metric-2:** Scores assigned to each generated video to measure its quality across dimensions like instruction following, motion smoothness, physical plausibility, and aesthetic appeal."
    },
    {
        "question_id": "2502.10248v1_9",
        "answer": "Human feedback plays a crucial role in refining the video generation quality of Step-Video-T2V by aligning the model's output with human preferences, particularly in terms of visual appeal and instruction following. This is achieved through a carefully designed training pipeline that incorporates **Direct Preference Optimization (DPO)**.\n\n***\n\n### Implementation of Human Feedback in Step-Video-T2V's Training Pipeline:\n\n1.  **Data Collection**:\n\n    *   A diverse set of prompts is created, partly from the training data and partly synthesized by human annotators to mimic real-world user interactions.\n    *   Step-Video-T2V generates multiple videos for each prompt using different random seeds.\n    *   Human annotators then evaluate these videos, rating their preferences to create pairs of preferred and non-preferred samples.\n    *   Quality control personnel monitor the annotation process to ensure accuracy and consistency.\n2.  **DPO Training**:\n\n    *   **DPO** is used to adjust the model based on human preferences. The goal is to make the model generate more of the preferred data and less of the non-preferred data under the same conditions.\n    *   A reference policy (reference model) is introduced to prevent the current policy from deviating too far from it, stabilizing the training process.\n\n***\n\n### Policy Objective:\n\nThe policy objective can be formulated as:\n\n$L_{DPO} = -E_{(y, x_w, x_l) \\sim D} \\left[ \\log \\sigma \\left( \beta \\left( \\log \frac{\\pi_\theta(x_w|y)}{\\pi_{ref}(x_w|y)} - \\log \frac{\\pi_\theta(x_l|y)}{\\pi_{ref}(x_l|y)} \right) \right) \right]$\n\nWhere:\n\n*   $\\pi_\theta$ and $\\pi_{ref}$ refer to the current policy and reference policy, respectively.\n*   $x_w$ and $x_l$ are the preferred and non-preferred samples, respectively.\n*   $y$ denotes the condition (prompt).\n\n***\n\n### Training Process:\n\n1.  At each step, a prompt and its corresponding positive and negative sample pairs are selected.\n2.  Each sample is generated by the model itself, ensuring smooth updates and improving overall training stability.\n3.  The initial noise and timestep are fixed to align the positive and negative samples, contributing to a more stable training process.\n4.  The training objective is based on the **DiffusionDPO** method but extends it to the **Flow Matching** framework with slight modifications.\n\n***\n\n### Addressing Limitations:\n\n*   The paper identifies that improvements saturate when the model easily distinguishes between positive and negative samples because the training data is generated by earlier versions of the model.\n*   To address this, a reward model is trained using human-annotated feedback data to dynamically evaluate the quality of newly generated samples during training.\n*   The reward model is periodically fine-tuned to maintain alignment with the evolving policy, improving data efficiency by scoring and ranking training data on-the-fly (on-policy).\n\n***\n\n### Overall Impact:\n\nHuman feedback, implemented through **DPO** and enhanced with a reward model, significantly improves the visual quality, coherence, and relevance of the videos generated by Step-Video-T2V. This approach ensures that the model not only produces high-quality videos but also aligns well with user expectations and preferences."
    }
]