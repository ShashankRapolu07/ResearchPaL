[
    {
        "question_id": "2502.10248v1_0",
        "answer": "The paper defines two levels of video foundation models, each with distinct capabilities and complexities:\n\n### Level-1: Translational Video Foundation Model\n\n*   This level focuses on cross-modal translation.\n*   It functions primarily as a system that generates videos from various inputs such as text, visual data, or multimodal contexts.\n*   Current diffusion-based text-to-video models like **Step-Video-T2V**, **Sora**, **Veo**, **Kling**, and **Hailuo** fall into this category.\n*   These models excel at producing high-quality videos from text prompts, thus lowering the barrier for video content creation.\n*   They are limited in generating videos that require complex action sequences or adherence to physical laws.\n*   These models mainly learn mappings between prompts and videos without modeling underlying causal relationships.\n\n***\n\n### Level-2: Predictable Video Foundation Model\n\n*   This level acts as a prediction system.\n*   It forecasts future events based on text, visual, or multimodal contexts.\n*   It handles advanced tasks such as reasoning with multimodal data or simulating real-world scenarios.\n*   It aims to perform causal or logical tasks, similar to large language models (**LLMs**).\n*   It requires explicit modeling of causal relationships within videos, which is a limitation of current Level-1 models.\n*   Autoregression-based models introduce causal modeling by predicting the next video token, frame, or clip but still lag behind diffusion-based models in text-to-video generation quality.\n\n***\n\nIn essence, Level-1 models translate inputs into videos, while Level-2 models predict future video content based on given contexts, requiring a deeper understanding of causality and real-world physics."
    },
    {
        "question_id": "2502.10248v1_1",
        "answer": "Step-Video-T2V uses a specialized **Video-VAE** architecture to achieve high compression ratios while preserving video quality. Here's how it works:\n\n***\n\n### Key Features of the Video-VAE\n\n*   **High Compression Ratios**: Achieves 16x16 spatial and 8x temporal compression.\n*   **Dual-Path Architecture**: Introduces a novel dual-path architecture with unified spatial-temporal compression in the later stage of the encoder and the early stage of the decoder.\n*   **Causal 3D Convolutions**: Employs temporal causal 3D convolutions to enable joint image and video modeling, ensuring frame dependencies.\n*   **Multi-Stage Training**: Uses a meticulously designed multi-stage training process to achieve efficient and high-quality video data modeling.\n\n***\n\n### Architecture Details\n\n1.  **Encoder**:\n\n    *   The encoder \\(E\\) processes an input video tensor \\(X \\in \\mathbb{R}^{B \times C \times T \times H \times W}\\) to produce a latent representation \\(Z = E(X) \\in \\mathbb{R}^{B \times C_z \times \\lceil T/8 \rceil \times \\lceil H/16 \rceil \times \\lceil W/16 \rceil}\\).\n    *   It consists of three stages, each featuring two Causal **Res3DBlock** modules and corresponding downsample layers. A **MidBlock** combines convolutional layers with attention mechanisms to refine the compressed representations.\n2.  **Dual-Path Latent Fusion**:\n\n    *   **Conv Path**: Combines causal 3D convolutions with pixel unshuffling:\n\n        $H_{conv} = U_s^{(3)}(C3D(X))$\n\n        where $U_s^{(3)}: \\mathbb{R}^{B \times C \times T_{st} \times H_{ss} \times W_{ss}} \rightarrow \\mathbb{R}^{B \times C \\cdot s^3 \times T_{st} \times H_{ss} \times W_{ss}}$ with spatial stride $s_s = 2$, temporal stride $s_t = 2$, and $C3D$ denoting the causal 3D convolution.\n    *   **Shortcut Path**: Preserves structural semantics through grouped channel averaging:\n\n        $H_{avg} = \frac{1}{s^3} \\sum_{k=0}^{s^3 - 1} U_s^{(3)}(X)[\\dots, kC_z:(k+1)C_z]$\n\n        where $U_s^{(3)}$ implements 3D pixel unshuffle with spatial-temporal blocking, and $C_z$ is the latent dimension of the next stage.\n    *   The output of fusion combines both paths through residual summation:\n\n        $Z = H_{conv} \\oplus H_{avg}$\n3.  **Decoder**:\n\n    *   The decoder mirrors the encoder's structure but replaces 3D pixel unshuffling with a 3D pixel shuffle operator \\(P\\).\n    *   The grouped channel averaging path is replaced by a grouped channel repeating operation, unfolding the compressed information into spatial-temporal dimensions.\n    *   **GroupNorm** is replaced with spatial **GroupNorm** to prevent temporal flickering between different chunks.\n\n***\n\n### Training Details\n\n1.  **Initial Training**:\n\n    *   A VAE is trained with a 4x8x8 compression ratio without a dual-path structure, jointly on images and videos with a preset ratio.\n    *   A lower compression goal is set for the model to learn low-level representations sufficiently.\n2.  **Enhanced Training**:\n\n    *   Two dual-path modules are incorporated in both the encoder and decoder, replacing the latter part after the mid-block.\n    *   The dual-path modules, the mid-block, and the ResNet backbone are gradually unfrozen for refined and flexible training.\n3.  **Loss Functions**:\n\n    *   A combination of **L1 reconstruction loss**, **Video-LPIPS**, and **KL-divergence** constrain is used to guide the model.\n    *   **GAN loss** is introduced to further refine the model\u2019s performance after the initial losses have converged.\n\n***\n\n### Key Advantages\n\n*   **Unified Spatial-Temporal Compression**: The dual-path architecture achieves 8\u00d716\u00d716 downscaling using 3D convolutions and optimized pixel unshuffling operations.\n*   **High-Frequency Detail Preservation**: The convolutional path maintains high-frequency details, while the channel averaging path preserves low-frequency structure.\n*   **Efficient Parameter Utilization**: The network uses its parameters more efficiently, overcoming blurring artifacts typically associated with traditional VAEs.\n*   **Joint Image and Video Modeling**: The unified structure adeptly handles both image and video data."
    },
    {
        "question_id": "2502.10248v1_2",
        "answer": "Direct Preference Optimization (**DPO**) is a technique used to align a model's behavior with human preferences by directly optimizing the model based on feedback data, indicating which outputs are preferred over others.\n\n***\n\nHere's how **DPO** is applied in **Step-Video-T2V** to enhance the visual quality of generated videos:\n\n### Core Idea\n\nThe primary goal is to adjust the model, so it generates outputs that align more closely with human preferences while discouraging the generation of less desirable outputs.\n\n### Preference Data\n\n**DPO** relies on a dataset of preference pairs, where, for a given input (e.g., a text prompt), there are two video samples: one that is preferred ($x_w$) and one that is non-preferred ($x_l$). Human annotators create this dataset by rating the visual quality and relevance of videos generated from the same prompt but with different random seeds.\n\n### Training Objective\n\nThe training objective of **DPO** is formulated as:\n\n$L_{DPO} = -E_{(y, x_w, x_l) \\sim D} \\left[ log \\sigma \\left( \beta \\left( log \frac{\\pi_\theta(x_w|y)}{\\pi_{ref}(x_w|y)} - log \frac{\\pi_\theta(x_l|y)}{\\pi_{ref}(x_l|y)} \right) \right) \right]$\n\nWhere:\n\n*   $\\pi_\theta$ is the current policy (i.e., the model being trained).\n*   $\\pi_{ref}$ is the reference policy (i.e., a fixed, pre-trained model).\n*   $x_w$ is the preferred sample.\n*   $x_l$ is the non-preferred sample.\n*   $y$ is the condition (e.g., text prompt).\n*   $\beta$ is a temperature parameter controlling the deviation from the reference policy.\n*   $\\sigma$ is the sigmoid function.\n\n### Implementation Details\n\n1.  **Prompt Set Construction**:\n\n    *   A diverse set of prompts is created by randomly selecting from the training data and synthesizing prompts based on real-world user interaction patterns.\n2.  **Sample Generation**:\n\n    *   For each prompt, multiple videos are generated using different random seeds.\n3.  **Human Annotation**:\n\n    *   Human annotators rate the preference between the generated video samples.\n    *   Quality control ensures accuracy and consistency.\n4.  **Training Process**:\n\n    *   During training, a prompt and its corresponding positive and negative sample pairs are selected.\n    *   The model is updated to increase the likelihood of generating the preferred samples and decrease the likelihood of generating the non-preferred samples.\n    *   The initial noise and timestep are fixed for positive and negative samples to maintain consistency.\n5.  **Modification to the Flow Matching Framework**:\n\n    *   The **DPO** objective is adapted to the Flow Matching framework with slight modifications.\n\n### Addressing Gradient Issues\n\nTo stabilize training and avoid gradient explosions, the parameter $\beta$ is reduced, and the learning rate is increased. This adjustment leads to faster convergence compared to using a large $\beta$ with gradient clipping and a low learning rate.\n\n### Benefits\n\n**DPO** enhances the visual quality of generated videos by:\n\n*   **Reducing Artifacts**: Aligning the model with human preferences helps eliminate common visual artifacts and inconsistencies.\n*   **Improving Realism**: The generated videos exhibit more plausible and coherent content.\n*   **Enhancing Prompt Alignment**: The model generates videos that are more accurate and relevant to the given prompts.\n\n### Limitations\n\n*   **Data Utilization**: The training data used in **DPO** is generated by earlier versions of the model, which may lead to diminishing returns as the model evolves.\n*   **Feedback Precision**: Human feedback is often sparse and imprecise, especially in high-resolution videos where only a few pixels may be problematic.\n*   **Optimization Efficiency**: Diffusion models rely on regression, which may be less efficient for preference optimization compared to token-level softmax in **LLMs**.\n\n### Proposed Solutions\n\nTo address the limitations, the following strategies are suggested:\n\n*   **Reward Model Training**: Train a reward model using human-annotated feedback data to evaluate the quality of newly generated samples dynamically.\n*   **On-Policy Training**: Integrate the reward model into the pipeline to score and rank training data on-the-fly, improving data efficiency."
    },
    {
        "question_id": "2502.10248v1_3",
        "answer": "The Step-Video-T2V model leverages a diffusion transformer (DiT) architecture with 3D full attention, offering several key advantages for video generation.\n\n***\n\n### Enhanced Spatial and Temporal Modeling\n\n*   3D full attention allows the model to capture both spatial and temporal information in a unified attention process. This approach has a theoretical upper bound for modeling both spatial and temporal information in videos.\n*   The model can generate videos with smooth and consistent motion, which has been observed in large-scale experiments.\n\n***\n\n### Implementation Details\n\n*   **Hyper-parameters**: The Step-Video-T2V model consists of 30B parameters and contains 48 layers. Each layer contains 48 attention heads, with each head's dimension set to 128.\n*   **Cross-Attention**: A cross-attention layer is introduced between the self-attention and feed-forward network (FFN) in each transformer block to incorporate text prompts. This enables the model to attend to textual information while processing visual features.\n*   **Text Encoders**: The prompt is embedded using two distinct bilingual text encoders, Hunyuan-CLIP and Step-LLM. The outputs from these two encoders are concatenated along the sequence dimension, creating the final text embedding sequence.\n*   **Adaptive Layer Normalization (AdaLN)**: The model uses an adaptive layer normalization (AdaLN) operation to embed timestep information. The AdaLN-Single structure is adopted to reduce the computational overhead of traditional AdaLN operations and improve overall model efficiency.\n*   **RoPE-3D**: The model uses RoPE-3D, an extension of the traditional Rotation-based Positional Encoding (RoPE), specifically designed to handle video data by accounting for temporal (frame) and spatial (height and width) dimensions.\n*   **QK-Norm:** Query-Key Normalization (QK-Norm) is used to stabilize the self-attention mechanism, which normalizes the dot product between the query (Q) and key (K) vectors, addressing numerical instability caused by large dot products that can lead to vanishing gradients or overly concentrated attention.\n\n***\n\n### Efficiency Considerations\n\n*   While 3D full attention offers higher performance potential, it comes at the cost of increased computational demands. The report mentions ongoing investigations into more efficient ways to reduce the computation overhead while preserving the same model quality."
    },
    {
        "question_id": "2502.10248v1_4",
        "answer": "Step-Video-T2V employs a dual-encoder approach to process bilingual text prompts, leveraging the strengths of two distinct models to handle the nuances of both English and Chinese languages.\n\n***\n\n### Bilingual Text Encoding in Step-Video-T2V\n\nThe model uses two text encoders:\n\n*   **Hunyuan-CLIP**: This is a bidirectional text encoder from an open-source bilingual **CLIP** model. It excels at producing text representations that align well with the visual space due to the training mechanism of **CLIP** models. However, it's limited to processing a maximum of 77 tokens, posing challenges for longer user prompts.\n*   **Step-LLM**: This is an in-house, unidirectional bilingual text encoder pre-trained using the next-token prediction task. It incorporates a redesigned **Alibi-Positional Embedding** to improve efficiency and accuracy in sequence processing. Unlike **Hunyuan-CLIP**, **Step-LLM** has no input length restrictions, making it effective for handling lengthy and complex text sequences.\n\n***\n\n### Advantages of Using Two Separate Text Encoders\n\n1.  **Handling Varying Prompt Lengths**: By combining these two text encoders, Step-Video-T2V can handle user prompts of varying lengths. **Hunyuan-CLIP** is used for shorter prompts, while **Step-LLM** is used for longer, more complex text sequences.\n2.  **Robust Text Representations**: The two encoders generate robust text representations that effectively guide the model in the latent space. **Hunyuan-CLIP** provides text representations well-aligned with the visual space, while **Step-LLM** excels at processing lengthy and complex text sequences.\n3.  **Improved Efficiency and Accuracy**: **Step-LLM** incorporates a redesigned **Alibi-Positional Embedding** to improve both efficiency and accuracy in sequence processing. This allows the model to handle complex text sequences without sacrificing performance.\n4.  **Effective Guidance in Latent Space**: The combined embeddings from the two encoders are injected into a cross-attention layer, allowing the model to generate videos conditioned on the input prompt. This enables the model to attend to textual information while processing visual features, resulting in videos more aligned with the input prompt."
    },
    {
        "question_id": "2502.10248v1_5",
        "answer": "Here's a breakdown of the challenges faced by current diffusion-based text-to-video models and how Step-Video-T2V attempts to address them:\n\n### Challenges of Current Diffusion-Based Text-to-Video Models\n\n*   **Complex Action Sequences and Physics:** Current models struggle with generating videos that require intricate action sequences or adherence to the laws of physics.\n*   **Compositionality:** Combining multiple concepts with low occurrence in the training data within a single generated video remains a difficult task.\n*   **Hallucination Issues:** Video captioning models often face hallucination issues, leading to unstable training and poor instruction-following performance.\n*   **Computational Cost:** Training and generating long-duration, high-resolution videos still face significant computational cost hurdles.\n*   **High-Quality Labeled Data:** Obtaining high-quality labeled data remains a significant hurdle due to the limitations of existing video captioning models and the expense and difficulty of human annotations.\n*   **Instruction Following:** Requires more attention, as it encompasses a wide range of scenarios, from generating videos based on detailed descriptions to handling complex action sequences and combinations of multiple concepts.\n*   **Following Laws of Physics:** Current models still face difficulties in generating videos that obey the laws of physics, an issue stemming from the inherent limitations of diffusion models.\n*   **RL-based Optimization Mechanisms:** Are areas worth exploring for post-training improvements in video generation models.\n\n***\n\n### How Step-Video-T2V Attempts to Address These Challenges\n\n*   **High-Quality Video Generation:** Step-Video-T2V is designed to generate high-quality videos from text, featuring strong motion dynamics, high aesthetics, and consistent content.\n*   **Deep Compression VAE:** A specially designed deep compression Variational Auto-encoder (**VAE**) achieves 16x16 spatial and 8x temporal compression ratios, significantly reducing the computational complexity of large-scale video generation training.\n*   **Bilingual Text Encoders:** Two bilingual text encoders enable Step-Video-T2V to directly understand Chinese or English prompts.\n*   **Cascaded Training Pipeline:** A cascaded training pipeline, including text-to-image pre-training, text-to-video pre-training, supervised fine-tuning (**SFT**), and direct preference optimization (**DPO**), is introduced to accelerate model convergence and fully leverage video datasets of varying quality.\n*   **Text-to-Image Pre-training:** Essential for the video generation model to acquire rich visual knowledge, including concepts, scenes, and their spatial relationships, providing a solid foundation for the subsequent text-to-video pre-training stages.\n*   **Text-to-Video Pre-training at Low Resolution:** Critical for the model to learn motion dynamics. The more stable the model is trained during this stage, using as much diverse training data as possible, the easier it becomes to refine and scale the model to higher resolutions and more complex video generation tasks.\n*   **High-Quality Videos in SFT:** Using high-quality videos with accurate captions and desired styles in **SFT** is crucial to the stability of the model and the style of the generated videos.\n*   **Video-based DPO:** Can further enhance the visual quality by reducing artifacts, ensuring smoother and more realistic video outputs.\n*   **Open-Sourcing:** By open-sourcing Step-Video-T2V, the aim is to provide researchers and engineers with a strong baseline, helping them better understand these challenges and accelerate innovations in the development and application of video foundation models.\n*   **Addressing Computational Challenges:** Various methods have been proposed to reduce the complexity of video modeling. These include approaches such as 3D Causal Convolution, wavelet transform and Residual Autoencoding in images.\n*   **New Benchmark Dataset:** A new benchmark dataset called Step-Video-T2V-Eval is created for text-to-video generation, which includes 128 diverse prompts across 11 categories, alongside video generation results from several top text-to-video open-source and commercial engines for comparison."
    },
    {
        "question_id": "2502.10248v1_6",
        "answer": "Step-Video-T2V employs Flow Matching as its training objective, a technique that offers a unique approach to training generative models. Here's a breakdown:\n\n### Flow Matching in Step-Video-T2V\n\n1.  **Noise Sampling:** The process begins by sampling Gaussian noise, denoted as $X_0$, from a standard normal distribution, $N(0, 1)$. A random timestep $t$ is also sampled from a uniform distribution between 0 and 1 (i.e., $t \u2208 [0, 1]$).\n\n2.  **Model Input Construction:** The model input, $X_t$, is constructed as a linear interpolation between the initial noise $X_0$ and the target sample $X_1$, where $X_1$ represents the noise-free input (i.e., the actual data sample). This is mathematically expressed as:\n\n    $X_t = (1 - t) \\cdot X_0 + t \\cdot X_1$\n\n3.  **Ground Truth Velocity Definition:** The ground truth velocity, $V_t$, represents the rate of change of $X_t$ with respect to the timestep $t$. It's defined as the difference between the target data $X_1$ and the initial noise $X_0$:\n\n    $V_t = \frac{dX_t}{dt} = X_1 - X_0$\n\n    In essence, $V_t$ captures the direction and magnitude of change required to transform the noise into the desired data sample.\n\n4.  **Model Training:** The model is trained to predict the velocity field $u(X_t, y, t; \u03b8)$ at timestep $t$, given the input $X_t$, an optional conditioning input $y$ (e.g., a text prompt), and the model parameters $\u03b8$. The training objective is to minimize the Mean Squared Error (MSE) loss between the predicted velocity and the ground truth velocity:\n\n    $loss = E_{t, X_0, X_1, y} [\\|u(X_t, y, t; \u03b8) - V_t\\|^2]$\n\n    This loss function guides the model to learn the underlying continuous transformation from noise to data.\n\n### Benefits of Flow Matching for Video Generation\n\n1.  **Stable Training:** Flow Matching provides a more stable and predictable training dynamic compared to other generative modeling techniques like GANs (Generative Adversarial Networks). By directly learning the velocity field, the training process becomes less prone to mode collapse and adversarial instability.\n\n2.  **High-Quality Samples:** The continuous nature of Flow Matching allows for generating high-quality and coherent video frames. The model learns a smooth transformation from noise to data, resulting in visually appealing and realistic video sequences.\n\n3.  **Flexibility and Control:** The conditioning input $y$ in Flow Matching enables incorporating various forms of control, such as text prompts or image inputs, allowing users to guide the video generation process effectively.\n\n4.  **Efficient Inference:** During inference, Flow Matching facilitates the use of ODE (Ordinary Differential Equation) solvers to efficiently traverse the learned velocity field, generating video frames with fewer steps than traditional diffusion models."
    },
    {
        "question_id": "2502.10248v1_7",
        "answer": "The Step-Video-T2V model employs a hierarchical data filtering approach that progressively increases the thresholds of filters to create pre-training subsets, followed by manual filtering to construct the final SFT dataset. This strategy is crucial for training high-quality video generation models for several reasons:\n\n***\n\n### Data Filtering Stages\n\nThe hierarchical data filtering approach includes several stages:\n\n1.  **Video Segmentation**: Raw videos are split into single-shot clips using scene change detection.\n2.  **Video Quality Assessment**: Video clips are evaluated and filtered using multiple **quality assessment tags** based on specific criteria. These tags include:\n\n    *   **Aesthetic Score**: Predicted using the LAION CLIP-based aesthetic predictor.\n    *   **NSFW Score**: Detected using the LAION CLIP-based NSFW detector.\n    *   **Watermark Detection**: Detected using an EfficientNet image classification model.\n    *   **Subtitle Detection**: Recognized and localized using PaddleOCR.\n    *   **Saturation Score**: Assessed by converting video frames to the HSV color space and extracting the saturation channel.\n    *   **Blur Score**: Detected using the variance of the Laplacian method.\n    *   **Black Border Detection**: Detected using FFmpeg to facilitate cropping.\n3.  **Video Motion Assessment**: Motion content is evaluated by calculating the motion score, which is the average of the mean magnitudes of the optical flow between pairs of resized grayscale frames. Evaluative tags include:\n\n    *   **Motion\\_Mean**: The average motion magnitude across all frames.\n    *   **Motion\\_Max**: The maximum motion magnitude observed in the clip.\n    *   **Motion\\_Min**: The minimum motion magnitude in the clip.\n4.  **Video Captioning**: Three types of caption labeling are introduced using an in-house Vision Language Model (VLM):\n\n    *   **Short Caption**: Concise description focusing on the main subject and action.\n    *   **Dense Caption**: Integrates key elements, emphasizing the main subject, events, environmental and visual aspects, video type and style, as well as camera shots and movements.\n    *   **Original Title**: Incorporates a portion of the original titles from the raw videos.\n5.  **Video Concept Balancing**: Addresses category imbalances and facilitates deduplication by computing embeddings for all video clips using an internal VideoCLIP model and applying K-means clustering to group them into clusters.\n\n    *   **Cluster\\_Cnt**: The total number of clips in the cluster to which the clip belongs.\n    *   **Center\\_Sim**: The cosine distance between the clip\u2019s embedding and the cluster center.\n6.  **Video-Text Alignment**: Measures the alignment between video content and textual descriptions using a **CLIP Score**.\n7.  **Post-training Data Filtering**: Utilizes both automated and manual filtering techniques to curate a high-quality video dataset:\n\n    *   Filtering by Video Assessment Scores\n    *   Filtering by Video Categories\n    *   Labeling by Human Annotators\n\n***\n\n### Importance for Training High-Quality Video Generation Models\n\n1.  **Improved Data Quality**: By systematically filtering out low-quality videos, the model trains on cleaner data, leading to better convergence and higher-quality output.\n2.  **Enhanced Motion Dynamics**: Assessing and filtering videos based on motion scores ensures that the model learns from dynamic scenes, which is crucial for generating videos with realistic motion.\n3.  **Accurate Captions**: High-quality and precise captions are essential for the model to learn the correct associations between text prompts and video content, enhancing the model's prompt-following ability.\n4.  **Balanced Dataset**: Addressing category imbalances ensures that the model does not overfit to certain types of videos, improving generalization.\n5.  **Better Video-Text Alignment**: Computing and filtering based on the CLIP Score ensures that the captions accurately reflect the video content, leading to more coherent and realistic video generation.\n6.  **Reduced Artifacts and Improved Visual Quality**: The video-based DPO approach is applied to reduce artifacts and improve the visual quality of the generated videos.\n7.  **Efficient Data Utilization**: By focusing on high-quality, human-annotated data in SFT, the model achieves significant improvements in overall video quality, demonstrating that data quality outweighs quantity.\n\n***\n\nIn summary, the hierarchical data filtering approach ensures that the Step-Video-T2V model is trained on a high-quality, diverse, and well-aligned dataset, which is crucial for achieving state-of-the-art video generation performance."
    },
    {
        "question_id": "2502.10248v1_8",
        "answer": "Step-Video-T2V is presented as achieving comparable performance to leading commercial video generation engines, such as Sora and Veo, for general text prompts. Furthermore, it even surpasses them in specific domains. These domains include generating videos with high motion dynamics or text content.\n\n***\n\nSpecifically, the key comparisons and capabilities of Step-Video-T2V are:\n\n*   **General Text Prompt Performance:** Step-Video-T2V delivers a similar level of quality compared to commercial engines for creating videos from standard text descriptions.\n*   **High Motion Dynamics:** It excels in generating videos that feature significant movement and action.\n*   **Text Content Generation:** It demonstrates strong capabilities in producing videos that include accurate text elements within the visual scene.\n*   **Model Size and Training:** Step-Video-T2V utilizes a 30B parameter model and a cascaded training pipeline, including text-to-image pre-training, text-to-video pre-training, supervised fine-tuning (SFT), and direct preference optimization (DPO).\n*   **Video Resolution:** It generates videos at a resolution of 544x992.\n*   **Video Length:** It is capable of generating videos up to 204 frames in length.\n*   **Bilingual Support:** It can understand and process both Chinese and English prompts directly due to its two bilingual text encoders.\n*   **Compression Technology:** A deep compression Variational Auto-encoder (VAE) achieves 16x16 spatial and 8x temporal compression ratios, reducing computational complexity.\n*   **DPO:** A video-based DPO approach is applied to reduce artifacts and improve the visual quality of the generated videos.\n\n***\n\nCompared to other open-source models, Step-Video-T2V has several key advantages:\n\n*   It is the largest open-source model available.\n*   It uses a high-compression VAE for videos.\n*   It supports bilingual text prompts in both English and Chinese.\n*   It implements a video-based DPO approach.\n*   It provides comprehensive training and inference documentation.\n\n***\n\nHowever, the evaluation also reveals some limitations:\n\n*   **Aesthetic Appeal:** Step-Video-T2V's generated videos sometimes have lower aesthetic appeal compared to some commercial models, potentially due to lower resolutions (540P vs. 720P or 1080P) and differences in post-training data.\n*   **Instruction Following:** Some commercial models exhibit better instruction-following capabilities, particularly in complex scenarios or when combining multiple concepts.\n*   **Data and Training Resources:** Commercial models often benefit from more extensive high-quality data and greater human effort in data labeling.\n\n***\n\nDespite these limitations, Step-Video-T2V is positioned as a state-of-the-art open-source model with strong motion dynamics modeling and generation capabilities. The authors assert that with comparable training resources and high-quality data, it could achieve state-of-the-art results in general domains as well."
    },
    {
        "question_id": "2502.10248v1_9",
        "answer": "Human feedback plays a crucial role in refining **Step-Video-T2V's** video generation by improving the visual quality of the generated videos and ensuring better alignment with user prompts. This is achieved through a specific implementation within the training pipeline.\n\n***\n\nHere's a breakdown of the process:\n\n1.  **Direct Preference Optimization (DPO):**\n\n    *   **Method Selection:** The **DPO** method is employed to incorporate human feedback. It adjusts the model to align with preferred data generation while avoiding non-preferred data generation.\n    *   **Objective:** The goal is to adjust the current policy (the model) to generate preferred data and avoid generating non-preferred data, given human preference data and non-preference data under the same conditions.\n    *   **Policy Stabilization:** A reference policy (reference model) is introduced to prevent the current policy from deviating too far.\n    *   **Loss Function:** The policy objective is formulated as:\n\n        $L_{DPO} = -E_{(y, x_w, x_l) \\sim D} \\left[ log \\sigma \\left( \beta \\left( log \frac{\\pi_\theta(x_w|y)}{\\pi_{ref}(x_w|y)} - log \frac{\\pi_\theta(x_l|y)}{\\pi_{ref}(x_l|y)} \right) \right) \right]$\n\n        where:\n\n        *   $\\pi_\theta$ and $\\pi_{ref}$ refer to the current and reference policies, respectively.\n        *   $x_w$ and $x_l$ are the preferred and non-preferred samples, respectively.\n        *   $y$ denotes the condition.\n        *   $D$ is the dataset of human preferences.\n\n2.  **Data Collection:**\n\n    *   **Prompt Set Construction:** A diverse prompt set is constructed by randomly selecting prompts from the training data and synthesizing prompts based on real-world user interaction patterns.\n    *   **Video Generation and Annotation:** **Step-Video-T2V** generates multiple videos for each prompt using different seeds, and human annotators rate the preference of these samples.\n    *   **Quality Control:** The annotation process is monitored to ensure accuracy and consistency, resulting in a set of preference and non-preference data for model training.\n    *   **Sample Alignment:** Positive and negative samples are aligned by fixing the initial noise and timestep to maintain consistency in the training data.\n\n3.  **Training Process:**\n\n    *   **Sample Selection:** At each training step, a prompt and its corresponding positive and negative sample pairs are selected.\n    *   **Model Updates:** Each sample is generated by the model itself, ensuring smooth updates and improving overall training stability.\n    *   **Modified Training Objective:** The training objective is based on the **DiffusionDPO** method and **DPO**, extended to the **Flow Matching** framework with slight modifications.\n    *   **Addressing Gradient Issues:** To address gradient explosion issues, the parameter $\beta$ is reduced, and the learning rate is increased for faster convergence.\n\n4.  **Reward Model Training (Future Work):**\n\n    *   **Addressing Saturation:** To address the saturation of improvements when the model easily distinguishes between positive and negative samples, a reward model is proposed.\n    *   **Dynamic Quality Evaluation:** The reward model dynamically evaluates the quality of newly generated samples during training.\n    *   **Periodic Fine-tuning:** The reward model is periodically fine-tuned with newly annotated human feedback to maintain alignment with the evolving policy.\n    *   **On-Policy Scoring:** By integrating the reward model into the pipeline, training data is scored and ranked on-the-fly, improving data efficiency.\n\n***"
    }
]