[
    {
        "question_id": "2502.10248v1_0",
        "answer": "The paper defines two levels of video foundation models, each with distinct capabilities:\n\n1.  **Level-1: Translational Video Foundation Model**:\n    *   This level focuses on cross-modal translation.\n    *   It generates videos from text, visual, or multimodal inputs.\n    *   It acts as a system that converts one type of input (e.g., text) into a video.\n    *   Current diffusion-based text-to-video models like Step-Video, Sora, Veo, Kling, and Hailuo belong to this level.\n\n2.  **Level-2: Predictable Video Foundation Model**:\n    *   This level functions as a prediction system, similar to large language models (LLMs).\n    *   It forecasts future events based on text, visual, or multimodal context.\n    *   It handles advanced tasks such as reasoning with multimodal data or simulating real-world scenarios.\n    *   It explicitly models the underlying causal relationships within videos, unlike Level-1 models.\n\n***\n\n**Key Differences Summarized:**\n\n*   Level 1 models translate inputs into video, while Level 2 models predict future events based on inputs.\n*   Level 1 models lack explicit causal modeling, while Level 2 models incorporate causal relationships within videos.\n*   Level 1 models are suitable for generating high-quality videos from prompts, while Level 2 models can perform complex reasoning and simulations."
    },
    {
        "question_id": "2502.10248v1_1",
        "answer": "Step-Video-T2V utilizes a specialized **Video-VAE** architecture to achieve high spatial and temporal compression while preserving video reconstruction quality. Here's a breakdown of the key components and techniques:\n\n### Architecture Overview\n\n*   The **Video-VAE** employs a dual-path architecture in the later stages of the encoder and the early stages of the decoder. This design facilitates unified spatial-temporal compression.\n*   It achieves an 8x temporal and 16x16 spatial downscaling through a combination of 3D convolutions and optimized pixel un-shuffling operations.\n\n***\n\n### Encoder Details\n\n*   **Causal 3D Convolutional Modules**: The encoder's early stages consist of three stages, each featuring two Causal **Res3DBlock** modules and corresponding downsample layers. A **MidBlock** then combines convolutional layers with attention mechanisms to refine the compressed representations further.\n*   **Temporal Causality**: To enable joint image and video modeling, temporal causal 3D convolution is employed. The temporal causality is implemented as follows:\n\n    $C3D(X)_t = \begin{cases}\n    Conv3D([0, ..., X_t], \\Theta) & t = 0 \\\n    Conv3D([X_{t-k}, ..., X_t], \\Theta) & t > 0\n    \\end{cases}$\n\n    where $k$ is the temporal kernel size, ensuring that frame $t$ only depends on previous frames.\n\n*   **Dual-Path Latent Fusion**: This module maintains high-frequency details through convolutional processing while preserving low-frequency structure via channel averaging.\n    1.  **Conv Path**: Combines causal 3D convolutions with pixel un-shuffling:\n\n        $H_{conv} = U^{(3)}_s(C3D(X))$\n\n        where $U^{(3)}_s : R^{B \times C \times T_{st} \times H_{ss} \times W_{ss}} \rightarrow R^{B \times C \\cdot s^3 \times \frac{T}{s_t} \times \frac{H}{s_s} \times \frac{W}{s_s}}$ with spatial stride $s_s = 2$, temporal stride $s_t = 2$, and $C3D$ denoting the causal 3D convolution.\n    2.  **Shortcut Path**: Preserves structural semantics through grouped channel averaging:\n\n        $H_{avg} = \frac{1}{s^3} \\sum_{k=0}^{s^3 - 1} U^{(3)}_s (X) [..., kC_z : (k+1)C_z]$\n\n        where $U^{(3)}_s$ implements 3D pixel un-shuffle with spatial-temporal blocking, and $C_z$ is the latent dimension of the next stage.\n    3.  **Fusion Output**: Combines both paths through residual summation:\n\n        $Z = H_{conv} \\oplus H_{avg}$\n\n***\n\n### Decoder Details\n\n*   The early stage of the decoder consists of two symmetric Dual Path architectures.\n*   The 3D pixel un-shuffle operation $U$ is replaced by a 3D pixel shuffle operator $P$, and the grouped channel averaging path is replaced by a grouped channel repeating operation.\n*   All groupnorm layers in the **ResNet** backbone are replaced with spatial groupnorm to avoid temporal flickering between different chunks.\n\n***\n\n### Training Details\n\nThe **VAE** training process is conducted in multiple stages:\n\n1.  **Initial Training**: A **VAE** with a 4x8x8 compression ratio is trained without a dual-path structure, jointly on images and videos.\n2.  **Dual-Path Enhancement**: Two dual-path modules are incorporated in both the encoder and decoder, gradually unfreezing the dual-path modules, the **MidBlock**, and the **ResNet** backbone.\n3.  **Loss Functions**: A combination of **L1 reconstruction loss**, **Video-LPIPS**, and **KL-divergence** constraint is used. **GAN loss** is introduced after these losses have converged.\n\n***\n\n### Performance Evaluation\n\n*   The **Video-VAE** was compared with several open-source baselines using 1,000 test videos with dimensions of 50 (frames) x 480 (height) x 768 (width).\n*   Metrics used for comparison:\n    *   **SSIM** (Structural Similarity Index)\n    *   **PSNR** (Peak Signal-to-Noise Ratio)\n    *   **rFVD** (Relative Frechet Video Distance)\n\n***\n\n### Results\n\n*   Despite having a compression ratio 8 times larger than most baselines, the reconstruction quality of **Video-VAE** maintains state-of-the-art performance.\n*   Even with a higher compression factor of 8x16x16, the reconstruction quality of Step-Video-T2V's **Video-VAE** significantly outperforms other methods."
    },
    {
        "question_id": "2502.10248v1_2",
        "answer": "Direct Preference Optimization (**DPO**) is a technique used to align a model's output with human preferences, particularly in tasks like generating text or videos. It stems from the idea of Reinforcement Learning from Human Feedback (**RLHF**), where models are adjusted based on feedback provided by humans. **DPO** aims to streamline this process, making it more intuitive and easier to implement.\n\n***\n\nHere's how **DPO** works and how it's applied in Step-Video-T2V to enhance video quality:\n\n1.  **Preference Data Collection**:\n\n    *   A diverse set of prompts is created. Some are randomly selected from the training data, while others are crafted by human annotators to mimic real-world user interactions.\n    *   For each prompt, the Step-Video-T2V model generates multiple videos using different random seeds.\n    *   Human annotators then evaluate these videos and rate their preferences, indicating which videos are preferred and which are not. Quality control measures are in place to ensure accuracy and consistency in annotations.\n2.  **Training with Preference Data**:\n\n    *   The collected preference data consists of pairs of preferred and non-preferred videos for each prompt.\n    *   The goal is to adjust the model (the \"current policy\") to generate outputs that align more closely with the preferred videos while avoiding the generation of non-preferred ones.\n    *   To stabilize training, a \"reference policy\" (a reference model) is introduced to prevent the current policy from deviating too far from it.\n3.  **Objective Function**:\n\n    *   The **DPO** objective function is designed to optimize the model based on the preference data while keeping it close to the reference policy. The objective function can be formulated as:\n\n        $L_{DPO} = -E_{(y,x_w,x_l) \\sim D} \\left[ log \\sigma \\left( \beta \\left( log \frac{\\pi_\theta(x_w|y)}{\\pi_{ref}(x_w|y)} - log \frac{\\pi_\theta(x_l|y)}{\\pi_{ref}(x_l|y)} \right) \right) \right]$\n\n        Where:\n\n        *   $\\pi_\theta$ is the current policy.\n        *   $\\pi_{ref}$ is the reference policy.\n        *   $x_w$ is the preferred sample.\n        *   $x_l$ is the non-preferred sample.\n        *   $y$ denotes the condition (prompt).\n        *   $\beta$ is a hyperparameter controlling the strength of the preference optimization.\n\n4.  **Implementation Details in Step-Video-T2V**:\n\n    *   Step-Video-T2V uses a modified version of **DPO** called DiffusionDPO, extending it to the Flow Matching framework.\n    *   To ensure stable training, the initial noise and timestep are fixed for positive and negative samples, maintaining consistency in the training data.\n    *   The training process involves selecting a prompt and its corresponding positive and negative sample pairs at each step. Each sample is generated by the model itself, ensuring smooth updates and improving overall training stability.\n5.  **Addressing Gradient Issues**:\n\n    *   A potential issue with **DPO** is that a large $\beta$ can cause gradient explosion when $z < 0$, where $z$ represents the policy-related terms in the objective function. This can lead to unstable training and slow convergence, often requiring gradient clipping and very low learning rates.\n    *   To mitigate this, Step-Video-T2V reduces $\beta$ and increases the learning rate, resulting in faster convergence.\n6.  **Iterative Refinement with a Reward Model**:\n\n    *   The model's ability to distinguish between positive and negative samples improves rapidly. The training data used in **Video-DPO** is generated by earlier versions of the model. After multiple iterations, the current policy evolves significantly, and the older data becomes outdated, leading to inefficient data utilization.\n    *   To address this, a reward model is trained using human-annotated feedback data. This model dynamically evaluates the quality of newly generated samples during training. The reward model is periodically fine-tuned with new human feedback to maintain alignment with the evolving policy.\n    *   By integrating the reward model into the pipeline, the training data is scored and ranked on-the-fly (on-policy), improving data efficiency.\n\n***\n\nIn summary, **DPO** enhances the visual quality of generated videos in Step-Video-T2V by aligning the model's output with human preferences. This is achieved through a pipeline that includes preference data collection, training with the **DPO** objective function, and iterative refinement using a reward model."
    },
    {
        "question_id": "2502.10248v1_3",
        "answer": "The Step-Video-T2V model leverages a Diffusion Transformer (DiT) architecture with 3D full attention, offering several key advantages:\n\n***\n\n### Enhanced Spatial and Temporal Modeling\n\n*   **3D full attention** captures both spatial and temporal information in a unified attention process. This is unlike spatial-temporal attention mechanisms that handle spatial and temporal aspects separately. By jointly processing spatial and temporal dimensions, the model can better understand motion dynamics and relationships within the video.\n*   Theoretically, 3D full attention has a higher upper bound for modeling spatial and temporal information in videos. Large-scale experiments have shown its superiority in generating videos with smooth and consistent motion.\n\n***\n\n### High-Quality Video Generation\n\n*   The model is capable of generating high-quality videos with strong motion dynamics, high aesthetics, and consistent content. This is achieved through the use of 3D full attention, which enhances the model's ability to capture complex movements and spatial-temporal relationships.\n*   A cascaded training pipeline, including text-to-image pre-training, text-to-video pre-training, supervised fine-tuning (**SFT**), and direct preference optimization (**DPO**), accelerates model convergence and leverages video datasets of varying quality.\n\n***\n\n### Flexibility and Adaptability\n\n*   The model uses **RoPE-3D** (Rotation-based Positional Encoding), which is designed to handle video data by accounting for temporal (frame) and spatial (height and width) dimensions. This allows the model to process videos with different frame counts and resolutions without being restricted by fixed positional encoding lengths.\n*   **RoPE-3D** improves generalization across diverse video data and effectively captures both spatial and temporal relationships within the video. It enhances the model\u2019s capacity to process and generate high-quality video content.\n\n***\n\n### Training Efficiency\n\n*   **Adaptive Layer Normalization (AdaLN)** with optimized computation reduces the computational overhead of traditional **AdaLN** operations and improves overall model efficiency. By removing class labels from **AdaLN** (since the text-to-video task does not require class labels) and adopting the **AdaLN-Single** structure, the model achieves better efficiency.\n*   **Query-Key Normalization (QK-Norm)** stabilizes the self-attention mechanism by normalizing the dot product between the query (Q) and key (K) vectors. This addresses numerical instability caused by large dot products, ensuring stable attention during training, accelerating convergence, and improving efficiency.\n\n***\n\n### Handling Bilingual Text Prompts\n\n*   The model uses two bilingual pre-trained text encoders to handle both English and Chinese prompts. This allows the model to understand and generate videos based on text input in either language.\n\n***\n\n### High Compression VAE\n\n*   A specially designed deep compression Variational Autoencoder (**VAE**) achieves 16x16 spatial and 8x temporal compression ratios, significantly reducing the computational complexity of large-scale video generation training.\n"
    },
    {
        "question_id": "2502.10248v1_4",
        "answer": "Step-Video-T2V uses two bilingual text encoders to process user text prompts: Hunyuan-CLIP and Step-LLM.\n\n*   **Hunyuan-CLIP**: This is a bidirectional text encoder from an open-source bilingual CLIP model. It produces text representations that align well with the visual space due to the training mechanism of the CLIP model. However, it has an input length limitation of 77 tokens, which can be a challenge for longer prompts.\n*   **Step-LLM**: This is an in-house, unidirectional bilingual text encoder pre-trained using the next-token prediction task. It uses a redesigned Alibi-Positional Embedding to improve efficiency and accuracy in sequence processing. Unlike Hunyuan-CLIP, Step-LLM has no input length restriction, making it effective for handling lengthy and complex text sequences.\n\n***\n\nThe outputs from these two encoders are concatenated along the sequence dimension to create the final text embedding sequence. This combined embedding is then injected into a cross-attention layer within the DiT architecture, allowing the model to generate videos conditioned on the input prompt.\n\n***\n\nAdvantages of using two separate text encoders:\n\n*   **Handling Varying Prompt Lengths**: By combining Hunyuan-CLIP and Step-LLM, the model can handle user prompts of varying lengths. Hunyuan-CLIP is effective for shorter prompts, while Step-LLM excels at processing longer, more complex text sequences.\n*   **Robust Text Representations**: The combination of the two encoders allows Step-Video-T2V to generate robust text representations that effectively guide the model in the latent space. This ensures that the model can accurately interpret and respond to a wide range of text prompts.\n*   **Improved Efficiency and Accuracy**: Step-LLM's redesigned Alibi-Positional Embedding improves both efficiency and accuracy in sequence processing. This, combined with Hunyuan-CLIP's strong alignment with the visual space, enhances the overall performance of the model."
    },
    {
        "question_id": "2502.10248v1_5",
        "answer": "Current diffusion-based text-to-video models face several key challenges, and Step-Video-T2V attempts to address these in various ways. Here's a breakdown:\n\n***\n\n### Main Challenges of Diffusion-Based Text-to-Video Models\n\n1.  **High-Quality Labeled Data:**\n\n    *   **Challenge:** Existing video captioning models often produce inaccurate or hallucinated captions. This makes training unstable. High-quality human annotations are expensive and difficult to obtain at scale.\n    *   **Step-Video-T2V's Approach:** Emphasizes the importance of high-quality, small-scale, and diverse datasets for post-training. The model achieves significant improvements in video quality with a small amount of high-quality human-labeled data in SFT (Supervised Fine-Tuning), suggesting that data quality outweighs sheer quantity.\n\n2.  **Instruction Following:**\n\n    *   **Challenge:** Generating videos based on detailed descriptions, handling complex action sequences, and combining multiple concepts remains difficult. Models struggle with scenarios beyond simple mappings.\n    *   **Step-Video-T2V's Approach:** Acknowledges that even a 30B parameter DiT-based model struggles with complex action sequences and combining multiple concepts with low occurrence in training data. The model analyzes cross-attention scores to understand how it interprets instructions.\n\n3.  **Adherence to the Laws of Physics:**\n\n    *   **Challenge:** Accurately simulating the real world and generating videos that adhere to the laws of physics is a struggle. Models often overfit to specific annotations and fail to generalize.\n    *   **Step-Video-T2V's Approach:** Recognizes this as a key limitation of diffusion-based models. The report suggests that future work may combine autoregressive and diffusion models within a unified framework to better simulate realistic interactions.\n\n4.  **Computational Cost:**\n\n    *   **Challenge:** Training and generating long-duration, high-resolution videos is computationally expensive.\n    *   **Step-Video-T2V's Approach:** Uses a deep compression Variational Auto-encoder (VAE) to achieve 16x16 spatial and 8x temporal compression ratios. This reduces the computational complexity of large-scale video generation training.\n\n5.  **Artifacts and Visual Quality:**\n\n    *   **Challenge:** Generating videos free of artifacts and with high visual quality is an ongoing issue.\n    *   **Step-Video-T2V's Approach:** Employs a video-based Direct Preference Optimization (DPO) approach to reduce artifacts and improve the visual quality of the generated videos.\n\n***\n\n### How Step-Video-T2V Attempts to Address These Challenges\n\n1.  **Data Strategy:**\n\n    *   Focuses on curating high-quality, diverse datasets for post-training.\n    *   Employs hierarchical data filtering to improve training data quality.\n\n2.  **Model Architecture:**\n\n    *   Utilizes a deep compression Video-VAE for efficient video representation.\n    *   Employs a DiT with 3D full attention for better motion dynamics.\n\n3.  **Training Techniques:**\n\n    *   Uses a cascaded training pipeline (text-to-image pre-training, text-to-video pre-training, supervised fine-tuning, and direct preference optimization) to accelerate model convergence.\n    *   Employs video-based DPO to enhance visual quality.\n\n4.  **Bilingual Support:**\n\n    *   Uses two bilingual text encoders to handle both Chinese and English prompts, broadening the applicability and user base.\n\n***\n"
    },
    {
        "question_id": "2502.10248v1_6",
        "answer": "Step-Video-T2V employs Flow Matching as its training objective, which offers a unique approach to guiding the model towards generating realistic videos.\n\n***\n\n### Flow Matching in Step-Video-T2V:\n\n1.  **Noise Sampling**: The process begins by sampling random Gaussian noise, denoted as $X_0$, which serves as the starting point for generating a video frame.\n2.  **Target Sample Selection**: A target sample, $X_1$, representing the desired noise-free input (i.e., the actual video frame), is selected.\n3.  **Linear Interpolation**: A linear interpolation is performed between $X_0$ and $X_1$ using a random time step $t$ within the range of $[0, 1]$. This creates an intermediate state $X_t$ that gradually transitions from noise to the target frame. The equation for this is:\n\n    $X_t = (1 - t) \\cdot X_0 + t \\cdot X_1$\n4.  **Velocity Calculation**: The \"ground truth velocity\" $V_t$ is computed, representing the rate of change required to transform $X_0$ into $X_1$ over time. Mathematically, it's expressed as:\n\n    $V_t = \frac{dX_t}{dt} = X_1 - X_0$\n5.  **Model Training**: The model is trained to predict the velocity $u(X_t, y, t; \theta)$ needed to move $X_t$ closer to $X_1$, given the input $X_t$, any conditioning input $y$ (such as a text prompt), the time step $t$, and the model parameters $\theta$. The training objective is to minimize the Mean Squared Error (MSE) between the predicted velocity and the ground truth velocity:\n\n    $loss = E_{t, X_0, X_1, y} [\\|u(X_t, y, t; \theta) - V_t\\|^2]$\n\n***\n\n### Benefits of Flow Matching for Video Generation:\n\n*   **Stable Training Dynamics**: Flow matching provides a more stable training process compared to other diffusion-based methods. By directly modeling the velocity field, the model learns a smoother transition from noise to the final video frames.\n*   **High-Quality Samples**: The method ensures that the generated video frames are of high quality. The model predicts the instantaneous rate of change, allowing for precise control over the denoising process.\n*   **Efficient Inference**: During inference, Flow Matching allows for the recovery of denoised samples by iteratively refining the noise through an ODE-based method. This process can be streamlined to reduce the number of steps required, making video generation faster and more efficient.\n*   **Flexibility**: Flow Matching can be easily adapted to different types of data and conditioning inputs. In the case of Step-Video-T2V, it is used with bilingual text prompts, allowing the model to generate videos from both English and Chinese descriptions."
    },
    {
        "question_id": "2502.10248v1_7",
        "answer": "Step-Video-T2V employs a hierarchical data filtering approach to refine its training data, which is crucial for achieving high-quality video generation. This process involves progressively applying filters with increasing thresholds to create pre-training subsets. The final step includes manual filtering to ensure the highest quality.\n\nHere's a breakdown of the key aspects:\n\n1.  **Progressive Filtering**: The data is subjected to a series of filters that become stricter at each stage. This allows the model to learn from increasingly cleaner and more relevant data.\n2.  **Key Filters**:\n\n    *   **CLIP Scores**: Filtering based on **CLIP** scores helps to retain videos that align well with the text prompts.\n    *   **Aesthetic Scores**: Videos are filtered based on aesthetic appeal, ensuring that the training data consists of visually pleasing content.\n3.  **Manual Filtering**: The final dataset is refined through manual inspection by human evaluators. They assess clarity, aesthetics, motion, scene transitions, and the absence of watermarks or subtitles. Captions are also refined to ensure accuracy and include details about camera movements, subjects, actions, backgrounds, and lighting.\n4.  **Impact on Training**: The reduction in loss during pre-training correlates with the improved quality of the training data. A notable drop in loss occurs as the quality of the training dataset improves, indicating that the model's learning process aligns with human cognitive patterns.\n\n***\n\nThis hierarchical filtering approach is important for several reasons:\n\n*   **Improved Data Quality**: By systematically filtering the data, Step-Video-T2V ensures that the model is trained on high-quality videos with accurate captions.\n*   **Enhanced Model Performance**: Training on cleaner data leads to better model convergence and improved video generation quality.\n*   **Alignment with Human Perception**: Adjusting the training data to reflect human preferences results in a significant reduction in training loss, suggesting that the model learns to generate videos that are more visually appealing."
    },
    {
        "question_id": "2502.10248v1_8",
        "answer": "Okay, let's break down how **Step-Video-T2V** stacks up against commercial video generation models such as **Sora** and **Veo**.\n\n***\n\n### General Comparison\n\n*   **Comparable Performance:** **Step-Video-T2V** delivers performance that is on par with closed-source engines like **Sora**, **Gen-3**, **Kling**, and **Hailuo** for general text prompts. In certain specific areas, it even outperforms them, particularly in generating videos with high motion dynamics or text content.\n*   **Pipeline Complexity:** Commercial video generation engines often involve more complex pipelines with extensive pre- and post-processing. **Step-Video-T2V** offers a more streamlined approach while maintaining competitive performance.\n\n***\n\n### Key Advantages of Step-Video-T2V\n\n*   **Open Source:** Unlike commercial models, **Step-Video-T2V** is open source, offering greater transparency and accessibility for researchers and content creators.\n*   **Model Size:** It is the largest open-source model to date.\n*   **High Compression VAE:** Utilizes a high-compression **VAE** for videos.\n*   **Bilingual Support:** Supports bilingual text prompts in both English and Chinese.\n*   **Video-Based DPO:** Implements a video-based **DPO** approach to reduce artifacts and enhance visual quality.\n*   **Comprehensive Documentation:** Provides comprehensive training and inference documentation.\n\n***\n\n### Limitations and Future Improvements\n\n*   **Training Data:** **Step-Video-T2V** uses significantly less high-quality data in the post-training phase compared to commercial engines.\n*   **Video Length and Resolution:** While it generates videos of 204 frames, the resolution is **540P**, which is lower than some commercial models (**720P** in **T2VTopA**, **1080P** in **T2VTopB**). Higher resolution can be a key factor in determining perceived quality.\n*   **Pre-training Insufficiency:** The pre-training of **Step-Video-T2V** is less extensive compared to models like **Movie Gen Video**. More extensive pre-training is planned for future work.\n*   **Human Labeling:** Lacks enough high-quality labeled data for supervised fine-tuning (**SFT**), which is crucial for refining visual style and quality.\n*   **Instruction Following:** Commercial models like **T2VTopA** exhibit better instruction-following capabilities, likely due to better video captioning models and more human effort in labeling post-training data.\n\n***\n\n### Specific Comparisons on Benchmarks\n\n*   **Step-Video-T2V-Eval:** Evaluated on a novel benchmark, demonstrating state-of-the-art text-to-video quality compared to both open-source and commercial engines.\n*   **Movie Gen Video Bench:** Achieves comparable performance to **Movie Gen Video**. It also shows significant improvements across all categories when compared to **HunyuanVideo**, solidifying its position as a leading open-source text-to-video model.\n\n***\n\n### Performance Metrics\n\nWhen compared to commercial models like **T2VTopA** and **T2VTopB**:\n\n*   **Aesthetic Appeal:** Commercial models often have higher aesthetic appeal due to higher resolutions and high-quality aesthetic data used during post-training.\n*   **Motion Dynamics:** **Step-Video-T2V** demonstrates strong capabilities in modeling and generating videos with high-motion dynamics, often outperforming commercial models in the **Sports** category.\n*   **Instruction Following:** Commercial models can have better instruction-following capabilities, contributing to superior performance in categories like **Combined Concepts**, **Surreal**, and **Cinematography**.\n\n***\n\nIn summary, **Step-Video-T2V** is a competitive open-source alternative to commercial video generation models, with particular strengths in motion dynamics and bilingual support. While it currently lags behind in aesthetic appeal and instruction following due to limitations in training data and resolution, ongoing improvements aim to bridge these gaps and further enhance its capabilities."
    },
    {
        "question_id": "2502.10248v1_9",
        "answer": "Human feedback plays a crucial role in refining Step-Video-T2V's video generation by improving the visual quality of the generated videos and ensuring better alignment with user prompts. This is implemented through a pipeline that incorporates human preferences into the training process.\n\n***\n\n### Human Feedback Implementation in Step-Video-T2V\n\n1.  **Data Collection**:\n    *   A diverse set of prompts is created by randomly selecting from training data and synthesizing new prompts based on real-world user interaction patterns.\n    *   Step-Video-T2V generates multiple videos for each prompt using different seeds.\n    *   Human annotators rate the preference of these samples, and quality control personnel monitor the annotation process to ensure accuracy and consistency.\n    *   This results in a dataset of preference and non-preference data used for model training.\n\n2.  **Direct Preference Optimization (DPO)**:\n    *   **DPO** is used to incorporate human feedback, adjusting the model to favor the generation of preferred data while avoiding non-preferred data.\n    *   A reference policy is introduced to stabilize training and prevent the current policy from deviating too far.\n    *   The policy objective is formulated as:\n\n        $L_{DPO} = -E_{(y,x_w,x_l) \\sim D} \\left[ \\log \\sigma \\left( \beta \\left( \\log \frac{\\pi_\theta(x_w|y)}{\\pi_{ref}(x_w|y)} - \\log \frac{\\pi_\theta(x_l|y)}{\\pi_{ref}(x_l|y)} \right) \right) \right]$\n\n        where:\n\n        *   $\\pi_\theta$ and $\\pi_{ref}$ refer to the current policy and reference policy, respectively.\n        *   $x_w$ and $x_l$ are the preferred sample and non-preferred sample, respectively.\n        *   $y$ denotes the condition.\n\n3.  **Training Process**:\n    *   At each step, a prompt and its corresponding positive and negative sample pairs are selected.\n    *   Each sample is generated by the model itself to ensure smooth updates and improve overall training stability.\n    *   Positive and negative samples are aligned by fixing the initial noise and timestep to maintain consistency in the training data, contributing to a more stable training process.\n\n***\n\n### Key Benefits and Considerations\n\n*   **Enhanced Visual Quality**: Video-based **DPO** reduces artifacts and ensures smoother, more realistic video outputs.\n*   **Improved Alignment**: Human feedback ensures better alignment with user prompts, resulting in more accurate and relevant video generation.\n*   **Challenges**:\n    *   The trajectory from initial noise to timestep-specific latents is underutilized due to computational limitations.\n    *   A tradeoff exists between sparse and imprecise feedback in video diffusion models.\n    *   Diffusion models rely on regression, which may result in less efficient preference optimization compared to **LLMs** using token-level softmax."
    }
]