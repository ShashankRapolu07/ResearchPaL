[
    {
        "question_id": "2502.10248v1_0",
        "answer": "The paper defines two levels of video foundation models, each with distinct capabilities and complexities:\n\n### Level 1: Translational Video Foundation Model\n\n*   A model at this level functions as a **cross-modal translation system**.\n*   It is capable of generating videos from text, visual, or multimodal contexts.\n*   Current diffusion-based text-to-video models like **Sora**, **Veo**, **Kling**, **Hailuo**, and **Step-Video** belong to this level.\n*   While these models excel at generating high-quality videos from text prompts, they often struggle with complex action sequences or adherence to the laws of physics.\n\n### Level 2: Predictable Video Foundation Model\n\n*   A model at this level acts as a **prediction system**, similar to large language models (**LLMs**).\n*   It can forecast future events based on text, visual, or multimodal contexts.\n*   It can handle more advanced tasks such as reasoning with multimodal data or simulating real-world scenarios.\n*   These models can perform causal or logical tasks.\n\n***\n\nIn essence, Level 1 models primarily focus on translating input data (text, images, etc.) into video form, while Level 2 models aim to understand and predict the temporal dynamics and underlying causal relationships within videos, enabling more sophisticated reasoning and simulation capabilities."
    },
    {
        "question_id": "2502.10248v1_1",
        "answer": "Step-Video-T2V employs a specialized **Video-VAE** architecture to achieve high compression ratios without sacrificing video quality. Here's a breakdown of the key components and techniques that enable this:\n\n### Architecture Overview\n\n*   The **Video-VAE** is designed for deep compression, achieving **16x16 spatial** and **8x temporal compression ratios**.\n*   It uses a novel **dual-path architecture** in the later stage of the encoder and the early stage of the decoder for unified spatial-temporal compression.\n\n***\n\n### Key Components and Techniques\n\n1.  **Dual-Path Latent Fusion:**\n\n    *   **Conv Path**: Uses causal 3D convolutions with pixel unshuffling to maintain high-frequency details.\n    *   **Shortcut Path**: Employs grouped channel averaging to preserve low-frequency structure.\n    *   These paths are combined through residual summation, allowing the network to efficiently use parameters and reduce blurring artifacts.\n2.  **Causal 3D Convolutional Modules:**\n\n    *   The encoder's early stage features three stages, each with two **Causal Res3DBlock** and corresponding downsample layers.\n    *   A **MidBlock** combines convolutional layers with attention mechanisms to refine compressed representations further.\n    *   Temporal causality is implemented using the following formula:\n        $C3D(X)_t = \begin{cases}\n        Conv3D([0, ..., X_t], \\Theta) & t = 0 \\\n        Conv3D([X_{t-k}, ..., X_t], \\Theta) & t > 0\n        \\end{cases}$\n        where $k$ is the temporal kernel size, ensuring frame $t$ only depends on previous frames.\n3.  **Pixel Shuffling and Unshuffling:**\n\n    *   **Pixel Unshuffling**: Used in the encoder's convolutional path to downscale the spatial-temporal dimensions while increasing the number of channels.\n    *   **Pixel Shuffling**: Used in the decoder to unfold the compressed information back into spatial-temporal dimensions.\n4.  **Spatial Group Normalization:**\n\n    *   Replaces group normalization in the ResNet backbone to avoid temporal flickering between different chunks, enhancing temporal consistency.\n\n***\n\n### Training Details\n\n1.  **Multi-Stage Training:**\n\n    *   **Stage 1**: Train a VAE with a **4x8x8 compression ratio** without a dual-path structure, focusing on learning low-level representations jointly on images and videos.\n    *   **Stage 2**: Incorporate two dual-path modules in both the encoder and decoder, gradually unfreezing these modules along with the mid-block and ResNet backbone for refined training.\n2.  **Loss Functions:**\n\n    *   Combination of **L1 reconstruction loss**, **Video-LPIPS**, and **KL-divergence** to guide the model.\n    *   **GAN loss** is introduced after the initial losses converge to further refine the model\u2019s performance.\n\n***\n\n### Benefits\n\n*   **High Compression Ratios**: Achieved through spatial-temporal downscaling and efficient latent space representation.\n*   **Preservation of Video Quality**: Dual-path architecture maintains both high-frequency details and low-frequency structure, reducing blurring artifacts.\n*   **Efficient Training**: Multi-stage training and optimized operators ensure robust and high-quality video data modeling."
    },
    {
        "question_id": "2502.10248v1_2",
        "answer": "Direct Preference Optimization (DPO) is a technique used to align a model's behavior with human preferences, particularly in generation tasks. It helps the model learn to produce outputs that are more desirable to humans by directly optimizing for these preferences.\n\n***\n\n### How DPO Works\n\n1.  **Preference Data:** DPO requires a dataset of preferences, where for a given input (e.g., a text prompt), there are pairs of outputs, one preferred over the other.\n\n2.  **Objective:** The goal is to train the model to generate the preferred output more often than the non-preferred one. DPO achieves this by adjusting the model's policy to increase the likelihood of the preferred samples while decreasing the likelihood of the non-preferred samples.\n\n3.  **Policy and Reference Policy:** DPO involves a current policy (the model being trained) and a reference policy (a fixed model used for stability). The objective is to update the current policy without deviating too far from the reference policy.\n\n4.  **Loss Function:** The DPO loss function is designed to maximize the probability of the preferred output relative to the non-preferred output, while also ensuring that the updated policy remains close to the reference policy. The loss function can be formulated as:\n\n    $L_{DPO} = -E_{(y, x_w, x_l) \\sim D} \\left[ \\log \\sigma \\left( \beta \\left( \\log \frac{\\pi_\theta(x_w|y)}{\\pi_{ref}(x_w|y)} - \\log \frac{\\pi_\theta(x_l|y)}{\\pi_{ref}(x_l|y)} \right) \right) \right]$\n\n    where:\n\n    *   $\\pi_\theta$ is the current policy.\n    *   $\\pi_{ref}$ is the reference policy.\n    *   $x_w$ is the preferred (winning) sample.\n    *   $x_l$ is the non-preferred (losing) sample.\n    *   $y$ is the condition (e.g., text prompt).\n    *   $\beta$ is a temperature parameter.\n    *   $\\sigma$ is the sigmoid function.\n\n***\n\n### Implementation Details in Step-Video-T2V\n\n1.  **Data Collection:** A diverse set of prompts is created, partly from the training data and partly synthesized by human annotators to mimic real-world user interactions. The model generates multiple videos for each prompt using different seeds, and human annotators rate the preference among these samples.\n\n2.  **Training Process:** During training, the model selects a prompt and its corresponding positive and negative sample pairs. The samples are generated by the model itself to ensure smooth updates and training stability. The initial noise and timestep are fixed to align the positive and negative samples, further stabilizing the training process.\n\n3.  **Addressing Gradient Issues:** The original DiffusionDPO method sometimes requires very low learning rates and gradient clipping due to potential gradient explosions. Step-Video-T2V addresses this by reducing the $\beta$ parameter and increasing the learning rate, which leads to faster convergence.\n\n***\n\n### Benefits for Visual Quality in Step-Video-T2V\n\n1.  **Improved Realism and Consistency:** Human feedback enhances the plausibility and consistency of the generated videos.\n\n2.  **Better Alignment with Prompts:** DPO improves the alignment with the given prompts, resulting in more accurate and relevant video generation.\n\n3.  **Enhanced Data Utilization:** By training a reward model with human-annotated feedback data, the system can dynamically evaluate the quality of newly generated samples, improving data efficiency. The reward model is periodically fine-tuned to maintain alignment with the evolving policy.\n\n***\n\n### Limitations Addressed\n\n1.  **Data Staleness:** The training data used in Video-DPO can become outdated as the model evolves. To address this, a reward model is trained to evaluate the quality of new samples dynamically.\n\n2.  **Trajectory Utilization:** The trajectory from initial noise to timestep-specific latents is underutilized due to computational limitations.\n\n3.  **Feedback Precision:** There is a tradeoff between sparse and imprecise feedback, especially in high-resolution videos where only a few pixels may be problematic.\n\n4.  **Optimization Efficiency:** Diffusion models rely on regression, which may be less efficient for preference optimization compared to token-level softmax in LLMs."
    },
    {
        "question_id": "2502.10248v1_3",
        "answer": "The Step-Video-T2V model leverages a **Diffusion Transformer (DiT)** architecture with **3D full attention**, offering several key advantages for text-to-video generation:\n\n*   **Superior Spatial-Temporal Modeling:** **3D full attention** allows the model to capture both spatial and temporal information in a unified attention process. This contrasts with spatial-temporal attention mechanisms that process spatial and temporal information separately. By jointly considering these dimensions, **3D full attention** can better model complex motions and relationships within video data, leading to more coherent and realistic video generation.\n*   **Enhanced Motion Dynamics:** The paper finds that **3D full attention** outperforms spatial-temporal attention, particularly in generating videos with high motion dynamics. This suggests that the unified attention mechanism is better suited for capturing and representing movement within the video.\n*   **Text Prompt Integration:** The model incorporates text prompts through a cross-attention layer in each transformer block. This allows the model to attend to textual information while processing visual features, enabling the generation of videos conditioned on the input prompt. The use of two bilingual text encoders (**Hunyuan-CLIP** and **Step-LLM**) further enhances the model's ability to understand and incorporate text prompts of varying lengths and complexities.\n*   **Computational Efficiency Enhancements:** To mitigate the high computational cost associated with **3D full attention**, the model employs Adaptive Layer Normalization (**AdaLN**) with optimized computation. By removing class labels from **AdaLN** and adopting the **AdaLN-Single** structure, the model reduces computational overhead and improves overall efficiency.\n*   **Positional Encoding:** The model uses **RoPE-3D**, an extension of Rotation-based Positional Encoding (**RoPE**), to handle video data's temporal and spatial dimensions. This allows the model to represent positions in sequences of varying lengths flexibly and continuously, improving its ability to model video data."
    },
    {
        "question_id": "2502.10248v1_4",
        "answer": "Step-Video-T2V uses a dual approach to handle bilingual text prompts, leveraging two distinct text encoders: Hunyuan-CLIP and Step-LLM. Each encoder brings unique strengths to the system, allowing it to effectively process a wide range of text inputs in both English and Chinese.\n\n***\n\n### Hunyuan-CLIP\nThis is a bidirectional text encoder, part of an open-source bilingual CLIP model. Its architecture is designed to create text representations that align well with visual information. This alignment is crucial for text-to-video generation, as it helps the model accurately translate textual descriptions into corresponding visual elements. However, Hunyuan-CLIP has a limitation: it can only process text prompts up to 77 tokens in length.\n\n### Step-LLM\nIn contrast, Step-LLM is a unidirectional bilingual text encoder, pre-trained using a next-token prediction task. It incorporates a redesigned Alibi-Positional Embedding to enhance both efficiency and accuracy in sequence processing. Unlike Hunyuan-CLIP, Step-LLM has no input length restriction. This makes it suitable for handling long and complex text sequences.\n\n***\n\n### Advantages of Using Two Separate Text Encoders\n\n1.  **Handling Varying Prompt Lengths**: By combining Hunyuan-CLIP and Step-LLM, Step-Video-T2V can handle user prompts of different lengths. Hunyuan-CLIP is used for shorter prompts that benefit from its strong visual alignment, while Step-LLM is used for longer prompts that require more extensive sequence processing.\n2.  **Improved Text Representation**: The two encoders contribute different strengths to the overall text representation. Hunyuan-CLIP provides representations well-aligned with the visual space, which helps in generating accurate visual content. Step-LLM, with its ability to handle longer sequences, ensures that all relevant information from complex prompts is captured and used effectively.\n3.  **Robustness**: Using two different encoders adds robustness to the system. If one encoder struggles with a particular type of prompt, the other can compensate, ensuring that the model consistently generates high-quality videos.\n4.  **Effective Guidance in Latent Space**: The combined text embeddings from both encoders are injected into a cross-attention layer within the DiT architecture. This allows the model to attend to textual information while processing visual features, resulting in videos that are more closely aligned with the input prompts."
    },
    {
        "question_id": "2502.10248v1_5",
        "answer": "Here's a breakdown of the challenges faced by current diffusion-based text-to-video models, and how Step-Video-T2V attempts to address them:\n\n***\n\n### Key Challenges\n\n*   **High-Quality Labeled Data:** A significant obstacle is the lack of high-quality labeled data. Existing video captioning models often struggle with hallucination issues, leading to unstable training and poor instruction-following performance. Human annotations are expensive and difficult to scale.\n*   **Instruction Following:** Current models struggle with videos involving complex action sequences or multiple concepts with low occurrence in the training data (e.g., an elephant and a penguin). This is because the distribution of **cross-attention scores** is occasionally highly concentrated, leading to missing objects, wrong details, or incomplete action sequences.\n*   **Laws of Physics:** Accurately simulating the real world and generating videos that adhere to the laws of physics remains a challenge. Many models overfit to specific annotations and cannot generalize well.\n*   **Computational Cost:** Training and generating long-duration, high-resolution videos face significant computational cost hurdles.\n\n***\n\n### How Step-Video-T2V Addresses These Challenges\n\n*   **High-Quality Labeled Data:**\n    *   Employs both automated and manual filtering techniques to curate a high-quality video dataset.\n    *   Uses video assessment scores and heuristic rules to filter the dataset.\n    *   Human evaluators assess each video for clarity, aesthetics, appropriate motion, and accurate captions.\n*   **Instruction Following:**\n    *   The paper addresses this issue by heuristically repeating the missing objects in the prompt.\n    *   The paper aims to refine the model\u2019s ability to better attend and follow all elements in the prompt in the future.\n*   **Laws of Physics:**\n    *   The paper plans to develop more advanced model paradigms in future work, such as combining autoregressive and diffusion models within a unified framework to better adhere to the laws of physics and more accurately simulate realistic interactions.\n*   **Computational Cost:**\n    *   The paper introduces a deep compression **Video-VAE**, achieving 16x16 spatial and 8x temporal compression ratios, significantly reducing the computational complexity of large-scale video generation training.\n    *   The paper details the optimizations of model hyper-parameters, operators, and parallelism in **Step-Video-T2V**, which ensure both the stability and efficiency of training from a system-level perspective."
    },
    {
        "question_id": "2502.10248v1_6",
        "answer": "Step-Video-T2V employs Flow Matching as its training objective, which offers a unique approach to guide the model in generating high-quality videos. Let's break down how this works and why it's advantageous:\n\n### Flow Matching in Step-Video-T2V\n\n1.  **Noise Sampling**: The process begins by sampling random Gaussian noise, denoted as $X_0$, which serves as the starting point for generating a video frame.\n\n2.  **Timestep Selection**: A random timestep $t$ is selected between 0 and 1. This timestep determines how much noise will be present in the intermediate state.\n\n3.  **Linear Interpolation**: The model constructs an input $X_t$ by linearly interpolating between the initial noise $X_0$ and the target, noise-free video frame $X_1$. This is represented as:\n\n    $X_t = (1 - t) \\cdot X_0 + t \\cdot X_1$\n\n    Here, as $t$ approaches 1, $X_t$ becomes closer to the real video frame $X_1$, and as $t$ approaches 0, it resembles the pure noise $X_0$.\n\n4.  **Velocity Prediction**: The model predicts the velocity $u(X_t, y, t; \theta)$, which represents the rate of change needed to transform the noise $X_0$ into the real frame $X_1$. The true velocity $V_t$ is defined as the difference between $X_1$ and $X_0$:\n\n    $V_t = \frac{dX_t}{dt} = X_1 - X_0$\n\n5.  **Loss Minimization**: The model is trained to minimize the **mean squared error (MSE)** between the predicted velocity $u(X_t, y, t; \theta)$ and the true velocity $V_t$. The loss function is:\n\n    $\text{loss} = E_{t, X_0, X_1, y} [\\|u(X_t, y, t; \theta) - V_t\\|^2]$\n\n    By minimizing this loss, the model learns to accurately predict the velocity required to transform noise into coherent video frames.\n\n***\n\n### Benefits of Flow Matching for Video Generation\n\n1.  **Coherent Motion**: Flow Matching ensures that the transitions between frames are smooth and realistic because the model is explicitly trained to predict the \"flow\" or change needed at each step. This results in videos with better temporal coherence.\n\n2.  **Stable Training**: By predicting velocities rather than directly generating frames, Flow Matching provides a more stable training process. The model learns continuous transformations, which are less prone to abrupt changes and artifacts.\n\n3.  **High-Quality Output**: The iterative refinement process, guided by the predicted velocities, allows the model to gradually denoise the input, leading to high-quality video frames. This is particularly important for complex scenes with intricate details.\n\n4.  **Flexibility**: Flow Matching can be combined with various architectures and techniques, such as diffusion models and **DPO**, to further enhance video generation. It offers a versatile framework that can be adapted to different requirements.\n\n***\n\nIn summary, Step-Video-T2V uses Flow Matching to train its model by predicting the velocity required to transform noise into video frames. This approach leads to more coherent, stable, and high-quality video generation by ensuring smooth transitions and continuous refinement."
    },
    {
        "question_id": "2502.10248v1_7",
        "answer": "The Step-Video-T2V model employs a hierarchical data filtering approach to curate high-quality video datasets for both pre-training and post-training stages. This process involves progressively applying filters with increasing thresholds to create subsets of the data, followed by manual filtering for the final supervised fine-tuning (SFT) dataset.\n\n***\n\n### Data Filtering Techniques\n\n**1. Video Assessment Scores**:\n   - Initial filtering step involves using video assessment scores and heuristic rules to reduce the dataset to a higher-quality subset.\n   - This significantly improves the overall quality of the dataset by removing low-quality videos.\n\n**2. Video Categories**:\n   - For videos within the same cluster, the \"Distance to Centroid\" values are used to remove outliers.\n   - This ensures that the video subset contains a sufficient number of videos for each cluster while maintaining diversity.\n\n**3. Human Annotation**:\n   - Involves manual assessment of each video by human evaluators for clarity, aesthetics, appropriate motion, and smooth scene transitions.\n   - Captions are manually refined to ensure accuracy and include essential details such as camera movements, subjects, actions, backgrounds, and lighting.\n\n***\n\n### Importance of Hierarchical Data Filtering\n\n1.  **Improved Data Quality**: Hierarchical filtering ensures that only high-quality data is used for training, which leads to better model convergence and generalization.\n2.  **Reduced Artifacts and Style Variations**: By using high-quality videos with accurate captions and desired styles in SFT, the model's stability and the style of the generated videos are improved.\n3.  **Enhanced Visual Quality**: Filtering based on aesthetics and clarity helps the model produce videos that are visually appealing and coherent.\n4.  **Better Instruction Following**: Accurate captions and refined details ensure that the model can better align with user prompts and generate videos that meet the specified requirements.\n5.  **Efficient Resource Utilization**: By progressively filtering the data, computational resources are focused on training with the most valuable data, improving overall training efficiency."
    },
    {
        "question_id": "2502.10248v1_8",
        "answer": "Step-Video-T2V is compared against commercial video generation engines, specifically **T2VTopA** and **T2VTopB** (leading text-to-video engines in China), focusing on various aspects of video generation quality. While direct comparisons to **Sora** and **Veo** are not explicitly detailed in the provided excerpts, Step-Video-T2V's performance is benchmarked against these commercial models using a novel evaluation benchmark (**Step-Video-T2V-Eval**).\n\n***\n\nHere's a breakdown of the comparison:\n\n*   **Overall Ranking:**\n    **T2VTopA** generally outperforms Step-Video-T2V, which in turn, outperforms **T2VTopB**.\n\n*   **Aesthetic Appeal:**\n    Step-Video-T2V lags behind **T2VTopA** and **T2VTopB** in aesthetic appeal. This is primarily attributed to the higher resolutions of videos generated by the commercial engines (**720P** in **T2VTopA**, **1080P** in **T2VTopB**, compared to **540P** in Step-Video-T2V) and the high-quality aesthetic data used during their post-training stages.\n\n*   **Motion Dynamics:**\n    Step-Video-T2V excels in modeling and generating videos with high-motion dynamics, particularly in the **Sports** category. It outperforms both **T2VTopA** and **T2VTopB** in this aspect, showcasing superior **Motion Smoothness** and **Physical Plausibility**.\n\n*   **Instruction Following:**\n    **T2VTopA** demonstrates better instruction-following capabilities, leading to superior performance in categories like **Combined Concepts**, **Surreal**, and **Cinematography**. This is attributed to a better video captioning model and greater human effort in labeling the post-training data used by **T2VTopA**.\n\n*   **Training Data and Resources:**\n    Step-Video-T2V's pre-training is identified as insufficient compared to commercial models. For instance, **Movie Gen Video** was trained on significantly more videos during its high-resolution pre-training phase (**73.8M** videos) compared to Step-Video-T2V (**27.3M** videos). The lack of high-quality labeled data in the post-training phase also affects Step-Video-T2V's ability to refine the visual style and quality of generated results.\n\n*   **Video Length:**\n    Step-Video-T2V generates videos up to **204 frames**, nearly twice the length of **T2VTopA** and **T2VTopB**, making its training more challenging.\n\n***\n\nIn summary, Step-Video-T2V demonstrates competitive performance, particularly in motion dynamics, but falls short in aesthetic appeal and instruction following compared to some commercial models. The limitations are primarily due to insufficient pre-training, a lack of high-quality labeled data, and lower video resolution."
    },
    {
        "question_id": "2502.10248v1_9",
        "answer": "Human feedback plays a crucial role in refining the video generation quality of Step-Video-T2V by aligning the model's output with human preferences. This is primarily achieved through a **Direct Preference Optimization (DPO)** approach integrated into the training pipeline.\n\n***\n\n### Incorporation of Human Feedback\n\n1.  **Preference Data Collection**:\n    *   A diverse set of prompts is created, partly from the training data and partly synthesized by human annotators to mimic real-world user interactions.\n    *   The model generates multiple videos for each prompt using different random seeds.\n    *   Human annotators then evaluate these generated videos, indicating their preferences. This results in pairs of preferred and non-preferred data for each prompt.\n\n2.  **DPO Training**:\n    *   The DPO method adjusts the model to favor the generation of preferred videos while discouraging the generation of non-preferred ones, given the same prompt.\n    *   A reference model is used to stabilize the training process, preventing the current model from deviating too far from the reference policy.\n    *   The DPO loss function can be expressed as:\n\n        $L_{DPO} = -E_{(y,x_w,x_l) \\sim D} \\left[ log \\sigma \\left( \beta \\left( log \frac{\\pi_\theta(x_w|y)}{\\pi_{ref}(x_w|y)} - log \frac{\\pi_\theta(x_l|y)}{\\pi_{ref}(x_l|y)} \right) \right) \right]$\n\n        where:\n\n        *   $\\pi_\theta$ is the current policy (model).\n        *   $\\pi_{ref}$ is the reference policy (model).\n        *   $x_w$ is the preferred sample.\n        *   $x_l$ is the non-preferred sample.\n        *   $y$ denotes the condition (prompt).\n\n3.  **Training Dynamics**:\n    *   The model generates its own samples during training, ensuring smooth updates and improved stability.\n    *   Positive and negative samples are aligned by fixing the initial noise and timestep to maintain consistency in the training data.\n    *   The training objective is based on DiffusionDPO, modified to extend to the Flow Matching framework.\n\n4.  **Reward Model Integration**:\n    *   A reward model is trained using human-annotated feedback data to evaluate the quality of newly generated samples dynamically.\n    *   The reward model is periodically fine-tuned with newly annotated human feedback to maintain alignment with the evolving policy.\n    *   This integration allows for on-the-fly scoring and ranking of training data, improving data efficiency.\n\n***"
    }
]