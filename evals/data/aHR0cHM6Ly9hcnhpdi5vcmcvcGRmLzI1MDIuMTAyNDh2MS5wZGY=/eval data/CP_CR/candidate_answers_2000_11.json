[
    {
        "question_id": "2502.10248v1_0",
        "answer": "***\n\nThe paper defines two levels of video foundation models, each representing a different stage of capability and complexity:\n\n### Level 1: Translational Video Foundation Model\n\n*   This level focuses on cross-modal translation.\n*   Models at this level function as systems that generate videos from various inputs like text, images, or a combination of both.\n*   Current text-to-video models, including Step-Video-T2V, fall into this category.\n*   These models excel at creating high-quality videos from text prompts, making video creation more accessible.\n\n### Level 2: Predictable Video Foundation Model\n\n*   This level emphasizes prediction and reasoning.\n*   Models at this level act as prediction systems, similar to large language models (LLMs).\n*   They can forecast future events based on different inputs and handle more advanced tasks.\n*   These tasks include reasoning with multimodal data and simulating real-world scenarios.\n\n### Key Differences\n\nThe primary distinction lies in the ability to model causal relationships and perform predictive reasoning. Level 1 models primarily translate inputs into video, while Level 2 models can understand and predict how scenes will evolve over time, adhering to physical laws and logical sequences."
    },
    {
        "question_id": "2502.10248v1_1",
        "answer": "Step-Video-T2V utilizes a Video-VAE with a novel dual-path architecture to achieve high spatial and temporal compression while maintaining video reconstruction quality. Here's a breakdown of the key components and techniques:\n\n***\n\n### 1. High Compression Ratios\n\n*   **Spatial Compression:** 16x16\n*   **Temporal Compression:** 8x\n\nThese compression ratios significantly reduce the computational complexity of large-scale video generation training.\n\n***\n\n### 2. Dual-Path Architecture\n\nThe Video-VAE introduces a dual-path architecture in the later stage of the encoder and the early stage of the decoder. This architecture facilitates unified spatial-temporal compression through the synergistic use of 3D convolutions and optimized pixel shuffling operations.\n\n***\n\n### 3. Encoder Details\n\nFor an input video tensor $X \\in R^{B \times C \times T \times H \times W}$, the encoder $E$ produces a latent representation $Z = E(X) \\in R^{B \times C_z \times \\lceil T/8 \rceil \times \\lceil H/16 \rceil \times \\lceil W/16 \rceil}$. The encoder consists of:\n\n*   **Causal 3D Convolutional Modules:** The early stage of the encoder consists of three stages, each featuring two Causal **Res3DBlock** and corresponding downsample layers. A **MidBlock** combines convolutional layers with attention mechanisms to refine the compressed representations. Temporal causality is implemented through:\n\n    $C3D(X)_t = \begin{cases} Conv3D([0, ..., X_t], \\Theta) & t = 0 \\ Conv3D([X_{t-k}, ..., X_t], \\Theta) & t > 0 \\end{cases}$\n\n    where $k$ is the temporal kernel size.\n*   **Dual-Path Latent Fusion:** This maintains high-frequency details through convolutional processing while preserving low-frequency structure via channel averaging.\n    1.  **Conv Path:** Combines causal 3D convolutions with pixel unshuffling:\n\n        $H_{conv} = U^{(3)}_s(C3D(X))$\n\n        where $U^{(3)}_s: R^{B \times C \times T_{st} \times H_{ss} \times W_{ss}} \rightarrow R^{B \times C \\cdot s^3 \times T_{st} \times H_{ss} \times W_{ss}}$ with spatial stride $s_s = 2$, temporal stride $s_t = 2$, and $C3D$ denoting causal 3D convolution.\n    2.  **Shortcut Path:** Preserves structural semantics through grouped channel averaging:\n\n        $H_{avg} = \frac{1}{s^3} \\sum_{k=0}^{s^3 - 1} U^{(3)}_s (X)[..., kC_z:(k+1)C_z]$\n\n        where $U^{(3)}_s$ implements 3D pixel unshuffle with spatial-temporal blocking, and $C_z$ is the latent dimension of the next stage.\n    3.  **Fusion Output:** Combines both paths through residual summation:\n\n        $Z = H_{conv} \\oplus H_{avg}$\n\n***\n\n### 4. Decoder Details\n\nThe early stage of the decoder consists of two symmetric Dual Path architectures. The 3D pixel unshuffle operation $U$ is replaced by a 3D pixel shuffle operator $P$, and the grouped channel averaging path is replaced by a grouped channel repeating operation. Spatial groupnorm is used to avoid temporal flickering between different chunks.\n\n***\n\n### 5. Training Details\n\nThe VAE training process involves multiple stages:\n\n1.  **Initial Training:** A VAE with a 4x8x8 compression ratio is trained jointly on images and videos without a dual-path structure.\n2.  **Dual-Path Incorporation:** Two dual-path modules are incorporated into both the encoder and decoder, gradually unfreezing the dual-path modules, the mid-block, and the ResNet backbone.\n\nThe training uses a combination of **L1 reconstruction loss**, **Video-LPIPS**, and **KL-divergence constrain**. **GAN loss** is introduced to further refine the model\u2019s performance.\n\n***\n\n### 6. Key Advantages\n\n*   **Unified Spatial-Temporal Compression:** Achieves 8x16x16 downscaling through 3D convolutions and optimized pixel shuffling.\n*   **Dual-Path Latent Fusion:** Maintains high-frequency details and preserves low-frequency structure, overcoming blurring artifacts.\n*   **Multi-Stage Training:** Ensures efficient and high-quality video data modeling.\n\n***\n\n### 7. Performance\n\nDespite having a larger compression ratio, the Video-VAE maintains state-of-the-art reconstruction quality, outperforming other baselines in scenarios including high-motion, text, and texture. For example, it achieves a **SSIM** of 0.9776 and a **PSNR** of 39.37, while maintaining a **rFVD** of 3.61."
    },
    {
        "question_id": "2502.10248v1_2",
        "answer": "**Direct Preference Optimization (DPO)** is a technique used to align a model's output with human preferences. Instead of using reinforcement learning, which can be complex and unstable, DPO directly optimizes the model based on preference data. The core idea is to adjust the model to generate outputs that humans prefer while avoiding outputs they dislike.\n\n***\n\nHere's how DPO is implemented in Step-Video-T2V to improve visual quality:\n\n1.  **Data Collection**:\n\n    *   A diverse set of prompts is created, which includes prompts from training data and prompts synthesized by human annotators based on real-world user interaction patterns.\n    *   For each prompt, the Step-Video-T2V model generates multiple videos using different random seeds.\n    *   Human annotators then rate the preference of these generated videos, indicating which ones are preferred and which are not. Quality control ensures accuracy and consistency in annotations.\n    *   This process results in a dataset of preferred and non-preferred video samples for each prompt.\n\n2.  **Training Objective**:\n\n    *   The DPO loss function is defined as:\n\n        $L_{DPO} = -E_{(y, x_w, x_l) \\sim D} \\left[ \\log \\sigma \\left( \beta \\left( \\log \frac{\\pi_\theta(x_w|y)}{\\pi_{ref}(x_w|y)} - \\log \frac{\\pi_\theta(x_l|y)}{\\pi_{ref}(x_l|y)} \right) \right) \right]$\n\n        where:\n        *   $\\pi_\theta$ is the current policy (i.e., the model being trained).\n        *   $\\pi_{ref}$ is the reference policy (i.e., a fixed reference model).\n        *   $x_w$ is the preferred sample.\n        *   $x_l$ is the non-preferred sample.\n        *   $y$ is the condition (e.g., text prompt).\n        *   $\beta$ is a hyperparameter controlling the strength of the preference optimization.\n        *   $\\sigma$ is the sigmoid function.\n\n    *   The goal is to adjust the current policy to generate more of the preferred data and less of the non-preferred data, relative to the reference policy.\n\n3.  **Training Process**:\n\n    *   During training, a prompt and its corresponding positive and negative sample pairs are selected.\n    *   Each sample is generated by the model to ensure smooth updates and improve training stability.\n    *   The initial noise and timestep for positive and negative samples are fixed to maintain consistency in the training data, contributing to a more stable training process.\n\n4.  **Addressing Gradient Issues**:\n\n    *   The derivative of the DPO loss with respect to the model parameters is:\n\n        $\frac{\\partial L_{DPO}}{\\partial \theta} \\propto -\beta (1 - \\sigma(\beta z)) \\cdot \frac{\\partial z}{\\partial \theta}$\n\n        where $z$ represents the terms inside the logarithm in the DPO loss function.\n    *   To prevent gradient explosion, the hyperparameter $\beta$ is reduced, and the learning rate is increased to achieve faster convergence.\n\n5.  **On-Policy Training with Reward Model**:\n\n    *   To address the issue of the training data becoming outdated (i.e., generated by earlier versions of the model), a reward model is trained using human-annotated feedback data.\n    *   This reward model dynamically evaluates the quality of newly generated samples during training.\n    *   The reward model is periodically fine-tuned with new human feedback to maintain alignment with the evolving policy.\n    *   By integrating the reward model, the training data is scored and ranked on-the-fly (on-policy), thereby improving data efficiency.\n\n***\n\nIn summary, DPO improves the visual quality of generated videos in Step-Video-T2V by directly optimizing the model based on human preferences, using a reward model for dynamic data evaluation, and carefully managing the training process to ensure stability and efficiency."
    },
    {
        "question_id": "2502.10248v1_3",
        "answer": "Here's a breakdown of the advantages of using a **diffusion transformer (DiT)** with **3D full attention** in **Step-Video-T2V**, synthesized from the provided document:\n\n*   **Superior Modeling of Spatio-Temporal Information:**\n\n    *   **3D full attention** is theoretically better at modeling both spatial and temporal information in videos compared to **spatial-temporal attention**.\n    *   This approach facilitates the generation of videos with smooth and consistent motion.\n*   **High Performance Potential:**\n\n    *   **3D full attention** offers higher performance potential, though at a greater computational cost.\n\n*   **Model Architecture Selection:**\n\n    *   **DiT** was chosen over **MMDiT** because it can separate text and video, and extend to pure video prediction models.\n*   **Performance Comparison:**\n\n    *   Experiments showed that a **3D full attention**-based model outperforms a **spatial-temporal attention**-based model, especially in generating videos with high motion dynamics."
    },
    {
        "question_id": "2502.10248v1_4",
        "answer": "Step-Video-T2V uses two separate bilingual text encoders to process user text prompts in both English and Chinese. Let's explore the advantages of this approach:\n\n*   **Hunyuan-CLIP**: This is a bidirectional text encoder from an open-source bilingual CLIP model. It excels at producing text representations that align well with the visual space due to the training mechanism of CLIP models. However, it has a limitation of a maximum input length of 77 tokens, posing challenges when processing longer user prompts.\n*   **Step-LLM**: This is an in-house, unidirectional bilingual text encoder pre-trained using the next-token prediction task. It incorporates a redesigned Alibi-Positional Embedding to improve efficiency and accuracy in sequence processing. Step-LLM has no input length restriction, making it effective for handling lengthy and complex text sequences.\n\nBy combining these two text encoders, Step-Video-T2V can handle user prompts of varying lengths and complexities. This design generates robust text representations that effectively guide the model in the latent space.\n\n***\n\n**Advantages of using two separate text encoders:**\n\n*   **Handling Varying Prompt Lengths**: Using both encoders allows the model to process both short, visually-aligned prompts (via Hunyuan-CLIP) and longer, more complex prompts (via Step-LLM).\n*   **Improved Representation**: Combining the strengths of both encoders results in more robust text representations. Hunyuan-CLIP provides good visual alignment, while Step-LLM handles complex sequences.\n*   **Flexibility**: The model can adapt to different types of text input, ensuring effective guidance in the latent space regardless of the prompt's characteristics."
    },
    {
        "question_id": "2502.10248v1_5",
        "answer": "Current diffusion-based text-to-video models face several key challenges, which Step-Video-T2V attempts to address through its architecture, training strategies, and data processing techniques. Here's a breakdown of these challenges and how Step-Video-T2V tackles them:\n\n***\n\n### 1. Computational Complexity and High-Resolution Video Generation\n\n**Challenge:** Generating high-resolution videos requires significant computational resources due to the large data volumes involved in both spatial and temporal dimensions. The quadratic scaling of computational costs with the number of tokens in attention mechanisms exacerbates this issue.\n\n**Step-Video-T2V's Approach:**\n\n*   **Deep Compression Video-VAE:** Step-Video-T2V employs a specially designed **deep compression Variational Autoencoder (VAE)** that achieves a **16x16 spatial** and **8x temporal compression ratio**. This reduces the computational complexity of large-scale video generation training by operating in a compressed latent space.\n*   **Dual-Path Architecture:** The Video-VAE introduces a novel dual-path architecture with unified spatial-temporal compression, maintaining high-frequency details through convolutional processing while preserving low-frequency structure via channel averaging.\n\n***\n\n### 2. Capturing Motion Dynamics and Long-Term Dependencies\n\n**Challenge:** Accurately modeling motion dynamics and ensuring temporal consistency over long video sequences is difficult. Many models struggle to generate videos with realistic movements and coherent actions.\n\n**Step-Video-T2V's Approach:**\n\n*   **3D Full Attention in DiT:** Step-Video-T2V uses a **Diffusion Transformer (DiT)** with **3D full attention**, enabling the model to capture both spatial and temporal information in a unified attention process. This is more effective than spatial-temporal attention mechanisms for generating videos with smooth and consistent motion.\n*   **RoPE-3D:** The model uses **RoPE-3D**, an extension of Rotation-based Positional Encoding, to handle video data by accounting for temporal and spatial dimensions. This allows the model to process videos with varying frame counts and resolutions effectively.\n*   **Cascaded Training Pipeline:** A cascaded training pipeline is used, including text-to-image pre-training, text-to-video pre-training at low resolution to learn motion dynamics, supervised fine-tuning (SFT), and direct preference optimization (DPO).\n\n***\n\n### 3. Instruction Following and Complex Scene Generation\n\n**Challenge:** Many models struggle to generate videos that accurately follow complex instructions, especially those involving multiple concepts or rare combinations of objects and actions.\n\n**Step-Video-T2V's Approach:**\n\n*   **Bilingual Text Encoders:** Step-Video-T2V uses two bilingual text encoders (**Hunyuan-CLIP** and **Step-LLM**) to process user text prompts in both English and Chinese. This allows the model to handle prompts of varying lengths and complexities.\n*   **Cross-Attention Mechanism:** A cross-attention layer is introduced between the self-attention and feed-forward network (FFN) in each transformer block to incorporate text prompts, enabling the model to attend to textual information while processing visual features.\n*   **Data Filtering and Balancing:** The training dataset is carefully constructed using techniques such as video concept balancing and video-text alignment to ensure accurate alignment between video content and textual descriptions.\n\n***\n\n### 4. Generating Videos Adhering to the Laws of Physics\n\n**Challenge:** Diffusion-based models often fail to accurately simulate the real world and generate videos that adhere to the laws of physics, such as realistic object interactions and movements.\n\n**Step-Video-T2V's Approach:**\n\n*   While Step-Video-T2V, like other diffusion-based models, still faces challenges in this area, the paper suggests that future work will involve combining autoregressive and diffusion models within a unified framework to better adhere to the laws of physics and more accurately simulate realistic interactions.\n\n***\n\n### 5. Data Quality and Captioning Accuracy\n\n**Challenge:** High-quality labeled data is essential for training effective text-to-video models, but existing video captioning models often struggle with hallucination issues, leading to unstable training and poor instruction-following performance.\n\n**Step-Video-T2V's Approach:**\n\n*   **High-Quality Data Curation:** The model uses both automated and manual filtering techniques to curate a high-quality video dataset with accurate captions, appropriate motion, realism, and aesthetics.\n*   **Video Captioning Process:** The video captioning process employs an in-house Vision Language Model (VLM) designed to generate both short and dense captions for video clips, incorporating key elements such as the main subject, events, environmental and visual aspects, video type and style, as well as camera shots and movements.\n\n***\n\n### 6. Visual Quality and Artifact Reduction\n\n**Challenge:** Generating videos with high visual quality and minimal artifacts is crucial for creating compelling content.\n\n**Step-Video-T2V's Approach:**\n\n*   **Video-based DPO:** Step-Video-T2V applies a video-based Direct Preference Optimization (DPO) approach to further enhance the visual quality of the generated videos, effectively reducing artifacts and ensuring smoother, more realistic video outputs.\n*   **Adaptive Layer Normalization:** Adaptive Layer Normalization (AdaLN) with optimized computation is used to improve overall model efficiency and reduce computational overhead.\n\n***\n\nIn summary, Step-Video-T2V addresses the challenges of diffusion-based text-to-video models through a combination of architectural innovations, training strategies, and data processing techniques. These efforts aim to improve computational efficiency, motion dynamics, instruction following, and visual quality, making it a state-of-the-art model in the field."
    },
    {
        "question_id": "2502.10248v1_6",
        "answer": "Step-Video-T2V employs **Flow Matching** as a training objective, which is beneficial for video generation due to its ability to model the data distribution and generate high-quality samples through a continuous transformation from noise to data.\n\n***\n\n### Flow Matching in Step-Video-T2V\n\n1.  **Noise Sampling**: The process starts by sampling Gaussian noise, denoted as $X_0 \\sim N(0,1)$.\n2.  **Timestep Sampling**: A random timestep $t$ is sampled from a uniform distribution between 0 and 1 ($t \\in [0, 1]$).\n3.  **Interpolation**: The model input $X_t$ is constructed as a linear interpolation between the noise $X_0$ and the target sample $X_1$ (noise-free input):\n\n    $X_t = (1 - t) \\cdot X_0 + t \\cdot X_1$\n4.  **Velocity Calculation**: The ground truth velocity $V_t$ is defined as the rate of change of $X_t$ with respect to the timestep $t$:\n\n    $V_t = \frac{dX_t}{dt} = X_1 - X_0$\n\n    This captures the direction and magnitude of change from the initial noise to the target data.\n5.  **Training Loss**: The model is trained to predict the velocity $u(X_t, y, t; \theta)$ at timestep $t$, given the input $X_t$ and optional conditioning input $y$ (e.g., a bilingual sentence). The training loss is the Mean Squared Error (**MSE**) between the predicted velocity and the true velocity:\n\n    $loss = E_{t, X_0, X_1, y} [\\|u(X_t, y, t; \theta) - V_t\\|^2]$\n\n    Here, $\theta$ represents the model parameters, and the expectation is taken over all training samples.\n6.  **Inference**: During inference, random noise $X_0$ is sampled from a Gaussian distribution. The denoised sample $X_1$ is recovered by iteratively refining the noise through an ODE-based method using a Gaussian solver. A sequence of timesteps $\\{t_0, t_1, ..., t_n\\}$ is defined, where $t_0 = 0$, $t_n = 1$, and $t_0 < t_1 < ... < t_n$. The denoising process is expressed as:\n\n    $X_1 = \\sum_{i=0}^{n-1} u(X_{t_i}, y, t_i; \theta) \\cdot (t_{i+1} - t_i)$\n\n    This iterative process allows the model to gradually denoise the input sample, progressing from noise $X_0$ towards the target sample $X_1$ over the defined timesteps.\n\n***\n\n### Benefits of Flow Matching for Video Generation\n\n*   **Stable Training**: Flow Matching provides a more stable training process compared to other generative modeling techniques. By predicting the velocity field that transforms noise into data, the model learns a continuous mapping, which reduces the risk of mode collapse and instability.\n*   **High-Quality Samples**: The continuous transformation from noise to data allows for generating high-quality and coherent video frames. The model learns to denoise the input gradually, resulting in visually appealing and realistic videos.\n*   **Flexibility**: Flow Matching is flexible and can be combined with various network architectures and conditioning techniques. In Step-Video-T2V, it is integrated with a **DiT** architecture and conditioned on text prompts, enabling the generation of videos from textual descriptions.\n*   **Efficiency**: By learning a direct mapping from noise to data, Flow Matching can reduce the number of steps required during inference. This leads to faster video generation without sacrificing quality. The paper also mentions that the model can reduce the number of function evaluations (**NFE**) to as few as 8 steps with negligible performance degradation through self-distillation."
    },
    {
        "question_id": "2502.10248v1_7",
        "answer": "Step-Video-T2V employs a hierarchical data filtering approach that progressively increases the thresholds of filters to create pre-training subsets. This process refines the dataset at each stage, ensuring that only high-quality data is used for training.\n\n***\n\nHere's a breakdown:\n\n1.  **Initial Data**: The process starts with a large-scale raw video dataset.\n2.  **Filtering Stages**:\n    *   **Video Quality Assessment**: Assesses and filters video clips based on various quality metrics.\n    *   **Video Motion Assessment**: Evaluates motion content to ensure dynamic scenes.\n    *   **Video Captioning**: Adds short, dense, and original title captions using a Vision Language Model (VLM).\n    *   **Video Concept Balancing**: Addresses category imbalances and facilitates deduplication using K-means clustering.\n    *   **Video-Text Alignment**: Measures the alignment between video content and textual descriptions using a CLIP Score.\n3.  **Pre-training Subsets**: The data is filtered to create multiple subsets for pre-training, with progressively increasing filter thresholds.\n4.  **Final SFT Dataset**: The process culminates in a manually filtered dataset for supervised fine-tuning (SFT).\n\n***\n\nThe importance of this hierarchical data filtering approach lies in several key aspects:\n\n*   **Improved Data Quality**: By systematically filtering the data based on quality, motion, and aesthetic criteria, the model is trained on cleaner and more visually appealing videos.\n*   **Enhanced Model Stability**: High-quality data leads to more stable training, reducing artifacts and improving the overall quality of generated videos.\n*   **Better Generalization**: Training on a balanced dataset with diverse concepts helps the model generalize better across different scenarios and prompts.\n*   **Efficient Resource Utilization**: Filtering out low-quality data reduces the computational resources needed for training, allowing the model to focus on learning from the most informative examples.\n*   **Human-Like Learning**: The process emulates human cognitive patterns by prioritizing data considered higher quality by humans, resulting in a significant reduction in training loss."
    },
    {
        "question_id": "2502.10248v1_8",
        "answer": "Okay, let's analyze how Step-Video-T2V stacks up against commercial models like Sora and Veo, based on the provided technical report.\n\nIt's important to note that a direct, apples-to-apples comparison is challenging because Sora and Veo are closed-source, and detailed architectural and training information isn't publicly available. However, we can infer some comparisons based on the information provided in the Step-Video-T2V report.\n\nHere's a breakdown:\n\n**1. General Performance and Quality**\n\n*   **Step-Video-T2V's Claim:** The report positions Step-Video-T2V as delivering \"comparable performance for general text prompts\" to commercial engines. It even claims to surpass them in specific domains.\n*   **Inference:** This suggests that Step-Video-T2V achieves a competitive level of video quality, motion dynamics, and content consistency compared to the likes of Sora and Veo, at least for typical text-to-video generation tasks.\n\n***\n\n**2. Strengths of Step-Video-T2V**\n\n*   **Motion Dynamics:** The report emphasizes Step-Video-T2V's \"strong capability in modeling and generating videos with high-motion dynamics.\" The evaluation results on the **Step-Video-T2V-Eval** benchmark and the **Movie Gen Video Bench** support this claim.\n*   **Bilingual Support:** Step-Video-T2V explicitly supports both Chinese and English prompts, which may not be a standard feature in all commercial models.\n*   **Open Source:** A significant advantage is that Step-Video-T2V is open-source, providing researchers and developers with access to the model architecture, training details, and code. This fosters transparency and allows for further innovation and customization, which isn't possible with closed-source commercial models.\n\n***\n\n**3. Limitations and Challenges**\n\n*   **Instruction Following:** The report acknowledges that Step-Video-T2V \"struggles to generate videos involving complex action sequences\" or \"multiple concepts with low occurrence in the training data.\" This indicates a potential weakness in precisely interpreting and executing complex instructions compared to more advanced commercial models.\n*   **Adherence to Physics:** Like other diffusion-based models, Step-Video-T2V faces challenges in \"generating videos that adhere to the laws of physics.\" This is a known limitation of current-generation video generation models.\n*   **High-Quality Data:** The report mentions that Step-Video-T2V uses \"significantly less high-quality data in the post-training phase\" compared to commercial engines. This suggests that Sora and Veo may have an advantage in terms of visual fidelity and aesthetic appeal due to superior training data.\n*   **Resolution:** Step-Video-T2V generates videos at a resolution of 544x992. The report notes that commercial engines like T2VTopA and T2VTopB produce higher-resolution videos (720P and 1080P, respectively), which contributes to their higher aesthetic appeal.\n\n***\n\n**4. Key Architectural and Training Differences**\n\n*   **Diffusion Transformer (DiT):** Step-Video-T2V uses a DiT architecture trained using Flow Matching. This is a common approach in state-of-the-art video generation models.\n*   **Video-VAE:** The model employs a deep compression Variational Autoencoder (VAE) to reduce the computational complexity of video generation. The VAE achieves 16x16 spatial and 8x temporal compression ratios.\n*   **Cascaded Training:** Step-Video-T2V uses a cascaded training pipeline, including text-to-image pre-training, text-to-video pre-training, supervised fine-tuning (SFT), and direct preference optimization (DPO).\n*   **Video-DPO:** A video-based DPO approach is used to reduce artifacts and improve visual quality.\n\nIt's difficult to know the precise architectural details of Sora and Veo, but they likely employ similar techniques, potentially with proprietary innovations in areas like data curation, model scaling, and training optimization.\n\n***\n\n**5. Summary Table**\n\n| Feature                | Step-Video-T2V                               | Commercial Models (Sora, Veo)                     |\n| :--------------------- | :------------------------------------------- | :------------------------------------------------ |\n| General Quality        | Comparable                                   | Potentially higher (due to data and scaling)        |\n| Motion Dynamics        | Strong                                       | Likely strong                                     |\n| Instruction Following  | Limitations acknowledged                      | Potentially better                                |\n| Physics Adherence      | Limited                                      | Limited                                           |\n| Bilingual Support      | Yes                                          | Potentially                                       |\n| Resolution             | 544x992                                      | Higher (720P, 1080P, or higher)                   |\n| Open Source            | Yes                                          | No                                                |\n| Data Efficiency        | Less high-quality data in post-training phase | Likely use more curated, high-quality data         |\n| Training Methodology | Flow Matching, DPO                           | Likely similar, with proprietary optimizations    |\n\n***\n\n**In Conclusion:**\n\nStep-Video-T2V appears to be a competitive open-source alternative to commercial video generation models. It demonstrates strengths in motion dynamics and offers the advantage of being open and customizable. However, it faces challenges in instruction following, physics adherence, and potentially overall visual quality compared to the best closed-source models, likely due to differences in training data and model scale. The specific capabilities of Sora and Veo remain somewhat opaque, but they likely benefit from significant investments in data curation, model optimization, and compute resources."
    },
    {
        "question_id": "2502.10248v1_9",
        "answer": "Human feedback is integrated into the Step-Video-T2V training pipeline to enhance the visual quality of the generated videos and ensure better alignment with user prompts. This is achieved through a process that involves collecting preference data from human annotators and using it to fine-tune the model.\n\n***\n\nHere\u2019s a breakdown of the implementation:\n\n1.  **Data Collection**:\n    *   A diverse set of prompts is created, including random prompts from the training data and those synthesized by human annotators based on real-world user interaction patterns.\n    *   The Step-Video-T2V model generates multiple videos for each prompt using different seeds.\n    *   Human annotators rate the preference of these samples, and quality control personnel monitor the annotation process to ensure accuracy and consistency.\n    *   This process results in a dataset of preference and non-preference data, which serves as the foundation for model training.\n\n2.  **Direct Preference Optimization (DPO)**:\n    *   **DPO** is selected as the method for incorporating human feedback due to its effectiveness and simplicity.\n    *   The goal is to adjust the model to generate preferred data while avoiding non-preferred data, given human preference data under the same conditions.\n    *   A reference policy (reference model) is introduced to stabilize training and prevent the current policy from deviating too far.\n    *   The policy objective is formulated as:\n\n        $L_{DPO} = -E_{(y, x_w, x_l) \\sim D} \\left[ log \\ \\sigma \\left( \beta \\left( log \\ \frac{\\pi_\theta(x_w|y)}{\\pi_{ref}(x_w|y)} - log \\ \frac{\\pi_\theta(x_l|y)}{\\pi_{ref}(x_l|y)} \right) \right) \right]$\n\n        where $\\pi_\theta$ and $\\pi_{ref}$ refer to the current policy and reference policy, respectively, $x_w$ and $x_l$ are the preferred sample and non-preferred sample, and $y$ denotes the condition.\n\n3.  **Training Process**:\n    *   At each training step, a prompt and its corresponding positive and negative sample pairs are selected.\n    *   Each sample is generated by the model itself, ensuring smooth updates and improving overall training stability.\n    *   The positive and negative samples are aligned by fixing the initial noise and timestep to maintain consistency in the training data.\n    *   The training objective is based on the **DiffusionDPO** method and **DPO**, with slight modifications to extend it to the **Flow Matching** framework.\n\n4.  **Addressing Gradient Issues**:\n    *   The original **DiffusionDPO** method may cause gradient explosion when $z < 0$ (where $z$ represents policy-related terms in the **DPO** equation).\n    *   To address this, the paper reduces $\beta$ and increases the learning rate, resulting in faster convergence.\n\n5.  **Reward Model (Future Enhancement)**:\n    *   The paper notes that improvements from human feedback saturate when the model can easily distinguish between positive and negative samples.\n    *   To address this, the paper proposes training a reward model using human-annotated feedback data.\n    *   This reward model dynamically evaluates the quality of newly generated samples during training and is periodically fine-tuned to maintain alignment with the evolving policy.\n    *   By integrating it into the pipeline, the model scores and ranks training data on-the-fly (on-policy), thereby improving data efficiency.\n\n***"
    }
]