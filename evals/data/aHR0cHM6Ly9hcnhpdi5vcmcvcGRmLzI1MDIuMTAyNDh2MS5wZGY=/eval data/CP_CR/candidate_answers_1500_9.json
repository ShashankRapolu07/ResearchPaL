[
    {
        "question_id": "2502.10248v1_0",
        "answer": "The paper defines two levels of video foundation models, each representing a different stage of capability and complexity:\n\n***\n\n### Level-1: Translational Video Foundation Model\n\n*   This level focuses on cross-modal translation.\n*   The model functions as a system that generates videos from various inputs such as text, visual, or multimodal contexts.\n*   Current diffusion-based text-to-video models like Step-Video-T2V, Sora, Veo, Kling, and Hailuo belong to this category.\n*   These models excel at generating high-quality videos from text prompts, making video content creation more accessible.\n*   They primarily learn mappings between text prompts and corresponding videos.\n\n***\n\n### Level-2: Predictable Video Foundation Model\n\n*   This level aims for predictive capabilities, similar to large language models (LLMs).\n*   The model acts as a prediction system that can forecast future events based on text, visual, or multimodal context.\n*   It handles advanced tasks such as reasoning with multimodal data or simulating real-world scenarios.\n*   It requires explicit modeling of underlying causal relationships within videos.\n\n***\n\n### Key Differences\n\n| Feature                | Level-1: Translational                                             | Level-2: Predictable                                                                                                                                                              |\n| :--------------------- | :----------------------------------------------------------------- | :-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| **Primary Function**   | Cross-modal translation (e.g., text-to-video)                      | Prediction and reasoning about future events                                                                                                                                      |\n| **Causality Modeling** | Limited; primarily learns mappings between inputs and outputs      | Explicitly models causal relationships within videos                                                                                                                               |\n| **Task Complexity**    | Generates videos from prompts                                      | Handles complex action sequences, adherence to the laws of physics, causal or logical tasks                                                                                       |\n| **Examples**           | Current diffusion-based text-to-video models (e.g., Step-Video-T2V) | Future models that can perform reasoning and simulation                                                                                                                            |\n| **Limitations**        | Often fails to generate videos requiring complex action sequences or adherence to physical laws                                                                                                                                                       |\n| **Capabilities**       | High-quality video generation from text prompts                     | Forecasting future events, reasoning with multimodal data, simulating real-world scenarios                                                                                     |\n| **Analogy**            | Translation system                                                 | Prediction system, similar to LLMs                                                                                                                                                |\n| **Learning Focus**     | Mapping between text prompts and corresponding videos               | Underlying causal relationships within videos                                                                                                                                      |\n| **Task Examples**      | Basic video generation                                             | Simulating a basketball bouncing on the floor, performing causal or logical tasks                                                                                                |\n| **Model Examples**     | Sora, Veo, Kling, Hailuo, and Step-Video-T2V                       | Future models that combine autoregressive and diffusion models within a unified framework                                                                                       |\n| **Key Challenge**      | Lack of explicit modeling of causal relationships                  | Balancing performance comparable to diffusion-based models with the causal modeling mechanism                                                                                     |\n| **Complexity**         | Simpler mappings                                                   | Requires explicit modeling of underlying causal relationships within videos                                                                                                       |\n| **Focus**              | Generating videos from given prompts                               | Predicting future events and understanding causal relationships                                                                                                                  |\n| **Real-world**         | Text-to-video generation                                           | Simulating real-world scenarios and understanding physical laws                                                                                                                   |\n| **Reasoning**          | Limited reasoning capabilities                                     | Advanced reasoning and simulation capabilities                                                                                                                                      |\n| **Future**             | Improving video quality and accessibility                            | Developing models that can reason, predict, and simulate complex scenarios                                                                                                         |\n| **Causal**             | Weak causal modeling                                               | Strong causal modeling                                                                                                                                                            |\n| **Prediction**         | Limited prediction                                                 | Strong prediction of future events                                                                                                                                                  |\n| **Abstraction**        | Low abstraction                                                    | High abstraction                                                                                                                                                                  |\n| **Understanding**      | Basic understanding of prompts                                     | Deep understanding of prompts and real-world scenarios                                                                                                                             |\n| **Generalization**     | Limited generalization                                              | Strong generalization                                                                                                                                                              |\n| **Application**        | Content creation                                                   | Advanced applications such as robotics, autonomous driving, and scientific simulation                                                                                                |\n| **Data**               | Primarily text and video data                                      | Multimodal data including text, video, audio, and sensor data                                                                                                                    |\n| **Training**           | Supervised learning                                                | Reinforcement learning and unsupervised learning                                                                                                                                   |\n| **Architecture**       | Diffusion-based models                                             | Hybrid models combining autoregressive and diffusion models                                                                                                                      |\n| **Evaluation**         | Visual quality and instruction following                             | Reasoning, prediction accuracy, and simulation fidelity                                                                                                                             |\n| **Challenge**          | Generating videos with complex action sequences                    | Simulating real-world physics and understanding causal relationships                                                                                                             |\n| **Goal**               | Lowering the barrier for creators to produce video content          | Developing models that can reason, predict, and simulate complex scenarios                                                                                                         |\n| **Limitation**         | Difficulty in generating videos requiring complex action sequences | Difficulty in achieving performance comparable to diffusion-based models on text-to-video generation                                                                             |\n| **Modeling**           | Only the mappings between text prompts and corresponding videos     | Explicitly modeling the underlying causal relationships within videos                                                                                                            |\n| **Mechanism**          | Diffusion Models                                                   | Autoregression                                                                                                                                                                    |\n| **Reasoning**          | No                                                                 | Yes                                                                                                                                                                               |\n| **Prediction**         | No                                                                 | Yes                                                                                                                                                                               |\n| **Causality**          | No                                                                 | Yes                                                                                                                                                                               |\n| **Level**              | Mapping                                                            | Prediction                                                                                                                                                                      |\n| **Type**               | Translation                                                        | Predictive                                                                                                                                                                      |\n| **Capability**         | Cross-Modal Translation                                            | Prediction System                                                                                                                                                                 |\n\nIn summary, Level-1 models focus on translating inputs (especially text) into videos, while Level-2 models aim to predict and reason about video content in a more advanced, LLM-like manner."
    },
    {
        "question_id": "2502.10248v1_1",
        "answer": "Step-Video-T2V's Video-VAE achieves high spatial and temporal compression while maintaining video reconstruction quality through a combination of architectural innovations and a carefully designed training process. Here's a breakdown:\n\n### Architecture\n\n1.  **Dual-Path Architecture**: The Video-VAE introduces a novel dual-path architecture in the later stage of the encoder and the early stage of the decoder. This design facilitates unified spatial-temporal compression.\n2.  **Spatial-Temporal Downscaling**: The architecture achieves 8x temporal and 16x16 spatial downscaling via 3D convolutions and optimized pixel unshuffling operations.\n3.  **Causal 3D Convolutional Modules**:\n    *   The early stage of the encoder uses three stages, each featuring two Causal **Res3DBlock** and corresponding downsample layers.\n    *   A **MidBlock** combines convolutional layers with attention mechanisms.\n    *   Temporal causality is implemented using Causal 3D convolution:\n\n        $C3D(X)\\_t =\n        \begin{cases}\n        Conv3D([0, ..., X\\_t], \\Theta) & t = 0 \\\n        Conv3D([X\\_{t-k}, ..., X\\_t], \\Theta) & t > 0\n        \\end{cases}$\n\n        where $k$ is the temporal kernel size.\n4.  **Dual-Path Latent Fusion**:\n    *   **Conv Path**: Combines causal 3D convolutions with pixel unshuffling:\n\n        $H\\_{conv} = U^{(3)}\\_s(C3D(X))$\n\n        where $U^{(3)}\\_s : R^{B \times C \times T\\_{st} \times H\\_{ss} \times W\\_{ss}} \rightarrow R^{B \times C \\cdot s^3 \times \frac{T}{st} \times \frac{H}{ss} \times \frac{W}{ss}}$ with spatial stride $s\\_s = 2$, temporal stride $s\\_t = 2$.\n    *   **Shortcut Path**: Preserves structural semantics through grouped channel averaging:\n\n        $H\\_{avg} = \frac{1}{s^3} \\sum\\_{k=0}^{s^3-1} U^{(3)}\\_s (X)[\\dots, kC\\_z:(k+1)C\\_z]$\n\n        where $C\\_z$ is the latent dim of the next stage.\n    *   The output of fusion combines both paths through residual summation:\n\n        $Z = H\\_{conv} \\oplus H\\_{avg}$\n5.  **Decoder Architecture**: The early stage of the decoder consists of two symmetric Dual Path architectures. The 3D pixel unshuffle operation $U$ is replaced by a 3D pixel shuffle operator $P$, and the grouped channel averaging path is replaced by a grouped channel repeating operation. Spatial groupnorm is used to prevent temporal flickering.\n\n***\n\n### Training Details\n\n1.  **Multi-Stage Training**: The VAE training process is designed in multiple stages.\n    *   **Stage 1**: Train a VAE with a 4x8x8 compression ratio without employing a dual-path structure, jointly on images and videos.\n    *   **Stage 2**: Enhance the model by incorporating two dual-path modules in both the encoder and decoder, gradually unfreezing the dual-path modules, the **mid-block**, and the **ResNet** backbone.\n2.  **Loss Functions**: A combination of **L1 reconstruction loss**, **Video-LPIPS**, and **KL-divergence** constrain is used. **GAN loss** is introduced to further refine the model\u2019s performance.\n\n***\n\n### Key Factors for Maintaining Reconstruction Quality\n\n1.  **Dual-Path Latent Fusion**: This maintains high-frequency details through convolutional processing while preserving low-frequency structure via channel averaging.\n2.  **Staged Training Approach**: This ensures a robust and high-quality VAE capable of handling complex video data efficiently.\n3.  **Specific Loss Functions**: The combination of different loss functions ensures that the model learns to reconstruct videos accurately while maintaining perceptual quality and adhering to the latent space distribution."
    },
    {
        "question_id": "2502.10248v1_2",
        "answer": "Direct Preference Optimization (DPO) is a technique used to align a model's output with human preferences. Instead of using reinforcement learning, which can be complex and unstable, DPO reframes the alignment process as a classification problem. It directly optimizes the model by comparing preferred outputs to dispreferred ones. This approach simplifies the training process, making it more stable and easier to implement.\n\n***\n\n### How DPO Works\n\n1.  **Preference Data:** DPO relies on a dataset of preferences, where for a given input (e.g., a text prompt), there are two outputs: one that is preferred and one that is dispreferred. These preferences are typically gathered from human annotators.\n\n2.  **Objective Function:** The goal of DPO is to train the model to generate outputs that are more like the preferred ones and less like the dispreferred ones. This is achieved by maximizing the probability of the preferred output relative to the dispreferred output. The DPO loss function can be expressed as:\n\n    $L_{DPO} = -E_{(y, x_w, x_l) \\sim D} \\left[ \\log \\sigma \\left( \beta \\left( \\log \frac{\\pi_\theta(x_w|y)}{\\pi_{ref}(x_w|y)} - \\log \frac{\\pi_\theta(x_l|y)}{\\pi_{ref}(x_l|y)} \right) \right) \right]$\n\n    Where:\n\n    *   $\\pi_\theta$ is the current policy (i.e., the model being trained).\n    *   $\\pi_{ref}$ is a reference policy (i.e., a fixed model used for stability).\n    *   $x_w$ is the preferred sample.\n    *   $x_l$ is the dispreferred sample.\n    *   $y$ is the condition (e.g., text prompt).\n    *   $\beta$ is a hyperparameter controlling the strength of the preference.\n    *   $\\sigma$ is the sigmoid function.\n\n3.  **Training Process:** During training, the model adjusts its parameters to minimize the DPO loss, effectively learning to generate outputs that align with human preferences. A reference policy is used to prevent the current policy from deviating too far from the initial model.\n\n***\n\n### Implementation Details in Step-Video-T2V\n\nIn Step-Video-T2V, DPO is used to improve the visual quality of the generated videos. The implementation involves the following steps:\n\n1.  **Prompt Set Construction**:\n\n    *   A subset of prompts is randomly selected from the training data to ensure diversity.\n    *   Human annotators synthesize prompts based on real-world user interaction patterns.\n\n2.  **Video Generation and Annotation:**\n\n    *   For each prompt, Step-Video-T2V generates multiple videos using different seeds.\n    *   Human annotators rate the preference of these samples.\n    *   Quality control personnel monitor the annotation process to ensure accuracy and consistency.\n\n3.  **Training with DPO:**\n\n    *   At each training step, a prompt and its corresponding positive and negative sample pairs are selected.\n    *   Each sample is generated by the model itself to ensure smooth updates and improve overall training stability.\n    *   The positive and negative samples are aligned by fixing the initial noise and timestep, contributing to a more stable training process.\n\n***\n\n### Benefits of DPO in Step-Video-T2V\n\n1.  **Improved Visual Quality**: DPO helps in generating videos that are more visually appealing and realistic.\n\n2.  **Better Alignment with User Prompts**: DPO ensures that the generated videos are more relevant and accurate with respect to the given prompts.\n\n3.  **Enhanced Consistency**: Human feedback enhances the consistency of generated videos, making them more coherent and plausible.\n\n***\n\n### Addressing Limitations\n\nThe paper also notes some limitations and ways to address them:\n\n1.  **Reward Model Training**: A reward model is trained using human-annotated feedback data to dynamically evaluate the quality of newly generated samples during training. This model is periodically fine-tuned to maintain alignment with the evolving policy, improving data efficiency.\n\n2.  **Balancing Attention**: Ensuring that all elements in the prompt receive appropriate attention is crucial. The paper suggests refining the model\u2019s ability to better attend and follow all elements in the prompt.\n\nBy incorporating DPO, Step-Video-T2V can generate videos that are not only visually appealing but also closely aligned with user preferences, marking a significant step forward in text-to-video generation technology."
    },
    {
        "question_id": "2502.10248v1_3",
        "answer": "Here are the key advantages of using a **DiT** with **3D full attention** in **Step-Video-T2V**, based on the provided document:\n\n*   **Superior Modeling of Spatial and Temporal Information**: **3D full attention** combines spatial and temporal information in a unified attention process. This approach offers a higher potential for performance compared to spatial-temporal attention mechanisms.\n*   **Generation of Smooth and Consistent Motion**: Large-scale experiments have shown that **3D full attention** is superior in generating videos with smooth and consistent motion.\n*   **Effective Use of Cross-Attention for Text Prompts**: The model incorporates a cross-attention layer between the self-attention and feed-forward network (**FFN**) in each transformer block. This layer allows the model to attend to textual information while processing visual features, enabling the generation of videos conditioned on the input prompt.\n*   **Optimized Adaptive Layer Normalization (AdaLN)**: The use of **AdaLN-Single** reduces the computational overhead of traditional **AdaLN** operations, improving overall model efficiency. Class labels are removed from **AdaLN** as they are not required for the text-to-video task.\n*   **RoPE-3D for Handling Video Data**: **RoPE-3D**, an extension of the traditional Rotation-based Positional Encoding (**RoPE**), is specifically designed to handle video data by accounting for temporal (frame) and spatial (height and width) dimensions."
    },
    {
        "question_id": "2502.10248v1_4",
        "answer": "Step-Video-T2V uses two bilingual text encoders to process user text prompts: Hunyuan-CLIP and Step-LLM.\n\n*   **Hunyuan-CLIP**: This is a bidirectional text encoder from an open-source bilingual CLIP model. It produces text representations well-aligned with the visual space due to the training mechanism of the CLIP model. However, it is limited to a maximum input length of 77 tokens, which poses challenges for longer user prompts.\n*   **Step-LLM**: This is an in-house, unidirectional bilingual text encoder pre-trained using the next-token prediction task. It incorporates a redesigned Alibi-Positional Embedding to improve both efficiency and accuracy in sequence processing. Unlike Hunyuan-CLIP, Step-LLM has no input length restriction, making it effective for handling lengthy and complex text sequences.\n\nBy combining these two text encoders, Step-Video-T2V can handle user prompts of varying lengths, generating robust text representations that effectively guide the model in the latent space.\n\n***\n\nThe advantages of using two separate text encoders are:\n\n*   **Handling Varying Prompt Lengths:** By using Hunyuan-CLIP for shorter prompts and Step-LLM for longer prompts, the model can effectively process a wide range of input text lengths without being limited by the token constraints of a single encoder.\n*   **Robust Text Representations:** The combination of a CLIP-based encoder (Hunyuan-CLIP) and a language model-based encoder (Step-LLM) allows the model to capture both the visual alignment and the semantic meaning of the text prompts, resulting in more robust and accurate text representations.\n*   **Improved Efficiency and Accuracy:** Step-LLM incorporates a redesigned Alibi-Positional Embedding to improve both efficiency and accuracy in sequence processing, making it particularly effective for handling lengthy and complex text sequences.\n*   **Better Guidance in Latent Space:** By generating robust text representations, the model can effectively guide the model in the latent space, leading to higher-quality video generation results."
    },
    {
        "question_id": "2502.10248v1_5",
        "answer": "Current diffusion-based text-to-video models face several significant challenges:\n\n*   **High-quality labeled data**: Existing video captioning models often struggle with hallucination issues, and obtaining human annotations is expensive and difficult to scale.\n*   **Instruction Following**: Accurately interpreting and executing instructions, especially those involving detailed descriptions, complex action sequences, or combinations of multiple concepts, remains challenging.\n*   **Laws of Physics**: Generating videos that adhere to the laws of physics is difficult due to the inherent limitations of diffusion models.\n*   **RL-based Optimization**: Effectively using reinforcement learning for post-training improvements in video generation models is an open area for exploration.\n\n***\n\nStep-Video-T2V attempts to address these challenges through several key design choices and training strategies:\n\n*   **High-Compression Video-VAE**: A deep compression **Variational Autoencoder (VAE)** is used to reduce the computational complexity of large-scale video generation training. This **Video-VAE** achieves 16x16 spatial and 8x temporal compression ratios, enabling more efficient processing of video data.\n*   **Bilingual Text Encoders**: Two bilingual text encoders are employed to handle both English and Chinese prompts. **Hunyuan-CLIP** is used for its alignment with the visual space, while **Step-LLM** is used for handling longer and more complex text sequences.\n*   **DiT with 3D Full Attention**: The model is built on a **Diffusion Transformer (DiT)** architecture with 3D full attention. This allows the model to capture both spatial and temporal information effectively.\n*   **Cascaded Training Pipeline**: A cascaded training pipeline is used, including text-to-image pre-training, text-to-video pre-training, supervised fine-tuning **(SFT)**, and direct preference optimization **(DPO)**. This pipeline is designed to accelerate model convergence and leverage video datasets of varying quality.\n*   **Video-based DPO**: A video-based **DPO** approach is applied to reduce artifacts and improve the visual quality of the generated videos. This helps ensure better alignment with user preferences.\n*   **Hierarchical Data Filtering**: A series of filters are applied to the data, progressively increasing their thresholds to create pre-training subsets. The final **SFT** dataset is then constructed through manual filtering, ensuring high-quality training data."
    },
    {
        "question_id": "2502.10248v1_6",
        "answer": "Here's an explanation of how Step-Video-T2V utilizes Flow Matching during training, along with its benefits for video generation:\n\n### Flow Matching in Step-Video-T2V\n\nThe Flow Matching technique is used to train the Step-Video-T2V model by predicting the velocity field that transforms a Gaussian noise distribution into a data distribution. Here's a breakdown:\n\n1.  **Noise Sampling**: During each training step, the process begins by sampling random Gaussian noise, denoted as $X_0$, from a standard normal distribution $N(0, 1)$.\n2.  **Timestep Sampling**: A random timestep $t$ is sampled from a uniform distribution between 0 and 1 (i.e., $t \u2208 [0, 1]$).\n3.  **Interpolation**: The model input $X_t$ is constructed through linear interpolation between the initial noise $X_0$ and the target sample $X_1$ (the noise-free input video frame). This is represented as:\n\n    $X_t = (1 - t) \\cdot X_0 + t \\cdot X_1$\n4.  **Velocity Calculation**: The ground truth velocity $V_t$ is computed, representing the rate of change of $X_t$ with respect to the timestep $t$. It captures the direction and magnitude of change from $X_0$ to $X_1$:\n\n    $V_t = \frac{dX_t}{dt} = X_1 - X_0$\n5.  **Model Training**: The model is trained to predict the velocity field $u(X_t, y, t; \u03b8)$ at timestep $t$, given the input $X_t$, conditioning input $y$ (e.g., text prompt), and model parameters $\u03b8$. The training objective is to minimize the **Mean Squared Error (MSE)** loss between the predicted velocity and the true velocity:\n\n    $\text{loss} = E_{t, X_0, X_1, y} [\\|u(X_t, y, t; \u03b8) - V_t\\|^2]$\n\n    The expectation is taken over all training samples, with $t$, $X_0$, $X_1$, and $y$ drawn from the dataset.\n6.  **Inference**: During inference, the model starts with random noise $X_0$ and iteratively refines it to recover the denoised sample $X_1$ using an ODE-based method. A sequence of timesteps $\\{t_0, t_1, ..., t_n\\}$ is defined, where $t_0 = 0$, $t_n = 1$, and $t_0 < t_1 < ... < t_n$. The denoising process is carried out by integrating over these timesteps:\n\n    $X_1 = \\sum_{i=0}^{n-1} u(X_{t_i}, y, t_i; \u03b8) \\cdot (t_{i+1} - t_i)$\n\n    This iterative process allows the model to gradually denoise the input sample, progressing from noise $X_0$ toward the target sample $X_1$ over the defined timesteps.\n\n***\n\n### Benefits of Flow Matching for Video Generation\n\n1.  **Stable Training**: Flow Matching provides a more stable training process compared to other generative modeling techniques like GANs (Generative Adversarial Networks). It avoids issues such as mode collapse and adversarial instability, leading to more reliable convergence.\n2.  **High-Quality Samples**: By learning to predict the velocity field, the model can generate high-quality video frames with consistent motion dynamics and coherent content. The generated videos exhibit fewer artifacts and better visual fidelity.\n3.  **Flexibility**: Flow Matching is flexible and can be combined with various network architectures and conditioning techniques. In Step-Video-T2V, it is integrated with a **diffusion Transformer (DiT)** architecture and conditioned on text prompts using bilingual text encoders, enabling the generation of videos from both English and Chinese prompts.\n4.  **Efficient Inference**: The ODE-based inference method allows for efficient generation of video frames. By iteratively refining the noise sample, the model can produce high-quality videos with a relatively small number of steps, reducing the computational cost of inference.\n5.  **Improved Visual Quality**: The use of Flow Matching, combined with techniques like **Video-DPO (Direct Preference Optimization)**, enhances the visual quality of the generated videos by reducing artifacts and ensuring smoother, more realistic video outputs."
    },
    {
        "question_id": "2502.10248v1_7",
        "answer": "The Step-Video-T2V model employs a hierarchical data filtering approach to refine its training data, which is crucial for achieving high-quality video generation. This process involves multiple stages of filtering, each designed to remove undesirable content and ensure that the model is trained on the most suitable data.\n\nHere's a breakdown of the key aspects of this approach:\n\n### Data Filtering Stages\n\nThe data filtering process is divided into pre-training and post-training stages, each employing specific filters to refine the dataset progressively.\n\n***\n\n### Pre-Training Data Filtering\n\nThe pre-training data undergoes several automated filtering steps:\n\n1.  **Video Segmentation**:\n    *   Raw videos are split into single-shot clips using scene change detection.\n    *   The first and last three frames of each clip are removed to avoid unstable camera movements or transition effects.\n2.  **Video Quality Assessment**:\n    *   Video clips are evaluated and filtered based on multiple quality assessment tags. These tags are computed by uniformly sampling eight frames from each clip.\n    *   **Aesthetic Score**: Assesses the visual appeal of the video clips.\n    *   **NSFW Score**: Identifies and removes content inappropriate for safe work environments.\n    *   **Watermark Detection**: Detects and removes videos with watermarks.\n    *   **Subtitle Detection**: Identifies and removes videos with excessive on-screen text or captions.\n    *   **Saturation Score**: Assesses color saturation to ensure visually appealing content.\n    *   **Blur Score**: Detects and removes blurry videos caused by camera shake or lack of clarity.\n    *   **Black Border Detection**: Detects and removes videos with black borders.\n3.  **Video Motion Assessment**:\n    *   Evaluates motion content to ensure effective model training by calculating the motion score, which is the average of the mean magnitudes of the optical flow between pairs of resized grayscale frames.\n    *   **Motion\\_Mean**: Indicates the general level of motion.\n    *   **Motion\\_Max**: Highlights instances of extreme motion or motion distortion.\n    *   **Motion\\_Min**: Identifies clips with minimal motion.\n4.  **Video Captioning**:\n    *   Employs an in-house **Vision Language Model (VLM)** to generate short and dense captions for video clips.\n    *   **Short Caption**: Provides a concise description of the main subject and action.\n    *   **Dense Caption**: Integrates key elements, emphasizing the main subject, events, environmental and visual aspects, video type and style, as well as camera shots and movements.\n    *   **Original Title**: Incorporates original titles from the raw videos to add diversity to the captions.\n5.  **Video Concept Balancing**:\n    *   Addresses category imbalances and facilitates deduplication by computing embeddings for all video clips using an internal **VideoCLIP** model.\n    *   Clips are grouped into clusters, each representing a specific concept or category, using **K-means clustering**.\n    *   **Cluster\\_Cnt**: The total number of clips in the cluster.\n    *   **Center\\_Sim**: The cosine distance between the clip\u2019s embedding and the cluster center.\n6.  **Video-Text Alignment**:\n    *   Measures how well the captions align with the visual content of the video clips using a **CLIP Score**.\n    *   The **CLIP Score** is computed by averaging the cosine similarities between each frame embedding and the caption embedding.\n\n***\n\n### Post-Training Data Filtering\n\nThe post-training data undergoes manual filtering to ensure high quality:\n\n1.  **Filtering by Video Assessment Scores**:\n    *   Using video assessment scores and heuristic rules, the dataset is filtered to a subset of 30M videos to improve overall quality.\n2.  **Filtering by Video Categories**:\n    *   Videos within the same cluster are filtered using the \"Distance to Centroid\" values to remove those whose distance exceeds a predefined threshold, ensuring diversity within the subset.\n3.  **Labeling by Human Annotators**:\n    *   Human evaluators assess each video for clarity, aesthetics, appropriate motion, smooth scene transitions, and the absence of watermarks or subtitles.\n    *   Captions are manually refined to ensure accuracy and include essential details such as camera movements, subjects, actions, backgrounds, and lighting.\n\n***\n\n### Importance of Hierarchical Data Filtering\n\nThe hierarchical data filtering approach is crucial for training high-quality video generation models for several reasons:\n\n*   **Improved Data Quality**: By systematically filtering out low-quality, inappropriate, or irrelevant content, the model is trained on a dataset that is more representative of the desired output.\n*   **Enhanced Model Performance**: Training on high-quality data leads to better generalization, reduced artifacts, and improved visual quality of the generated videos.\n*   **Efficient Training**: Filtering reduces noise and inconsistencies in the data, allowing the model to converge faster and more effectively.\n*   **Controlled Style and Content**: Curating datasets with specific styles and motion dynamics allows the model to generalize these characteristics across a broader range of prompts.\n*   **Better Instruction Following**: Accurate and rich captions enhance the model's ability to follow text prompts, ensuring that the generated videos align with the user's intentions.\n*   **Resource Optimization**: By focusing on high-quality data, the model can achieve better results with fewer computational resources.\n\n***\n\nIn summary, the hierarchical data filtering approach ensures that the Step-Video-T2V model is trained on a clean, relevant, and high-quality dataset, which is essential for generating state-of-the-art videos with strong motion dynamics, high aesthetics, and consistent content."
    },
    {
        "question_id": "2502.10248v1_8",
        "answer": "Step-Video-T2V is a state-of-the-art text-to-video model and this is how it fares against commercial models:\n\n***\n\n### General Performance\n\n*   Step-Video-T2V delivers comparable performance for general text prompts and even surpasses commercial engines in specific domains.\n*   It stands out in generating videos with high motion dynamics or text content.\n\n***\n\n### Strengths of Step-Video-T2V\n\n*   **Motion Dynamics:** Step-Video-T2V has strong motion dynamics modeling and generation capabilities.\n*   **Efficiency:** It delivers comparable performance for general text prompts without extensive pre- and post-processing.\n*   **Open Source:** The model is open-source, providing transparency and accessibility for researchers and content creators.\n*   **High Compression VAE:** It incorporates a powerful high compression VAE for large-scale video generation training.\n*   **Bilingual Text Prompt Understanding:** It supports bilingual text prompt understanding in both English and Chinese.\n*   **Video-based DPO:** It adds an additional **DPO** stage to the training process, reducing artifacts and improving the visual quality of the generated videos.\n\n***\n\n### Weaknesses of Step-Video-T2V\n\n*   **Aesthetic Appeal:** Commercial models like T2VTopA and T2VTopB have higher aesthetic appeal due to higher resolutions and high-quality aesthetic data used during post-training stages.\n*   **Instruction Following:** T2VTopA has better instruction-following capability, contributing to its superior performance in categories such as Combined Concepts, Surreal, and Cinematography, due to better video captioning model and the greater human effort involved in labeling the post-training data.\n\n***\n\n### Key Differentiators\n\n*   **Pipeline Complexity:** Commercial video generation engines often involve longer and more complex video generation pipelines with extensive pre- and post-processing, Step-Video-T2V delivers comparable performance for general text prompts.\n*   **Training Data:** Commercial models use significantly more high-quality data in the post-training phase.\n*   **Video Length:** Step-Video-T2V generates longer videos (204 frames) compared to some commercial engines, making training more challenging.\n*   **Resolution:** Commercial models like Movie Gen Video can generate higher resolution videos (720P) compared to Step-Video-T2V (540P), which can be a key factor in determining which model performs better."
    },
    {
        "question_id": "2502.10248v1_9",
        "answer": "Human feedback plays a crucial role in refining **Step-Video-T2V**'s video generation by improving the visual quality of the generated videos and ensuring better alignment with user prompts. This is achieved through a pipeline that incorporates human preferences into the training process.\n\n***\n\n### Incorporation of Human Feedback\n\n1.  **Data Collection**: A diverse set of prompts is constructed, including random prompts from the training data and prompts synthesized by human annotators based on real-world user interaction patterns.\n2.  **Video Generation**: For each prompt, **Step-Video-T2V** generates multiple videos using different seeds.\n3.  **Preference Annotation**: Human annotators rate the preference of these generated videos, and quality control personnel monitor the annotation process to ensure accuracy and consistency. This results in a dataset of preference and non-preference data.\n\n***\n\n### Implementation with Direct Preference Optimization (DPO)\n\n*   **Selection of DPO**: **Direct Preference Optimization (DPO)** is chosen as the method for incorporating human feedback due to its effectiveness and simplicity.\n*   **Objective**: The goal is to adjust the model to generate preferred data while avoiding the generation of non-preferred data, given human preference data under the same conditions.\n*   **Reference Policy**: A reference policy (reference model) is introduced to stabilize training and prevent the current policy from deviating too far.\n*   **Policy Objective Formulation**:\n\n    $LDPO = \u2212E(y,xw,xl)\u223cD[log \u03c3(\u03b2(log \frac{\u03c0\u03b8(xw|y)}{\u03c0ref(xw|y)} \u2212log \frac{\u03c0\u03b8(xl|y)}{\u03c0ref(xl|y)}))]$\n\n    where:\n\n    *   $\u03c0\u03b8$ is the current policy.\n    *   $\u03c0ref$ is the reference policy.\n    *   $xw$ is the preferred sample.\n    *   $xl$ is the non-preferred sample.\n    *   $y$ denotes the condition.\n*   **Training Step**: At each step, a prompt and its corresponding positive and negative sample pairs are selected. Each sample is generated by the model itself to ensure smooth updates and improve training stability.\n*   **Consistency**: The positive and negative samples are aligned by fixing the initial noise and timestep, contributing to a more stable training process.\n\n***\n\n### Addressing Limitations and Enhancements\n\n*   **Issue of Saturation**: Improvements saturate when the model easily distinguishes between positive and negative samples because the training data is generated by earlier versions of the model.\n*   **Reward Model**: To address this, a reward model is trained using human-annotated feedback data. This model dynamically evaluates the quality of newly generated samples during training and is periodically fine-tuned to maintain alignment with the evolving policy.\n*   **On-Policy Training**: By integrating the reward model, the training data is scored and ranked on-the-fly (on-policy), improving data efficiency."
    }
]