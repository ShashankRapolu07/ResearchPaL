[
    {
        "question_id": "2502.10248v1_0",
        "answer": "The paper outlines two distinct levels of video foundation models, each characterized by its capabilities and complexity:\n\n### Level 1: Translational Video Foundation Model\n\n*   **Functionality:** This level operates as a cross-modal translation system.\n*   **Capabilities:** It can generate videos from text, visual, or multimodal inputs.\n*   **Examples:** Current diffusion-based text-to-video models like Sora, Veo, Kling, Hailuo, and Step-Video fall into this category. These models excel at producing high-quality videos from text prompts, thereby lowering the barrier for video content creation.\n*   **Limitations:** These models often struggle with generating videos that require complex action sequences or adherence to the laws of physics. They primarily learn mappings between prompts and videos without explicitly modeling underlying causal relationships.\n\n***\n\n### Level 2: Predictable Video Foundation Model\n\n*   **Functionality:** This level functions as a prediction system.\n*   **Capabilities:** Similar to large language models (**LLMs**), it can forecast future events based on text, visual, or multimodal context. It can handle advanced tasks such as reasoning with multimodal data or simulating real-world scenarios.\n*   **Examples:** The paper indicates that autoregression-based text-to-video models introduce causal modeling by predicting the next video token, frame, or clip.\n*   **Limitations:** Autoregression models still cannot achieve performance comparable to **diffusion-based models** on text-to-video generation.\n\n***\n\n### Key Differences Summarized\n\n| Feature                | Level 1: Translational Model                                                 | Level 2: Predictable Model                                                                                                    |\n| :--------------------- | :----------------------------------------------------------------------------- | :---------------------------------------------------------------------------------------------------------------------------- |\n| Primary Function       | Cross-modal translation                                                        | Prediction system                                                                                                             |\n| Core Capability        | Generating videos from various inputs                                          | Forecasting future events and handling advanced reasoning tasks                                                              |\n| Causal Understanding   | Limited; primarily focuses on input-output mappings                             | Explicitly models causal relationships within videos                                                                         |\n| Task Complexity        | Struggles with complex action sequences and physics adherence                  | Capable of reasoning with multimodal data and simulating real-world scenarios                                                 |\n| Model Type (Examples) | **Diffusion-based models** (Sora, Veo, Kling, Hailuo, Step-Video-T2V (report)) | **Autoregression-based models** (the report states these are inferior to diffusion-based models for text-to-video generation) |"
    },
    {
        "question_id": "2502.10248v1_1",
        "answer": "Step-Video-T2V uses a Video-VAE to achieve high spatial and temporal compression while maintaining video reconstruction quality through several key architectural and training innovations.\n\n***\n\n### Key Components of Step-Video-T2V\u2019s Video-VAE\n\n1.  **Dual-Path Architecture**:\n    *   A novel dual-path architecture is introduced in the later stage of the encoder and the early stage of the decoder. This design facilitates unified spatial-temporal compression.\n    *   It maintains high-frequency details through convolutional processing while preserving low-frequency structure via channel averaging.\n2.  **Spatial-Temporal Compression**:\n    *   Achieves 8x16x16 downscaling using 3D convolutions and optimized pixel shuffling operations.\n    *   For an input video tensor $X \\in R^{B \times C \times T \times H \times W}$, the encoder E produces a latent representation $Z = E(X) \\in R^{B \times C_z \times \\lceil T/8 \rceil \times \\lceil H/16 \rceil \times \\lceil W/16 \rceil}$.\n3.  **Causal 3D Convolutional Modules**:\n    *   The early stage of the encoder consists of three stages, each featuring two Causal Res3DBlocks and corresponding downsample layers. A MidBlock combines convolutional layers with attention mechanisms.\n    *   Temporal causality is implemented through:\n        $C3D(X)_t =  \begin{cases}   Conv3D([0, ..., X_t], \\Theta) & t = 0 \\   Conv3D([X_{t-k}, ..., X_t], \\Theta) & t > 0  \\end{cases}$\n        where $k$ is the temporal kernel size.\n4.  **Dual-Path Latent Fusion**:\n    *   **Conv Path**: Combines causal 3D convolutions with pixel unshuffling:\n        $H_{conv} = U^{(3)}_s(C3D(X))$\n        where $U^{(3)}_s: R^{B \times C \times T_{st} \times H_{ss} \times W_{ss}} \rightarrow R^{B \times C \\cdot s^3 \times T_{st} \times H_{ss} \times W_{ss}}$ with spatial stride $s_s = 2$, temporal stride $s_t = 2$, and C3D denoting causal 3D convolution.\n    *   **Shortcut Path**: Preserves structural semantics through grouped channel averaging:\n        $H_{avg} = \frac{1}{s^3} \\sum_{k=0}^{s^3 - 1} U^{(3)}_s (X)[..., kC_z:(k+1)C_z]$\n        where $U^{(3)}_s$ implements 3D pixel unshuffle with spatial-temporal blocking, and $C_z$ is the latent dimension of the next stage.\n    *   The output of fusion combines both paths through residual summation:\n        $Z = H_{conv} \\oplus H_{avg}$\n5.  **Decoder Architecture**:\n    *   The early stage of the decoder consists of two symmetric Dual Path architectures.\n    *   The 3D pixel unshuffle operation $U$ is replaced by a 3D pixel shuffle operator $P$, and the grouped channel averaging path is replaced by a grouped channel repeating operation.\n    *   Spatial groupnorm replaces all groupnorm to avoid temporal flickering between different chunks.\n\n***\n\n### Training Details\n\n1.  **Multi-Stage Training**:\n    *   **First Stage**: Train a VAE with a 4x8x8 compression ratio without a dual-path structure, jointly on images and videos of varying frame counts.\n    *   **Second Stage**: Enhance the model by incorporating two dual-path modules in both the encoder and decoder, gradually unfreezing the dual-path modules, the mid-block, and the ResNet backbone.\n2.  **Loss Functions**:\n    *   A combination of L1 reconstruction loss, Video-LPIPS, and KL-divergence constrain to guide the model.\n    *   GAN loss is introduced to further refine the model\u2019s performance once the above losses have converged.\n\n***\n\n### Impact\n\n*   Enables the model to use its parameters more efficiently, overcoming blurring artifacts typically associated with traditional VAEs.\n*   Achieves state-of-the-art reconstruction quality despite having a higher compression ratio compared to other models.\n*   Facilitates extending the context length or scaling up the DiT model more aggressively."
    },
    {
        "question_id": "2502.10248v1_2",
        "answer": "Direct Preference Optimization (DPO) is a technique used to align a model's behavior with human preferences by directly optimizing the model based on feedback data. Instead of using reinforcement learning, which can be complex and unstable, DPO reframes the problem as a supervised learning task.\n\n***\n\n### How DPO Works:\n\n1.  **Preference Data**: DPO relies on a dataset of preferences, where for a given input (e.g., a text prompt), there are two outputs (e.g., two generated videos): one preferred and one dispreferred. This data is collected from human annotators who evaluate and rank the outputs.\n\n2.  **Objective Function**: The DPO objective aims to adjust the model's policy to favor the generation of preferred outputs while avoiding the generation of dispreferred outputs. It does this by maximizing the log probability of the preferred output relative to the dispreferred output.\n\n3.  **Reference Model**: To stabilize training and prevent the model from deviating too far from its initial state, a reference model is introduced. The DPO objective includes a term that penalizes deviations from this reference model.\n\n4.  **Optimization**: The model is trained by minimizing the DPO loss function, which encourages it to generate outputs that are more aligned with human preferences.\n\n***\n\n### DPO in Step-Video-T2V:\n\nIn Step-Video-T2V, DPO is used to improve the visual quality of the generated videos by incorporating human feedback. The process involves:\n\n1.  **Collecting Preference Data**: Human annotators are presented with multiple videos generated by Step-Video-T2V from the same text prompt but with different random seeds. They then rate the videos based on their preferences.\n\n2.  **Training with DPO**: The DPO loss function is applied to fine-tune the video generation model. This involves adjusting the model's parameters to increase the likelihood of generating preferred videos and decrease the likelihood of generating dispreferred ones.\n\n3.  **Addressing Gradient Issues**: The paper notes that a large $\u03b2$ (a parameter in the DPO loss function) can cause gradient explosion, leading to unstable training. To address this, the authors reduce $\u03b2$ and increase the learning rate, which results in faster convergence.\n\n4.  **Iterative Refinement**: The DPO process is repeated iteratively, with the model being fine-tuned based on new preference data. This allows the model to gradually improve its ability to generate high-quality videos that align with human preferences.\n\n***\n\n### Benefits of DPO in Step-Video-T2V:\n\n*   **Improved Visual Quality**: By incorporating human feedback, DPO helps to reduce artifacts and improve the overall visual appeal of the generated videos.\n*   **Better Alignment with Prompts**: DPO enhances the alignment between the generated videos and the given text prompts, resulting in more accurate and relevant video generation.\n*   **Enhanced Plausibility and Consistency**: Human feedback improves the realism and coherence of the generated videos, making them more visually plausible.\n\n***"
    },
    {
        "question_id": "2502.10248v1_3",
        "answer": "The Step-Video-T2V model employs a Diffusion Transformer (DiT) architecture with 3D full attention, leveraging several key advantages for video generation:\n\n*   **Superior Modeling of Spatial and Temporal Information**: 3D full attention facilitates the simultaneous processing of spatial and temporal data within videos. This contrasts with spatial-temporal attention mechanisms, which handle these dimensions separately. The unified approach allows the model to capture intricate relationships and dependencies across both space and time more effectively.\n*   **Enhanced Video Generation Quality**: By using 3D full attention, Step-Video-T2V is able to generate videos with smoother and more consistent motion. This is particularly important for creating realistic and visually appealing video content.\n*   **Flexibility and Generalization**: The 3D full attention mechanism enables the model to handle video inputs with varying lengths and resolutions, making it more versatile and adaptable to different video datasets. This is achieved without being restricted by fixed positional encoding lengths, thereby improving generalization across diverse video data.\n*   **Effective Use of Positional Encoding**: The model uses **RoPE-3D**, an extension of Rotation-based Positional Encoding, designed to handle video data by accounting for temporal (frame) and spatial (height and width) dimensions. This allows for a flexible and continuous representation of positions in video sequences of varying lengths and resolutions."
    },
    {
        "question_id": "2502.10248v1_4",
        "answer": "The Step-Video-T2V model is designed to understand and process text prompts in both English and Chinese using two distinct bilingual text encoders. Let's delve into the advantages of this approach:\n\n***\n\n### Bilingual Text Encoding in Step-Video-T2V\n\n*   **Two Bilingual Text Encoders**: The model employs both Hunyuan-CLIP and Step-LLM to process user text prompts.\n*   **Hunyuan-CLIP**: It is a bidirectional text encoder from an open-source bilingual CLIP model. Due to the training mechanism of the CLIP model, Hunyuan-CLIP produces text representations well-aligned with the visual space. However, it is limited to processing prompts with a maximum length of 77 tokens.\n*   **Step-LLM**: It is an in-house, unidirectional bilingual text encoder pre-trained using the next-token prediction task. It incorporates a redesigned Alibi-Positional Embedding to improve efficiency and accuracy in sequence processing. Unlike Hunyuan-CLIP, Step-LLM has no input length restriction, making it effective for handling lengthy and complex text sequences.\n\n***\n\n### Advantages of Using Two Separate Text Encoders\n\n1.  **Handling Varying Prompt Lengths**:\n    *   Hunyuan-CLIP is effective for shorter prompts due to its CLIP-based training, which ensures good alignment with the visual space.\n    *   Step-LLM excels with longer, more complex prompts because it has no input length restrictions.\n2.  **Robust Text Representations**: By combining these two encoders, Step-Video-T2V generates robust text representations that guide the model effectively in the latent space.\n3.  **Optimized Efficiency and Accuracy**:\n    *   Step-LLM's redesigned Alibi-Positional Embedding enhances both efficiency and accuracy in processing sequences.\n4.  **Leveraging CLIP Model Advantages**: Hunyuan-CLIP benefits from the CLIP model's training, creating text representations well-aligned with the visual space."
    },
    {
        "question_id": "2502.10248v1_5",
        "answer": "Current diffusion-based text-to-video models face several significant challenges, which Step-Video-T2V attempts to address through its architecture, training strategies, and system-level optimizations. These challenges include:\n\n***\n\n### 1. Computational Complexity and High-Dimensional Data\n\n**Challenge:** Videos inherently contain both spatial and temporal information, leading to significantly larger data volumes compared to images. This high dimensionality results in substantial computational costs for modeling video data, making large-scale training difficult.\n\n**Step-Video-T2V's Approach:**\n\n*   **Deep Compression Video-VAE:** Step-Video-T2V employs a specially designed deep compression **Variational Autoencoder (VAE)** to reduce the computational complexity of large-scale video generation training. The **Video-VAE** achieves a **16x16 spatial and 8x temporal compression ratio**, significantly reducing the number of tokens that the model needs to process.\n*   **Dual-Path Architecture:** The **Video-VAE** introduces a novel dual-path architecture at the later stage of the encoder and the early stage of the decoder, featuring unified spatial-temporal compression. This design achieves **8\u00d716\u00d716 downscaling** through the synergistic use of 3D convolutions and optimized pixel unshuffling operations.\n*   **Causal 3D Convolutional Modules:** The encoder consists of three stages, each featuring two **Causal Res3DBlock** and corresponding downsample layers. A **MidBlock** combines convolutional layers with attention mechanisms to further refine the compressed representations. Temporal causality is implemented using the formula:\n\n    $C3D(X)_t = \begin{cases} \text{Conv3D}([0, ..., X_t], \\Theta) & t = 0 \\ \text{Conv3D}([X_{t-k}, ..., X_t], \\Theta) & t > 0 \\end{cases}$\n\n    where $k$ is the temporal kernel size.\n*   **Dual-Path Latent Fusion:** Maintains high-frequency details through convolutional processing while preserving low-frequency structure via channel averaging.\n\n***\n\n### 2. Lack of High-Quality Labeled Data\n\n**Challenge:** High-quality labeled data is crucial for supervised fine-tuning (SFT) but remains a significant hurdle. Existing video captioning models often struggle with hallucination issues, and human annotations are expensive and difficult to scale.\n\n**Step-Video-T2V's Approach:**\n\n*   **Cascaded Training Pipeline:** To make the most of available data, Step-Video-T2V employs a cascaded training pipeline, including text-to-image pre-training, text-to-video pre-training, supervised fine-tuning (SFT), and direct preference optimization (DPO).\n*   **Data Filtering and Preprocessing:** The model uses a comprehensive data pipeline to transform raw videos into high-quality video-text pairs suitable for model pre-training, including video segmentation, quality assessment, motion assessment, captioning, concept balancing, and text alignment.\n*   **Post-Training Data Curation:** For SFT, the model curates a high-quality video dataset that captures good motion, realism, aesthetics, a broad range of concepts, and accurate captions.\n*   **Video-Text Alignment:** CLIP Score is computed by averaging the cosine similarities between each frame embedding and the caption embedding.\n\n***\n\n### 3. Instruction Following and Compositionality\n\n**Challenge:** Current models struggle with generating videos that involve complex action sequences or require the composition of multiple concepts with low occurrence in the training data (e.g., an elephant and a penguin).\n\n**Step-Video-T2V's Approach:**\n\n*   **Bilingual Text Encoders:** Step-Video-T2V uses two bilingual text encoders (**Hunyuan-CLIP** and **Step-LLM**) to process user text prompts, enabling the model to handle user prompts of varying lengths and generate robust text representations.\n*   **Cross-Attention Mechanism:** Introduces a cross-attention layer between the self-attention and feed-forward network (FFN) in each transformer block to incorporate text prompts.\n*    **Hierarchical Data Filtering**: Applies a series of filters to the data, progressively increasing their thresholds to create pre-training subsets. The final SFT dataset is then constructed through manual filtering.\n*   **Addressing Attention Distribution:** The model seeks to balance the distribution of cross-attention scores to ensure that all elements in the prompt receive appropriate attention, preventing missing objects or incomplete action sequences.\n\n***\n\n### 4. Adherence to the Laws of Physics\n\n**Challenge:** Many models struggle to accurately simulate the real world and generate videos that adhere to the laws of physics, such as a ball bouncing on the floor.\n\n**Step-Video-T2V's Approach:**\n\n*   **3D Full Attention:** Employs 3D full attention in Step-Video-T2V instead of spatial-temporal attention to model both spatial and temporal information in videos.\n\n***\n\n### 5. Post-Training Optimization\n\n**Challenge:** Optimizing video generation models through reinforcement learning is challenging because defining clear reward functions for video generation is difficult.\n\n**Step-Video-T2V's Approach:**\n\n*   **Video-based DPO:** Applies a video-based **Direct Preference Optimization (DPO)** approach to reduce artifacts and improve the visual quality of the generated videos. The training objective can be formulated as:\n\n    $L_{DPO} = -E_{(y, x_w, x_l) \\sim D} \\left[ \\log \\sigma \\left( \beta \\left( \\log \frac{\\pi_\theta(x_w|y)}{\\pi_{ref}(x_w|y)} - \\log \frac{\\pi_\theta(x_l|y)}{\\pi_{ref}(x_l|y)} \right) \right) \right]$\n*   **Training a Reward Model:** The model trains a reward model using human-annotated feedback data to dynamically evaluate the quality of newly generated samples during training.\n\n***\n\n### 6. System-Level Optimization\n\n**Challenge:** Training large models requires efficient infrastructure and resource management.\n\n**Step-Video-T2V's Approach:**\n\n*   **Training Framework Optimizations:** The model employs a training emulator (**Step Emulator**) to estimate resource consumption and end-to-end performance during training under various model architectures and parallelism configurations.\n*   **Distributed Training:** Uses parallelism strategies such as Tensor-parallelism (TP), Sequence-parallelism (SP), Context-parallelism (CP), Pipeline-parallelism (PP) and Virtual Pipeline-parallelism (VPP).\n*   **StepRPC:** Leverages a high-performance communication framework (**StepRPC**) to facilitate data transfer, incorporating tensor-native communication over RDMA and TCP and flexible workload patterns with high resilience.\n*   **StepTelemetry:** Implements an observability suite (**StepTelemetry**) for training frameworks to enhance anomaly detection capabilities and establish a reusable pipeline for collecting, post-processing, and analyzing training-related data.\n*   **StepMind:** Utilizes a distributed training platform (**StepMind**) designed for large-scale machine learning workloads, achieving high GPU utilization rates."
    },
    {
        "question_id": "2502.10248v1_6",
        "answer": "Here's how Step-Video-T2V utilizes Flow Matching during training, and the benefits it brings to video generation:\n\n### Flow Matching in Step-Video-T2V\n\nThe training process involves predicting the velocity field that transforms noise into a coherent video frame.\n\n1.  **Sampling Noise and Timestep**:\n    *   A Gaussian noise, denoted as $X_0$, is sampled from a standard normal distribution, $X_0 \\sim N(0, 1)$.\n    *   A random timestep $t$ is sampled from a uniform distribution between 0 and 1, $t \\in [0, 1]$.\n\n2.  **Constructing Model Input**:\n    *   The model input $X_t$ is created through a linear interpolation between the initial noise $X_0$ and the target sample $X_1$, where $X_1$ represents the noise-free input.\n    *   The equation for creating $X_t$ is:\n        $X_t = (1 - t) \\cdot X_0 + t \\cdot X_1$\n\n3.  **Defining Ground Truth Velocity**:\n    *   The ground truth velocity $V_t$ represents the rate of change of $X_t$ with respect to the timestep $t$.\n    *   It is defined as the difference between the target data $X_1$ and the initial noise $X_0$:\n        $V_t = \frac{dX_t}{dt} = X_1 - X_0$\n\n4.  **Training the Model**:\n    *   The model is trained to predict the velocity field $u(X_t, y, t; \theta)$ at timestep $t$, given the input $X_t$ and optional conditioning input $y$ (e.g., text prompt).\n    *   The training loss is the Mean Squared Error (**MSE**) between the predicted velocity $u(X_t, y, t; \theta)$ and the true velocity $V_t$.\n    *   The loss function is:\n        $loss = E_{t, X_0, X_1, y} [\\|u(X_t, y, t; \theta) - V_t\\|^2]$\n\n        where the expectation is taken over all training samples.\n\n5.  **Inference**:\n    *   Starts by sampling random noise $X_0 \\sim N(0, 1)$.\n    *   The denoised sample $X_1$ is recovered by iteratively refining the noise through an **ODE**-based method using a Gaussian solver.\n    *   A sequence of timesteps $\\{t_0, t_1, ..., t_n\\}$ is defined, where $t_0 = 0$, $t_n = 1$, and $t_0 < t_1 < ... < t_n$.\n    *   The denoising process is expressed as:\n        $X_1 = \\sum_{i=0}^{n-1} u(X_{t_i}, y, t_i; \theta) \\cdot (t_{i+1} - t_i)$\n\n### Benefits of Flow Matching for Video Generation\n\n1.  **Stable Training**:\n    *   Flow Matching provides a more stable training process compared to other diffusion methods. By directly predicting the velocity field, the model learns a smoother and more continuous transformation from noise to data.\n    *   This stability is particularly beneficial for video generation, where temporal coherence is crucial.\n\n2.  **High-Quality Samples**:\n    *   The use of Flow Matching results in the generation of high-quality video samples with fewer artifacts. The model learns to denoise the input progressively, leading to more realistic and coherent video frames.\n\n3.  **Efficient Inference**:\n    *   Flow Matching allows for efficient inference through **ODE** solvers. The denoising process can be performed in a relatively small number of steps, reducing the computational cost of generating videos.\n    *   This efficiency is important for practical applications where real-time or near real-time video generation is required.\n\n4.  **Flexibility and Control**:\n    *   The framework allows for the incorporation of conditioning inputs (e.g., text prompts) to guide the video generation process. This provides greater control over the content and style of the generated videos.\n\n5.  **Emulating Human Cognition**:\n    *   Adjusting training data based on human-perceived quality leads to significant reductions in training loss, suggesting that the model's learning process may emulate human cognitive patterns to some extent."
    },
    {
        "question_id": "2502.10248v1_7",
        "answer": "Step-Video-T2V employs a hierarchical data filtering approach to refine its training data progressively. This process involves applying a series of filters with increasing thresholds, creating multiple subsets of pre-training data. The final stage involves manual filtering to construct a high-quality dataset for supervised fine-tuning (**SFT**).\n\nHere's a breakdown of the key aspects:\n\n1.  **Progressive Filtering:**\n    *   The filtering process is structured in a hierarchical manner, where data subsets are created by applying filters with progressively stricter criteria.\n    *   This allows the model to initially train on a broader dataset and then gradually refine its focus to higher-quality examples.\n\n***\n\n2.  **Key Filters:**\n\n    Based on the text excerpts, the filtering process involves:\n\n    *   **Video Assessment Scores**: Utilizing video assessment scores and heuristic rules to filter the initial dataset down to a smaller, higher-quality subset.\n    *   **Video Categories**: Removing videos within the same cluster that are too distant from the centroid, ensuring diversity while maintaining category representation.\n    *   **Human Annotation**: Human evaluators assess videos for clarity, aesthetics, motion, scene transitions, and the absence of artifacts like watermarks. Captions are also refined for accuracy and detail.\n\n***\n\n3.  **Importance for High-Quality Video Generation Models:**\n\n    *   **Improved Data Quality**: Filtering removes noisy, low-quality, or irrelevant data, leading to a cleaner dataset.\n    *   **Enhanced Model Performance**: Training on high-quality data results in improved video generation quality, stability, and reduced artifacts. The paper notes a reduction in loss correlating with improved data quality during pre-training.\n    *   **Better Generalization**: A well-filtered dataset ensures a diverse range of concepts, styles, and motion dynamics, improving the model's ability to generalize to various prompts and scenarios.\n    *   **Emulating Human Cognition**: The paper suggests that adjusting the training data to reflect what humans consider higher quality leads to a significant reduction in training loss. This implies that the model's learning process may emulate human cognitive patterns to some extent.\n    *   **Efficient Training**: By focusing on high-quality data, the model can learn more effectively and efficiently, requiring fewer resources and iterations to achieve desired performance levels."
    },
    {
        "question_id": "2502.10248v1_8",
        "answer": "Step-Video-T2V, as detailed in the technical report, aims to achieve comparable or superior performance to commercial video generation engines like Sora and Veo, especially in specific domains. Here's a breakdown of how it stacks up:\n\n### General Text Prompt Performance\n\n*   **Step-Video-T2V**: Delivers comparable performance for general text prompts.\n*   **Commercial Engines (Sora, Veo)**: Often involve more complex video generation pipelines with extensive pre- and post-processing.\n\n### Specific Domain Performance\n\n*   **Step-Video-T2V**: Surpasses commercial engines in generating videos with high motion dynamics or text content.\n*   **Commercial Engines (Sora, Veo)**: May not always excel in these specific areas compared to Step-Video-T2V.\n\n### Key Differentiators for Step-Video-T2V\n\n*   **High Compression VAE**: Incorporates a more powerful high compression **VAE** for large-scale video generation training.\n*   **Bilingual Support**: Supports bilingual text prompt understanding in both English and Chinese.\n*   **DPO Stage**: Adds an additional **DPO** stage to the training process, reducing artifacts and improving the visual quality of the generated videos.\n*   **Open Source**: Open-source and provides state-of-the-art video generation quality comparing with both open-source and commercial engines.\n\n### Challenges and Limitations\n\n*   **Instruction Following**: Struggles to generate videos involving complex action sequences or requiring adherence to the laws of physics.\n*   **Composition of Concepts**: Faces challenges when generating videos that incorporate multiple concepts with low occurrence in the training data (e.g., an elephant and a penguin).\n*   **Training Resources**: Requires significant computational resources for training and generating long-duration, high-resolution videos.\n\n***"
    },
    {
        "question_id": "2502.10248v1_9",
        "answer": "Human feedback plays a crucial role in refining **Step-Video-T2V's** video generation by improving the visual quality of the generated videos and ensuring better alignment with user prompts. This is achieved through a pipeline that incorporates human preferences into the training process.\n\n***\n\nHere's a breakdown of how human feedback is implemented in the training pipeline:\n\n1.  **Data Collection**:\n    *   A diverse set of prompts is constructed, including prompts from the training data and those synthesized by human annotators based on real-world user interaction patterns.\n    *   **Step-Video-T2V** generates multiple videos for each prompt using different seeds.\n    *   Human annotators rate the preference of these samples, and the annotation process is monitored for accuracy and consistency.\n    *   This process results in a set of preference and non-preference data, which serves as the foundation for model training.\n\n2.  **Direct Preference Optimization (DPO)**:\n    *   **DPO** is selected as the method for incorporating human feedback due to its effectiveness and simplicity.\n    *   The goal is to adjust the model to generate preferred data while avoiding non-preferred data, given human preference data under the same conditions.\n    *   A reference policy is introduced to stabilize training and prevent the current policy from deviating too far.\n    *   The policy objective is formulated as:\n\n        $L_{DPO} = -E_{(y, x_w, x_l) \\sim D} \\left[ \\log \\sigma \\left( \beta \\left[ \\log \frac{\\pi_\theta(x_w|y)}{\\pi_{ref}(x_w|y)} - \\log \frac{\\pi_\theta(x_l|y)}{\\pi_{ref}(x_l|y)} \right] \right) \right]$\n\n        where $\\pi_\theta$ and $\\pi_{ref}$ refer to current policy and reference policy, respectively, $x_w$ and $x_l$ are the preferred sample and non-preferred sample, and $y$ denotes the condition.\n\n3.  **Training Process**:\n\n    *   At each step, a prompt and its corresponding positive and negative sample pairs are selected.\n    *   Each sample is generated by the model itself, ensuring smooth updates and improving overall training stability.\n    *   The positive and negative samples are aligned by fixing the initial noise and timestep to maintain consistency in the training data.\n\n4.  **Addressing Gradient Issues**:\n\n    *   The training objective is based on the **DiffusionDPO** method and **DPO**, with slight modifications to extend it to the **Flow Matching** framework.\n    *   To address potential gradient explosion issues, the parameter $\beta$ is reduced, and the learning rate is increased to achieve faster convergence.\n\n5.  **Limitations and Improvements**:\n\n    *   The improvements from human feedback saturate when the model can easily distinguish between positive and negative samples.\n    *   To address this, a reward model is trained using human-annotated feedback data to dynamically evaluate the quality of newly generated samples during training.\n    *   The reward model is periodically fine-tuned to maintain alignment with the evolving policy, improving data efficiency."
    }
]