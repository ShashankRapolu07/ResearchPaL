[
    {
        "question_id": "2502.10248v1_0",
        "answer": "The paper defines two levels of video foundation models, each representing a different stage of sophistication and capability:\n\n### Level 1: Translational Video Foundation Model\n\n*   This level focuses on cross-modal translation.\n*   Models at this level can generate videos from text, visual, or multimodal contexts.\n*   They function as a system that translates input from one modality (e.g., text) into a video.\n*   Current diffusion-based text-to-video models belong to this level.\n*   **Limitation**: These models often struggle with generating videos that require complex action sequences or adherence to the laws of physics, because they learn mappings between prompts and videos, without modeling underlying causal relationships.\n\n***\n\n### Level 2: Predictable Video Foundation Model\n\n*   This level focuses on acting as a prediction system.\n*   Models can forecast future events based on text, visual, or multimodal contexts.\n*   They can handle more advanced tasks such as reasoning with multimodal data or simulating real-world scenarios.\n*   These models are similar to large language models (LLMs) in their ability to predict and reason.\n*   **Limitation**: Autoregression-based models introduce causal modeling by predicting the next video token, frame, or clip, but they still cannot achieve performance comparable to diffusion-based models on text-to-video generation.\n\n***\n\n### Key Differences Summarized\n\n| Feature               | Level 1: Translational                  | Level 2: Predictable                         |\n| :-------------------- | :-------------------------------------- | :--------------------------------------------- |\n| Primary Function      | Cross-Modal Translation                 | Prediction and Reasoning                       |\n| Input                 | Text, Visual, Multimodal Contexts       | Text, Visual, Multimodal Contexts              |\n| Output                | Generated Video                         | Forecasted Events, Simulated Scenarios         |\n| Complexity            | Lower                                   | Higher                                         |\n| Causal Relationship   | Not Explicitly Modeled                | Modeled                                        |\n| Examples              | Current Diffusion-Based Models          | Advanced Models (Future Development)           |\n| Tasks                 | Basic video generation from prompts      | Reasoning with multimodal data, simulating scenarios |"
    },
    {
        "question_id": "2502.10248v1_1",
        "answer": "Step-Video-T2V uses a specialized **Video-VAE** architecture to achieve high spatial and temporal compression while maintaining video reconstruction quality. Here's a breakdown of the key components and techniques:\n\n***\n\n### 1. High-Compression Ratios\n\n*   The **Video-VAE** achieves a **16x16 spatial** and **8x temporal compression ratio**. This means the video is significantly downsampled in both spatial dimensions (height and width) and the temporal dimension (number of frames).\n\n### 2. Novel Dual-Path Architecture\n\n*   **Unified Spatial-Temporal Compression:** The architecture introduces a dual-path design in the later stages of the encoder and the early stages of the decoder. This design facilitates simultaneous spatial and temporal compression.\n*   For an input video tensor $X \\in R^{B \times C \times T \times H \times W}$, the encoder $E$ produces a latent representation $Z = E(X) \\in R^{B \times C_z \times \\lceil T/8 \rceil \times \\lceil H/16 \rceil \times \\lceil W/16 \rceil}$.\n\n### 3. Causal 3D Convolutional Modules\n\n*   The encoder's early stages consist of three stages, each featuring two **Causal Res3DBlock** modules and corresponding downsample layers. A **MidBlock** combines convolutional layers with attention mechanisms to refine the compressed representations further.\n*   **Temporal Causality:** The architecture implements temporal causality using causal 3D convolutions:\n\n    $C3D(X)_t = \begin{cases} Conv3D([0, ..., X_t], \\Theta) & t = 0 \\ Conv3D([X_{t-k}, ..., X_t], \\Theta) & t > 0 \\end{cases}$\n\n    where $k$ is the temporal kernel size, ensuring frame $t$ only depends on previous frames.\n\n### 4. Dual-Path Latent Fusion\n\n*   **Conv Path:** Combines causal 3D convolutions with pixel unshuffling:\n\n    $H_{conv} = U_s^{(3)}(C3D(X))$\n\n    where $U_s^{(3)}: R^{B \times C \times T_{st} \times H_{ss} \times W_{ss}} \rightarrow R^{B \times C \\cdot s^3 \times T_{st} \times H_{ss} \times W_{ss}}$ with spatial stride $s_s = 2$ and temporal stride $s_t = 2$, and $C3D$ denotes the causal 3D convolution.\n*   **Shortcut Path:** Preserves structural semantics through grouped channel averaging:\n\n    $H_{avg} = \frac{1}{s^3} \\sum_{k=0}^{s^3 - 1} U_s^{(3)}(X)[..., kC_z:(k+1)C_z]$\n\n    where $U_s^{(3)}$ implements 3D pixel unshuffle with spatial-temporal blocking, and $C_z$ is the latent dimension of the next stage.\n*   The output of the fusion combines both paths through residual summation:\n\n    $Z = H_{conv} \\oplus H_{avg}$\n\n### 5. Decoder Architecture\n\n*   The decoder's early stage consists of two symmetric **Dual Path** architectures. The 3D pixel unshuffle operation $U$ is replaced by a 3D pixel shuffle operator $P$, and the grouped channel averaging path is replaced by a grouped channel repeating operation.\n\n### 6. Multi-Stage Training\n\n*   **Stage 1:** Train a VAE with a 4x8x8 compression ratio without a dual-path structure, jointly on images and videos.\n*   **Stage 2:** Enhance the model by incorporating two dual-path modules in both the encoder and decoder. Gradually unfreeze the dual-path modules, the **MidBlock**, and the **ResNet** backbone.\n*   **Loss Functions:** Utilizes a combination of **L1 reconstruction loss**, **Video-LPIPS**, and **KL-divergence** to guide the model. **GAN loss** is introduced to further refine the model\u2019s performance.\n***"
    },
    {
        "question_id": "2502.10248v1_2",
        "answer": "Direct Preference Optimization (**DPO**) is a technique used to align a model's behavior with human preferences, particularly in generative tasks. In the context of Step-Video-T2V, **DPO** is employed to enhance the visual quality of the generated videos by incorporating human feedback.\n\n***\n\nHere's a breakdown of how **DPO** works and its impact on Step-Video-T2V:\n\n### Core Idea\n\n**DPO** aims to train a model to generate outputs that are preferred by humans, while avoiding outputs that are considered undesirable. It directly optimizes the model's policy based on preference data, which consists of pairs of preferred and non-preferred samples.\n\n### Process\n\n1.  **Data Collection**: Human annotators are presented with multiple videos generated by the model from the same prompt. They then select which video they prefer and which they disprefer.\n2.  **Objective Function**: The **DPO** loss function adjusts the model to favor the generation of preferred data while discouraging the generation of non-preferred data. A reference policy is introduced to stabilize training and prevent the current policy from deviating too far. The policy objective is formulated as:\n\n    $L_{DPO} = -E_{(y, x_w, x_l) \\sim D} \\left[ log \\ \\sigma \\left( \beta \\left( log \\ \frac{\\pi_\theta (x_w | y)}{\\pi_{ref}(x_w | y)} - log \\ \frac{\\pi_\theta (x_l | y)}{\\pi_{ref}(x_l | y)} \right) \right) \right]$\n\n    where:\n\n    *   $\\pi_\theta$ is the current policy.\n    *   $\\pi_{ref}$ is the reference policy.\n    *   $x_w$ is the preferred sample.\n    *   $x_l$ is the non-preferred sample.\n    *   $y$ denotes the condition (e.g., text prompt).\n    *   $\beta$ is a hyperparameter controlling the strength of the preference optimization.\n3.  **Training**: At each step, a prompt and its corresponding positive and negative sample pairs are selected. The model is trained to align with the preferred samples.\n4.  **Addressing Gradient Issues**: The paper modifies the **DPO** loss to mitigate gradient explosion issues. High $\beta$ values can cause gradient explosions, so the paper reduces $\beta$ and increases the learning rate for faster convergence.\n\n### Benefits\n\n*   **Improved Visual Quality**: By incorporating human feedback, **DPO** enhances the plausibility and consistency of the generated videos.\n*   **Better Alignment with Prompts**: **DPO** improves the alignment with the given prompts, resulting in more accurate and relevant video generation.\n*   **Reduction of Artifacts**: The **DPO** approach effectively reduces artifacts and ensures smoother, more realistic video outputs.\n\n### Limitations\n\n*   **Data Staleness**: The training data used in **DPO** is generated by earlier versions of the model, which may lead to inefficient data utilization as the model evolves.\n*   **Tradeoff Between Feedback Sparsity and Imprecision**: Feedback in video diffusion models is often a single scalar or lacks precision, despite videos having millions of pixels.\n*   **Regression vs. Softmax**: Diffusion models rely on regression, which may be less efficient for preference optimization compared to token-level softmax in **LLM**s.\n\n***\n\nIn summary, **DPO** enhances Step-Video-T2V by incorporating human preferences directly into the training process, leading to improved visual quality, better alignment with prompts, and reduced artifacts."
    },
    {
        "question_id": "2502.10248v1_3",
        "answer": "The Step-Video-T2V model leverages a Diffusion Transformer (DiT) architecture with 3D full attention, offering several key advantages for video generation:\n\n*   **Superior Spatial-Temporal Modeling:** 3D full attention is theoretically superior for modeling both spatial and temporal information in videos compared to spatial-temporal attention. This allows the model to capture intricate relationships between objects and their movements across the video frames.\n*   **Enhanced Video Quality:** Large-scale experiments revealed that 3D full attention leads to the generation of videos with smoother and more consistent motion. This results in a more visually appealing and realistic output.\n*   **Effective Text Prompt Integration:** The model incorporates text prompts through cross-attention layers within each transformer block. This enables the model to attend to textual information while processing visual features, allowing for the generation of videos conditioned on the input prompt.\n*   **Computational Efficiency Optimizations:** Adaptive Layer Normalization (AdaLN) is used, but without class labels, to reduce computational overhead. Additionally, the AdaLN-Single structure is adopted to improve overall model efficiency.\n*   **Handling Variable Video Dimensions:** RoPE-3D (Rotation-based Positional Encoding) is employed to handle video data by accounting for temporal (frame) and spatial (height and width) dimensions. This allows the model to process videos with different frame counts and resolutions without being restricted by fixed positional encoding lengths."
    },
    {
        "question_id": "2502.10248v1_4",
        "answer": "Step-Video-T2V employs a dual approach to handle bilingual text prompts, leveraging two distinct text encoders, to process user input in both English and Chinese. Here's a breakdown:\n\n### Bilingual Text Encoding in Step-Video-T2V\n\n1.  **Hunyuan-CLIP:** This is a bidirectional text encoder derived from an open-source bilingual CLIP model.\n\n    *   **Advantage:** It excels at producing text representations that are well-aligned with the visual space due to the training mechanism inherent in CLIP models.\n    *   **Limitation:** It has a restricted input length of 77 tokens, posing challenges when dealing with longer user prompts.\n2.  **Step-LLM:** This is an in-house, unidirectional bilingual text encoder pre-trained using the next-token prediction task.\n\n    *   **Advantage:** It incorporates a redesigned Alibi-Positional Embedding, enhancing both efficiency and accuracy in sequence processing. It has no input length restrictions, making it suitable for handling lengthy and complex text sequences.\n    *   **Limitation:** Being unidirectional, it might capture contextual information differently compared to bidirectional models like Hunyuan-CLIP.\n\n### Advantages of Using Two Separate Text Encoders\n\n*   **Handling Variable-Length Prompts:** By combining Hunyuan-CLIP and Step-LLM, Step-Video-T2V can effectively process user prompts of varying lengths. Hunyuan-CLIP handles shorter prompts with strong visual alignment, while Step-LLM manages longer, more complex text sequences.\n*   **Robust Text Representations:** The dual-encoder approach allows the model to generate robust text representations that guide the model effectively in the latent space. This ensures that the generated videos are well-conditioned on the input text, regardless of its length or complexity.\n*   **Optimized Efficiency and Accuracy:** Hunyuan-CLIP's architecture is optimized for shorter sequences, providing efficiency in processing, while Step-LLM's redesigned Alibi-Positional Embedding enhances both efficiency and accuracy in handling longer sequences.\n\n***\n"
    },
    {
        "question_id": "2502.10248v1_5",
        "answer": "Okay, I will address the challenges faced by current diffusion-based text-to-video models and how Step-Video-T2V attempts to address them.\n\n***\n\nHere's a breakdown of the main challenges and Step-Video-T2V's approach:\n\n### Challenges of Current Diffusion-Based Text-to-Video Models\n\n1.  **Difficulty in Modeling Complex Action Sequences and Physics**:\n\n    *   Current models struggle to generate videos that involve intricate action sequences (e.g., gymnastic performances) or adhere to the laws of physics (e.g., a bouncing basketball).\n    *   This limitation arises because these models primarily learn mappings between text prompts and corresponding videos without explicitly modeling underlying causal relationships.\n2.  **Instruction Following**:\n\n    *   Generating videos that incorporate multiple concepts with low occurrence in the training data (e.g., an elephant and a penguin) remains challenging.\n    *   Models often miss objects, details, or complete action sequences due to concentrated cross-attention scores on specific elements.\n3.  **Hallucination Issues in Video Captioning**:\n\n    *   Existing video captioning models face hallucination issues, leading to unstable training and poor instruction-following performance.\n4.  **Computational Cost**:\n\n    *   Training and generating long-duration, high-resolution videos pose significant computational hurdles.\n5.  **High-Quality Labeled Data**:\n\n    *   The availability of high-quality labeled data remains a significant hurdle. Human annotations are expensive and difficult to scale.\n\n***\n\n### How Step-Video-T2V Attempts to Address These Challenges\n\n1.  **High-Compression Video-VAE**:\n\n    *   Step-Video-T2V employs a deep compression **Video-VAE**, achieving **16x16 spatial** and **8x temporal compression ratios**.\n    *   This reduces the computational complexity of large-scale video generation training, making it more efficient.\n2.  **Bilingual Text Encoders**:\n\n    *   The model uses two bilingual text encoders to handle both English and Chinese prompts, enhancing its ability to understand diverse user inputs.\n3.  **Cascaded Training Pipeline**:\n\n    *   A cascaded training pipeline, including text-to-image pre-training, text-to-video pre-training, supervised fine-tuning (**SFT**), and direct preference optimization (**DPO**), is introduced.\n    *   This accelerates model convergence and leverages video datasets of varying quality.\n4.  **Video-Based DPO Approach**:\n\n    *   A **Video-DPO** approach is applied to reduce artifacts and improve the visual quality of the generated videos, addressing issues related to realism and consistency.\n5.  **Text-to-Image Pre-training**:\n\n    *   Text-to-image pre-training is used to help the video generation model acquire rich visual knowledge, including concepts, scenes, and their spatial relationships.\n6.  **Low-Resolution Text-to-Video Pre-training**:\n\n    *   Text-to-video pre-training at low resolution is critical for the model to learn motion dynamics.\n7.  **Use of High-Quality Videos in SFT**:\n\n    *   Using high-quality videos with accurate captions and desired styles in **SFT** is crucial to the stability of the model and the style of the generated videos.\n8.  **Hierarchical Data Filtering**:\n\n    *   Employs a series of filters to the data, progressively increasing their thresholds to create pre-training subsets. The final **SFT** dataset is then constructed through manual filtering.\n9.  **Bucketization for Variable Duration and Size**:\n\n    *   To accommodate varying video lengths and aspect ratios during training, variable-length and variable-resolution strategies are employed.\n\n***"
    },
    {
        "question_id": "2502.10248v1_6",
        "answer": "Step-Video-T2V employs **Flow Matching** as its training objective, a technique that offers a smooth and continuous approach to transforming noise into data. This method is particularly beneficial for video generation due to its ability to model complex temporal dynamics.\n\nHere's a breakdown of how **Flow Matching** is used in Step-Video-T2V and its advantages:\n\n### Flow Matching Implementation\n\n1.  **Noise Sampling**: The process starts by sampling random **Gaussian noise** ($X_0$) from a standard normal distribution.\n2.  **Target Sample**: A target sample ($X_1$) is selected, representing the noise-free input.\n3.  **Linear Interpolation**: The model input ($X_t$) is constructed as a linear interpolation between the initial noise ($X_0$) and the target data ($X_1$) at a random time step $t$ \u2208 \\[0, 1\\].\n    This is mathematically represented as:\n\n    $X_t = (1 - t) \\cdot X_0 + t \\cdot X_1$\n4.  **Velocity Calculation**: The ground truth velocity ($V_t$) is calculated, representing the rate of change of $X_t$ with respect to time $t$:\n\n    $V_t = \frac{dX_t}{dt} = X_1 - X_0$\n\n    This velocity captures both the direction and magnitude of change from the initial noise to the target data.\n5.  **Model Training**: The model is trained to predict the velocity field $u(X_t, y, t; \u03b8)$ at any point $X_t$, conditioned on an optional input $y$ (e.g., a text prompt). The training loss minimizes the **Mean Squared Error (MSE)** between the predicted velocity and the true velocity:\n\n    $\text{loss} = \\mathbb{E}_{t, X_0, X_1, y} \\left[ \\| u(X_t, y, t; \u03b8) - V_t \\|_2^2 \right]$\n\n    Here, $\u03b8$ represents the model parameters, and the expectation is taken over all training samples.\n6.  **Inference**: During inference, the model starts with random noise and iteratively refines it using an **Ordinary Differential Equation (ODE)** solver. The denoised sample $X_1$ is recovered by integrating over a sequence of timesteps:\n\n    $X_1 = \\sum_{i=0}^{n-1} u(X_{t_i}, y, t_i; \u03b8) \\cdot (t_{i+1} - t_i)$\n\n    This iterative process gradually denoises the input sample, progressing from noise $X_0$ towards the target sample $X_1$.\n\n***\n\n### Benefits of Flow Matching for Video Generation\n\n1.  **Coherent Temporal Dynamics**: **Flow Matching** inherently models the continuous transformation from noise to data. This is particularly beneficial for video generation as it helps in creating smooth transitions between frames, leading to more coherent and realistic motion.\n2.  **Stable Training**: By predicting the velocity field, **Flow Matching** avoids the abrupt changes often encountered in other generative models. This leads to more stable training dynamics, especially important for the high-dimensional space of video data.\n3.  **Flexibility and Control**: Conditioning the velocity field on text prompts ($y$) allows for precise control over the generated video content. The model can effectively translate textual descriptions into corresponding video sequences.\n4.  **High-Quality Output**: The iterative refinement process ensures that the generated frames are consistent and of high quality. By gradually denoising the initial noise, the model avoids artifacts and produces visually appealing videos.\n5.  **Efficient Inference**: With techniques like distillation, the number of steps required during inference can be significantly reduced, making the video generation process more efficient without sacrificing quality.\n\n***\n\nIn summary, Step-Video-T2V leverages **Flow Matching** to achieve stable, controlled, and high-quality video generation by learning a continuous transformation from noise to coherent video frames."
    },
    {
        "question_id": "2502.10248v1_7",
        "answer": "The Step-Video-T2V model employs a hierarchical data filtering approach to curate high-quality video data for pre-training and post-training. This process involves progressively applying filters with increasing thresholds to create subsets of the data.\n\n***\n\n### Key Aspects of the Hierarchical Data Filtering:\n\n1.  **Pre-training Data Filtering (Step-2: T2VI Pre-training):**\n    *   A series of filters are applied to the data with progressively increasing thresholds.\n    *   This results in six pre-training subsets.\n2.  **Post-training Data Filtering (SFT):**\n    *   The final SFT dataset is constructed through manual filtering.\n\n***\n\n### Filtering Techniques and Metrics Used:\n\n1.  **Video Assessment Scores:**\n    *   Using scores and heuristic rules, the dataset is filtered to a subset of 30M videos.\n    *   This significantly improves the overall quality.\n2.  **Video Categories:**\n    *   For videos within the same cluster, \"Distance to Centroid\" values are used to remove outliers.\n    *   This ensures the video subset contains a sufficient number of videos for each cluster while maintaining diversity.\n3.  **Human Evaluation:**\n    *   Human evaluators assess each video for clarity, aesthetics, appropriate motion, and smooth scene transitions.\n    *   They also check for the absence of watermarks or subtitles.\n    *   Captions are manually refined to ensure accuracy and include essential details such as camera movements, subjects, actions, backgrounds, and lighting.\n4.  **Quality Assessment Tags:**\n    *   **Aesthetic Score:** Predicted using the public LAION CLIP-based aesthetic predictor.\n    *   **NSFW Score:** Detected using the public LAION CLIP-based NSFW detector.\n    *   **Watermark Detection:** Detected using an EfficientNet image classification model.\n    *   **Subtitle Detection:** Recognized and localized using PaddleOCR.\n    *   **Saturation Score:** Assessed by converting video frames from BGR to HSV color space and extracting the saturation channel.\n    *   **Blur Score:** Detected using the variance of the Laplacian method.\n    *   **Black Border Detection:** Detected using FFmpeg to identify black borders in frames.\n5.  **Motion Scores:**\n    *   **Motion\\_Mean:** The average motion magnitude across all frames.\n    *   **Motion\\_Max:** The maximum motion magnitude observed in the clip.\n    *   **Motion\\_Min:** The minimum motion magnitude in the clip.\n6.  **Video-Text Alignment:**\n    *   **CLIP Score:** Computed by averaging the cosine similarities between image embeddings and caption embeddings.\n    *   This score assesses how well the captions align with the visual content of the video clips.\n7.  **Cluster Analysis:**\n    *   **Cluster\\_Cnt:** The total number of clips in the cluster to which the clip belongs.\n    *   **Center\\_Sim:** The cosine distance between the clip\u2019s embedding and the cluster center.\n\n***\n\n### Importance for Training High-Quality Video Generation Models:\n\n1.  **Improved Data Quality:**\n    *   Filtering ensures that only high-quality videos are used for training, which leads to better model performance.\n2.  **Reduced Artifacts and Enhanced Visual Quality:**\n    *   By removing videos with watermarks, subtitles, and other visual artifacts, the model can focus on learning the underlying content.\n3.  **Balanced Dataset:**\n    *   Addressing category imbalances and deduplication ensures that the model is trained on a diverse set of concepts.\n4.  **Accurate Video-Text Alignment:**\n    *   Ensuring accurate alignment between video content and textual descriptions is essential for generating high-quality output and effective data filtering.\n5.  **Enhanced Motion Dynamics:**\n    *   Assessing and filtering videos based on motion scores ensures that the model learns to generate videos with appropriate motion.\n6.  **Emulating Human Cognitive Patterns:**\n    *   Adjusting the training data to reflect what is considered higher quality by humans results in a significant reduction in training loss.\n7.  **Stable Training and Style Consistency:**\n    *   Using high-quality videos with accurate captions and desired styles in SFT is crucial to the stability of the model and the style of the generated videos."
    },
    {
        "question_id": "2502.10248v1_8",
        "answer": "Okay, let's break down how Step-Video-T2V stacks up against commercial models such as Sora and Veo, based on the information provided in the technical report.\n\n***\n\n### Overall Performance and Capabilities\n\nStep-Video-T2V is presented as a state-of-the-art text-to-video model with 30B parameters, capable of generating high-quality videos from text prompts. The report emphasizes its strong motion dynamics, high aesthetics, and consistent content. Here's a comparison with commercial engines like Sora and Veo:\n\n*   **General Text Prompt Performance**: Step-Video-T2V delivers comparable performance for general text prompts.\n*   **Specific Domain Superiority**: Step-Video-T2V surpasses commercial engines in specific domains, such as generating videos with high motion dynamics.\n*   **Key Features**:\n    *   **High Compression VAE**: Step-Video-T2V incorporates a high-compression **Video-VAE**, achieving 16x16 spatial and 8x temporal compression ratios, significantly reducing computational complexity.\n    *   **Bilingual Text Prompts**: It supports bilingual text prompts in both English and Chinese.\n    *   **Video-based DPO**: It implements a video-based **Direct Preference Optimization (DPO)** approach to reduce artifacts and enhance visual quality.\n\n***\n\n### Benchmarking and Metrics\n\nThe paper introduces a new benchmark dataset called **Step-Video-T2V-Eval** for text-to-video generation. This benchmark includes 128 diverse prompts across 11 categories. Two human evaluation metrics are proposed:\n\n*   **Metric-1**: Compares Step-Video-T2V with a target model by having human annotators assign a Win/Tie/Loss label to each generated video pair.\n*   **Metric-2**: Assigns four scores to each generated video to measure its quality across dimensions like instruction following, motion smoothness, physical plausibility, and aesthetic appeal.\n\nThese metrics are used to evaluate the performance of Step-Video-T2V against both open-source and commercial engines.\n\n***\n\n### Key Advantages of Step-Video-T2V\n\n1.  **Strong Motion Dynamics**: Step-Video-T2V demonstrates strong capabilities in modeling and generating videos with high-motion dynamics.\n2.  **High Compression Rate**: The **Video-VAE** achieves high compression ratios, enabling the generation of longer videos (up to 204 frames).\n\n***\n\n### Challenges and Limitations\n\n1.  **Instruction Following**: The model struggles with complex action sequences and generating videos that incorporate multiple concepts with low occurrence in the training data.\n2.  **Physical Plausibility**: The model faces difficulties in generating videos that adhere to the laws of physics.\n3.  **Data Quality**: High-quality labeled data remains a significant hurdle. Existing video captioning models often struggle with hallucination issues, and human annotations are expensive and difficult to scale.\n\n***\n\n### Comparisons with Other Models\n\nThe report also includes comparisons with other models:\n\n*   **HunyuanVideo**: Step-Video-T2V demonstrates state-of-the-art performance as the strongest open-source text-to-video generation model to date.\n*   **Movie Gen Video**: Step-Video-T2V achieves comparable performance, but Movie Gen Video benefits from more extensive pre-training and high-quality SFT data.\n\n***\n\n### Overall\n\nStep-Video-T2V stands out due to its specific strengths in motion dynamics and technical innovations like the high-compression **Video-VAE** and **video-based DPO**. While it faces challenges in instruction following and physical plausibility, it remains competitive with commercial models and surpasses open-source alternatives."
    },
    {
        "question_id": "2502.10248v1_9",
        "answer": "Human feedback is integrated into the Step-Video-T2V training pipeline to enhance the visual quality of generated videos and ensure better alignment with user prompts. This is primarily achieved through a process that leverages human preferences to guide the model towards generating more desirable outputs.\n\n***\n\n### Incorporation of Human Feedback\n\n1.  **Data Collection**: Human annotators are involved in creating a diverse set of prompts, some of which are synthesized based on real-world user interaction patterns. The model generates multiple videos for each prompt using different seeds, and human annotators then rate the preference of these samples.\n2.  **Direct Preference Optimization (DPO)**: Step-Video-T2V employs DPO to incorporate human feedback. DPO aims to adjust the model to favor the generation of preferred data while avoiding non-preferred data. A reference policy is introduced to stabilize training and prevent the current policy from deviating too far.\n3.  **Training Objective**: The training objective is based on the DiffusionDPO method, with modifications to extend it to the Flow Matching framework. The model is trained to minimize the difference between preferred and non-preferred samples, guided by human annotations.\n\n    $L_{DPO} = -E_{(y, x_w, x_l) \\sim D} \\left[ \\log \\sigma \\left( \beta \\left[ \\log \frac{\\pi_\theta(x_w|y)}{\\pi_{ref}(x_w|y)} - \\log \frac{\\pi_\theta(x_l|y)}{\\pi_{ref}(x_l|y)} \right] \right) \right]$\n\n    Where:\n\n    *   $\\pi_\theta$ and $\\pi_{ref}$ refer to the current policy and reference policy, respectively.\n    *   $x_w$ and $x_l$ are the preferred and non-preferred samples.\n    *   $y$ denotes the condition (prompt).\n4.  **Reward Model Training**: To address the issue of improvements saturating when the model can easily distinguish between positive and negative samples, a reward model is trained using human-annotated feedback data. This reward model dynamically evaluates the quality of newly generated samples during training and is periodically fine-tuned to maintain alignment with the evolving policy.\n\n***"
    }
]