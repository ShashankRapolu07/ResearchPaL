[
    {
        "question_id": "2502.10248v1_0",
        "answer": "The paper defines two levels of video foundation models, each representing a different stage of development and capability:\n\n***\n\n### Level-1: Translational Video Foundation Model\n\n*   This level focuses on cross-modal translation.\n*   These models generate videos from text, visual, or multimodal contexts, acting as a conversion system between different modalities.\n*   Current diffusion-based text-to-video models like **Sora**, **Veo**, **Kling**, **Hailuo**, and **Step-Video-T2V** belong to this level.\n*   They excel at producing high-quality videos from text prompts, lowering the barrier for video content creation.\n*   These models primarily learn mappings between input prompts and corresponding videos.\n\n***\n\n### Level-2: Predictable Video Foundation Model\n\n*   This level aims at prediction capabilities, similar to large language models (**LLMs**).\n*   These models forecast future events based on text, visual, or multimodal contexts.\n*   They handle more advanced tasks, such as reasoning with multimodal data or simulating real-world scenarios.\n*   These models explicitly model the underlying causal relationships within videos.\n\n***\n\n### Key Differences\n\n| Feature                | Level-1: Translational                                    | Level-2: Predictable                                                                                                                                                                                             |\n| :--------------------- | :-------------------------------------------------------- | :--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| **Primary Function**   | Cross-modal translation                                   | Prediction and reasoning                                                                                                                                                                                         |\n| **Core Capability**    | Generating videos from given prompts                       | Forecasting future events and simulating scenarios                                                                                                                                                               |\n| **Causal Modeling**    | Limited or absent                                         | Explicitly models causal relationships                                                                                                                                                                             |\n| **Task Complexity**    | Simpler mappings between inputs and outputs               | More complex tasks involving reasoning, simulation, and understanding of physical laws                                                                                                                            |\n| **Example Models**     | **Sora**, **Veo**, **Kling**, **Hailuo**, **Step-Video-T2V** | (Hypothetical) Future models capable of advanced reasoning and prediction                                                                                                                                      |\n| **Current Limitations** | Struggle with complex action sequences and physical laws | (Future challenges) Achieving accurate predictions and simulations, handling uncertainty, and scaling to real-world complexity                                                                                  |\n| **Learning Focus**     | Mapping between prompts and videos                        | Modeling causal relationships and understanding the dynamics of video content                                                                                                                                  |\n| **Analogy**            | Translation system                                        | Prediction system, similar to **LLMs**                                                                                                                                                                         |\n| **Key tasks**          | Text/image/video-to-video generation                       | Video understanding and editing, as well as video-based conversion, question answering, and task completion.                                                                                                   |\n| **Level of understanding** | Only learn the mappings between text prompts and corresponding videos | Performing causal or logical tasks like **LLMs**                                                                                                                                                                |"
    },
    {
        "question_id": "2502.10248v1_1",
        "answer": "Step-Video-T2V uses a specialized **Video-VAE** architecture to achieve high spatial and temporal compression while maintaining video reconstruction quality. Here's a breakdown of how it works:\n\n***\n\n### Key Components and Techniques\n\n1.  **High Compression Ratios**: The **Video-VAE** achieves a **16x16 spatial** and **8x temporal compression ratio**. This significant reduction in dimensionality helps reduce the computational load for subsequent processing.\n\n2.  **Dual-Path Architecture**:\n    *   The architecture incorporates a novel dual-path design in the later stages of the encoder and the early stages of the decoder.\n    *   This design facilitates unified spatial-temporal compression.\n\n3.  **Causal 3D Convolutional Modules**:\n    *   The encoder's early stages consist of three stages, each featuring two **Causal Res3DBlock** modules and corresponding downsample layers.\n    *   A **MidBlock** combines convolutional layers with attention mechanisms to refine the compressed representations further.\n    *   **Temporal causality** is implemented using 3D convolutions, ensuring each frame only depends on previous frames:\n\n        $C3D(X)_t = \begin{cases}\n        Conv3D([0, ..., X_t], \\Theta) & t = 0 \\\n        Conv3D([X_{t-k}, ..., X_t], \\Theta) & t > 0\n        \\end{cases}$\n\n        where $k$ is the temporal kernel size.\n\n4.  **Dual-Path Latent Fusion**:\n    *   This technique maintains high-frequency details through convolutional processing and preserves low-frequency structure via channel averaging.\n    *   It uses a unified structure to handle both image and video data, improving parameter efficiency and reducing blurring artifacts.\n    *   The dual paths consist of:\n        1.  **Conv Path**: Combines causal 3D convolutions with pixel unshuffling:\n\n            $H_{conv} = U^{(3)}_s(C3D(X))$\n\n            where $U^{(3)}_s$ is a 3D pixel unshuffle operation with spatial stride $s_s = 2$ and temporal stride $s_t = 2$.\n        2.  **Shortcut Path**: Preserves structural semantics through grouped channel averaging:\n\n            $H_{avg} = \frac{1}{s^3} \\sum_{k=0}^{s^3 - 1} U^{(3)}_s (X)[..., kC_z:(k+1)C_z]$\n\n            where $C_z$ is the latent dimension of the next stage.\n    *   The output of fusion combines both paths through residual summation:\n\n        $Z = H_{conv} \\oplus H_{avg}$\n\n5.  **Decoder Architecture**:\n    *   The decoder's early stages consist of two symmetric Dual Path architectures.\n    *   3D pixel unshuffle operations are replaced by 3D pixel shuffle operators, and grouped channel averaging is replaced by a grouped channel repeating operation.\n    *   **Spatial groupnorm** is used in the ResNet backbone to prevent temporal flickering between different chunks.\n\n***\n\n### Training Details\n\n1.  **Multi-Stage Training**: The **VAE** training is meticulously designed in multiple stages:\n    *   **Stage 1**: Training a **VAE** with a **4x8x8 compression ratio** without a dual-path structure, jointly on images and videos.\n    *   **Stage 2**: Incorporating two dual-path modules in both the encoder and decoder, gradually unfreezing the dual-path modules, the **MidBlock**, and the ResNet backbone.\n\n2.  **Loss Functions**: A combination of loss functions is used:\n    *   **L1 reconstruction loss**\n    *   **Video-LPIPS**\n    *   **KL-divergence constrain**\n    *   **GAN loss** (introduced after the above losses have converged)\n\n***\n\n### Performance Metrics\n\n1.  **Reconstruction Quality**: The **Video-VAE** maintains state-of-the-art reconstruction quality despite having a larger compression ratio compared to other models.\n2.  **Evaluation Metrics**: Performance is evaluated using metrics like **SSIM**, **PSNR**, and **rFVD**.\n\n    | Model                           | Downsample Factor | **SSIM**\u2191 | **PSNR**\u2191 | **rFVD**\u2193 |\n    | :------------------------------ | :---------------- | :-------- | :-------- | :-------- |\n    | OpenSora-1.2                   | 4 \u00d7 8 \u00d7 8         | 0.9126    | 31.41     | 20.42     |\n    | CogvideoX-1.5                  | 4 \u00d7 8 \u00d7 8         | 0.9373    | 38.10     | 16.33     |\n    | HunyuanVideo                   | 4 \u00d7 8 \u00d7 8         | 0.9710    | 39.56     | 4.17      |\n    | Cosmos-VAE (4 \u00d7 8 \u00d7 8)         | 4 \u00d7 8 \u00d7 8         | 0.9315    | 37.66     | 9.10      |\n    | Cosmos-VAE (8 \u00d7 16 \u00d7 16)       | 8 \u00d7 16 \u00d7 16       | 0.8862    | 34.82     | 40.33     |\n    | **Video-VAE (Ours)**          | 8 \u00d7 16 \u00d7 16       | **0.9776**| **39.37** | **3.61**  |\n\n    *   The **Video-VAE** achieves **SSIM** of 0.9776, **PSNR** of 39.37, and **rFVD** of 3.61, outperforming other models even with a higher compression ratio.\n\nBy combining these architectural innovations and training strategies, Step-Video-T2V's **Video-VAE** effectively balances high compression with excellent video reconstruction quality."
    },
    {
        "question_id": "2502.10248v1_2",
        "answer": "Direct Preference Optimization (**DPO**) is a technique used to fine-tune generative models by directly optimizing for human preferences. Instead of using reinforcement learning, which can be unstable and complex, **DPO** reframes the problem as a simple classification task. It adjusts the model to generate outputs that are preferred by humans, while discouraging the generation of less desirable outputs.\n\n***\n\n### How DPO Works in Step-Video-T2V\n\n1.  **Preference Data Collection**:\n    *   A diverse set of prompts is created, including prompts from the training data and those synthesized by human annotators.\n    *   For each prompt, the **Step-Video-T2V** model generates multiple videos using different random seeds.\n    *   Human annotators then rate these videos, indicating which ones are preferred and which are not.\n\n2.  **Training Objective**:\n\n    *   The goal is to train the model to align with preferred data and avoid non-preferred data.\n    *   A reference policy (reference model) is used to prevent the current policy from deviating too far.\n    *   The policy objective is formulated as:\n\n        $L_{DPO} = -E_{(y, x_w, x_l) \\sim D} \\left[ \\log \\sigma \\left( \beta \\left( \\log \frac{\\pi_\theta(x_w|y)}{\\pi_{ref}(x_w|y)} - \\log \frac{\\pi_\theta(x_l|y)}{\\pi_{ref}(x_l|y)} \right) \right) \right]$\n\n        where:\n\n        *   $\\pi_\theta$ is the current policy.\n        *   $\\pi_{ref}$ is the reference policy.\n        *   $x_w$ is the preferred sample.\n        *   $x_l$ is the non-preferred sample.\n        *   $y$ is the condition (prompt).\n\n3.  **Model Update**:\n\n    *   At each training step, a prompt and its corresponding positive and negative sample pairs are selected.\n    *   Samples are generated by the model, ensuring smooth updates and training stability.\n    *   Initial noise and timestep are fixed to align positive and negative samples, further stabilizing the training process.\n\n4.  **Addressing Gradient Issues**:\n\n    *   To avoid gradient explosion, the paper reduces $\beta$ and increases the learning rate for faster convergence.\n\n5.  **Reward Model (Advanced)**:\n\n    *   To address the issue of outdated training data, a reward model is trained using human-annotated feedback.\n    *   This model evaluates the quality of newly generated samples during training.\n    *   The reward model is periodically fine-tuned to maintain alignment with the evolving policy, improving data efficiency.\n\n***\n\n### Benefits of DPO in Step-Video-T2V\n\n*   **Improved Visual Quality**: Human feedback enhances the plausibility and consistency of generated videos.\n*   **Better Prompt Alignment**: Videos generated with **DPO** align more accurately with the given prompts, resulting in more relevant video generation."
    },
    {
        "question_id": "2502.10248v1_3",
        "answer": "The Step-Video-T2V model leverages a Diffusion Transformer (DiT) architecture with 3D full attention, offering several key advantages for video generation:\n\n***\n\n### Model Architecture\n\n*   **DiT as the Foundation:**\n\n    *   The choice of DiT allows for disentangling text and video information, making it naturally extendable to pure video prediction models.\n*   **3D Full Attention Mechanism:**\n\n    *   It captures both spatial and temporal information in a unified attention process, enhancing the generation of videos with high motion dynamics.\n    *   It has a higher performance potential compared to spatial-temporal attention mechanisms, which process spatial and temporal information separately.\n\n***\n\n### Text Prompt Incorporation\n\n*   **Cross-Attention Layer:**\n\n    *   A cross-attention layer is introduced in each transformer block to incorporate text prompts, enabling the model to attend to textual information while processing visual features.\n*   **Bilingual Text Encoders:**\n\n    *   The model uses two distinct bilingual text encoders, Hunyuan-CLIP and Step-LLM, to embed the prompt. The outputs from these encoders are concatenated, creating a combined text embedding sequence that guides video generation.\n\n***\n\n### Positional Encoding\n\n*   **RoPE-3D:**\n\n    *   It handles video data by accounting for temporal (frame) and spatial (height and width) dimensions, allowing the model to process videos with varying frame counts and resolutions.\n    *   It improves generalization across diverse video data and effectively captures both spatial and temporal relationships within the video.\n\n***\n\n### Training and Objective\n\n*   **Flow Matching:**\n\n    *   DiT is trained using Flow Matching, which involves denoising input noise into latent frames, with text embeddings and timesteps serving as conditioning factors.\n*   **Adaptive Layer Normalization (AdaLN):**\n\n    *   The model employs AdaLN with optimized computation to embed timestep information, improving overall model efficiency by reducing computational overhead.\n\n***\n\n### Efficiency and Stability\n\n*   **QK-Norm:**\n\n    *   Query-Key Normalization (QK-Norm) is used to stabilize the self-attention mechanism, addressing numerical instability and ensuring stable attention during training.\n*   **Video-based DPO:**\n\n    *   A video-based Direct Preference Optimization (DPO) approach is applied to enhance visual quality by reducing artifacts and ensuring smoother, more realistic video outputs."
    },
    {
        "question_id": "2502.10248v1_4",
        "answer": "To handle bilingual text prompts, Step-Video-T2V employs two distinct bilingual text encoders: Hunyuan-CLIP and Step-LLM.\n\nHere's a breakdown of how these encoders work together and the advantages of this approach:\n\n*   **Hunyuan-CLIP:** This is a bidirectional text encoder derived from an open-source bilingual CLIP model. It excels at producing text representations that are well-aligned with the visual space due to the CLIP model's training mechanism. However, it has a limitation of a maximum input length of 77 tokens, which poses challenges when processing longer user prompts.\n\n*   **Step-LLM:** This is an in-house, unidirectional bilingual text encoder pre-trained using the next-token prediction task. It incorporates a redesigned Alibi-Positional Embedding to improve both efficiency and accuracy in sequence processing. Unlike Hunyuan-CLIP, Step-LLM has no input length restriction, making it particularly effective for handling lengthy and complex text sequences.\n\nBy combining these two text encoders, Step-Video-T2V can handle user prompts of varying lengths and complexities.\n\nHere are the advantages of using two separate text encoders:\n\n*   **Handling Varying Prompt Lengths:** Hunyuan-CLIP is effective for shorter prompts due to its strong alignment with the visual space, while Step-LLM is better suited for longer, more complex prompts due to its ability to handle longer sequences without input length restrictions.\n\n*   **Robust Text Representations:** By processing text prompts through both encoders, the model can generate more robust text representations that effectively guide the model in the latent space. This is because the encoders capture different aspects of the text, with Hunyuan-CLIP focusing on visual alignment and Step-LLM focusing on sequence understanding.\n\n*   **Improved Performance:** The combination of these two encoders results in improved performance in text-to-video generation, as the model can leverage the strengths of each encoder to better understand and interpret user prompts.\n\nIn summary, Step-Video-T2V leverages the strengths of two different bilingual text encoders to handle a wider range of text prompts and generate more robust text representations, which ultimately leads to improved video generation quality."
    },
    {
        "question_id": "2502.10248v1_5",
        "answer": "Here's a breakdown of the main challenges facing current diffusion-based text-to-video models, and how Step-Video-T2V attempts to address them:\n\n***\n\n### Challenges of Current Diffusion-Based Text-to-Video Models\n\n1.  **High-Quality Labeled Data:**\n\n    *   **Challenge:** Existing video captioning models often struggle with hallucination issues, and human annotations are expensive and difficult to scale.\n    *   **Why it Matters:** High-quality, accurate captions are crucial for training models that can faithfully translate text descriptions into video content.\n2.  **Instruction Following:**\n\n    *   **Challenge:** It's difficult for models to generate videos based on detailed descriptions, handle complex action sequences, and combine multiple concepts effectively, especially when those concepts have a low occurrence in the training data.\n    *   **Why it Matters:** Instruction following is key to creating videos that accurately reflect the user's intent and vision.\n3.  **Adherence to the Laws of Physics:**\n\n    *   **Challenge:** Current models often struggle to simulate the real world accurately, leading to videos that don't adhere to basic physical laws (e.g., a ball bouncing realistically).\n    *   **Why it Matters:** Realistic physics are essential for creating believable and immersive video content.\n\n***\n\n### How Step-Video-T2V Attempts to Address These Challenges\n\n1.  **High-Quality Labeled Data:**\n\n    *   **Step-Video-T2V's Approach:** The paper emphasizes the importance of high-quality, small-scale, and diverse datasets for post-training. By applying a small amount of high-quality human-labeled data in SFT (Supervised Fine-Tuning), Step-Video-T2V achieves significant improvements in overall video quality.\n    *   **Why it Helps:** This suggests that the quality and diversity of the data are more important than sheer scale. The model generalizes well across a broader range of prompts when trained on curated datasets with diverse video styles and motion dynamics. The paper also mentions plans to build a comprehensive video knowledge base with structured labels.\n2.  **Instruction Following:**\n\n    *   **Step-Video-T2V's Approach:** The paper acknowledges that even a 30B parameter model like Step-Video-T2V struggles with complex action sequences and combining multiple low-occurrence concepts. One approach to improve this is to ensure that all elements in the prompt receive appropriate attention.\n    *   **Why it Helps:** By heuristically repeating missing objects in the prompt, some problematic cases can be significantly improved. The paper suggests balancing attention across all elements in the prompt to better follow instructions.\n3.  **Adherence to the Laws of Physics:**\n\n    *   **Step-Video-T2V's Approach:** The paper identifies this as a key limitation of diffusion-based models. To address this, the authors plan to develop more advanced model paradigms in future work, such as combining autoregressive and diffusion models within a unified framework.\n    *   **Why it Helps:** This combined approach aims to better adhere to the laws of physics and more accurately simulate realistic interactions.\n\n***\n\n### Additional Components of Step-Video-T2V\n\n*   **Model Architecture:** Step-Video-T2V uses a diffusion Transformer (DiT)-based model trained using Flow Matching. A deep compression Variational Auto-encoder (VAE) achieves 16x16 spatial and 8x temporal compression ratios, reducing the computational complexity of large-scale video generation training.\n*   **Training Pipeline:** A cascaded training pipeline, including text-to-image pre-training, text-to-video pre-training, supervised fine-tuning (SFT), and direct preference optimization (DPO), accelerates model convergence and fully leverages video datasets of varying quality.\n*   **Bilingual Text Encoders:** Two bilingual text encoders enable Step-Video-T2V to directly understand Chinese or English prompts.\n*   **Video-Based DPO:** A video-based DPO (Direct Preference Optimization) approach is applied to reduce artifacts and improve the visual quality of the generated videos.\n*   **Step-Video-T2V-Eval Benchmark:** A new benchmark dataset is created for text-to-video generation, which includes 128 diverse prompts across 11 categories, alongside video generation results from several top text-to-video open-source and commercial engines for comparison."
    },
    {
        "question_id": "2502.10248v1_6",
        "answer": "Step-Video-T2V employs Flow Matching as its training objective, which offers a way to train generative models by learning to match a continuous vector field that transports a simple noise distribution to the data distribution. Here's a breakdown of how it's used and why it's beneficial:\n\n### Flow Matching in Step-Video-T2V\n\n1.  **Noise Sampling**: At each training step, Gaussian noise ($X_0$) is sampled from a standard normal distribution.\n\n2.  **Timestep Sampling**: A random timestep ($t$) is sampled from a uniform distribution between 0 and 1.\n\n3.  **Interpolation**: The model input ($X_t$) is constructed as a linear interpolation between the noise ($X_0$) and the target sample ($X_1$), where $X_1$ represents the noise-free input (i.e., the actual video frame or image). The formula for this interpolation is:\n    $X_t = (1 - t) \\cdot X_0 + t \\cdot X_1$\n\n4.  **Velocity Calculation**: The ground truth velocity ($V_t$) is calculated, representing the rate of change of $X_t$ with respect to the timestep $t$. This is defined as:\n    $V_t = \frac{dX_t}{dt} = X_1 - X_0$\n\n    In essence, $V_t$ captures the direction and magnitude of change needed to transform the initial noise into the target data.\n\n5.  **Model Training**: The model is trained to predict the velocity field $u(X_t, y, t; \theta)$, where $X_t$ is the input at timestep $t$, $y$ is an optional conditioning input (like a text prompt), and $\theta$ represents the model parameters. The training loss is the mean squared error (MSE) between the predicted velocity and the true velocity:\n\n    $loss = E_{t, X_0, X_1, y} [\\|u(X_t, y, t; \theta) - V_t\\|^2]$\n\n    The model learns to predict how to transform the noisy sample $X_t$ at any given timestep $t$ into a less noisy sample, gradually approaching the real data $X_1$.\n\n6.  **Inference**: During inference, the process starts with random noise ($X_0$). The goal is to recover the denoised sample ($X_1$) by iteratively refining the noise through an ODE-based method. A sequence of timesteps $\\{t_0, t_1, ..., t_n\\}$ is defined, where $t_0 = 0$ and $t_n = 1$. The denoising process integrates over these timesteps:\n\n    $X_1 = \\sum_{i=0}^{n-1} u(X_{t_i}, y, t_i; \theta) \\cdot (t_{i+1} - t_i)$\n\n### Benefits of Flow Matching for Video Generation\n\n1.  **Stable Training**: Flow Matching provides a more stable training process compared to other diffusion-based methods. By directly learning the velocity field, the model avoids issues like mode collapse and vanishing gradients, which can be common in traditional GANs or VAEs.\n\n2.  **High-Quality Samples**: The method ensures that the generated samples progressively refine from noise to realistic video frames. The continuous nature of the flow allows for smoother transitions and more coherent video generation.\n\n3.  **Flexibility**: Flow Matching is flexible and can be conditioned on various inputs, such as text prompts. This allows for controllable video generation, where the generated content aligns with the given conditions or descriptions.\n\n4.  **Efficiency**: By learning the underlying data distribution, Flow Matching can generate high-quality videos with fewer steps compared to other iterative refinement methods. This efficiency is crucial for large-scale video generation tasks.\n\n5.  **Reduced Artifacts**: The approach helps in reducing artifacts and inconsistencies in the generated videos, leading to more visually appealing and realistic outputs.\n\nIn summary, Step-Video-T2V uses Flow Matching to train a diffusion model that learns to transform noise into coherent video frames. This approach offers stability, flexibility, and high-quality video generation, making it a powerful tool for creating state-of-the-art video content."
    },
    {
        "question_id": "2502.10248v1_7",
        "answer": "The Step-Video-T2V model employs a multi-stage, hierarchical approach to data filtering, progressively increasing the stringency of filters applied to the data. This process creates several pre-training subsets, ultimately leading to a high-quality dataset refined through manual filtering.\n\nHere's a breakdown of the filtering stages and their significance:\n\n1.  **Initial Data Pool**: The process begins with a large, raw video dataset.\n2.  **Automated Filtering**: A series of automated filters are applied. These filters remove data based on several criteria.\n3.  **Manual Filtering**: The data undergoes manual inspection and refinement by human evaluators.\n\nThe specific filters applied include:\n\n*   **Aesthetic Score**: Uses a CLIP-based aesthetic predictor to assess the visual appeal of video frames.\n*   **NSFW Score**: Employs a CLIP-based NSFW detector to remove inappropriate content.\n*   **Watermark Detection**: Identifies and removes videos containing watermarks.\n*   **Subtitle Detection**: Filters out videos with excessive on-screen text or captions.\n*   **Saturation Score**: Assesses color saturation levels to ensure visual quality.\n*   **Blur Score**: Measures frame sharpness to remove blurry videos.\n*   **Black Border Detection**: Detects and removes frames with black borders.\n\n***\n\nThe importance of this hierarchical data filtering approach lies in several key aspects:\n\n*   **Improved Data Quality**: By systematically filtering out low-quality or inappropriate content, the model is trained on a cleaner, more consistent dataset.\n*   **Enhanced Model Performance**: Training on high-quality data leads to better model convergence, improved generalization, and the generation of more visually appealing and coherent videos.\n*   **Increased Training Efficiency**: Filtering reduces the amount of noisy or irrelevant data, allowing the model to learn more efficiently and effectively.\n*   **Better Control Over Content**: The filtering process allows for greater control over the content the model learns, ensuring that it aligns with desired aesthetic and ethical standards.\n*   **Emulating Human Cognitive Patterns**: The paper notes that adjusting the training data to reflect what is considered higher quality by humans results in a significant reduction in training loss."
    },
    {
        "question_id": "2502.10248v1_8",
        "answer": "Step-Video-T2V is evaluated against commercial video generation engines, revealing a mix of strengths and weaknesses. Here's a breakdown:\n\n### Overall Performance\n\n*   **General Text Prompts**: Step-Video-T2V delivers comparable performance.\n*   **Specific Domains**: It surpasses commercial engines in generating videos with high motion dynamics or text content.\n\n***\n\n### Key Differentiators\n\n*   **Pipeline Complexity**: Commercial engines often involve longer, more complex video generation pipelines with extensive pre- and post-processing. Step-Video-T2V aims for a more streamlined approach while maintaining competitive quality.\n*   **Motion Dynamics**: Step-Video-T2V has achieved the strongest motion dynamics modeling and generation capabilities among all commercial engines.\n*   **Aesthetic Appeal**: Commercial models (T2VTopA, T2VTopB) exhibit higher aesthetic appeal, potentially due to higher video resolutions (720P, 1080P) and high-quality aesthetic data used during post-training.\n\n***\n\n### Benchmarking Results\n\nWhen compared against two commercial text-to-video engines (T2VTopA and T2VTopB) on the **Step-Video-T2V-Eval** benchmark:\n\n*   **Overall Ranking**: T2VTopA > Step-Video-T2V > T2VTopB\n*   **Aesthetic Appeal**: T2VTopA and T2VTopB were rated as having higher aesthetic appeal by most annotators.\n*   **Motion Smoothness & Physical Plausibility**: Step-Video-T2V shows superiority.\n*   **Instruction Following**: T2VTopA demonstrates better instruction-following capability, contributing to its superior performance in categories such as Combined Concepts, Surreal, and Cinematography.\n\n***\n\n### Limitations and Future Improvements\n\n*   **Training Data**: Step-Video-T2V uses significantly less high-quality data in the post-training phase compared to commercial engines, which will be continuously improved in the future.\n*   **Video Length**: The video length is 204 frames, nearly twice the length of T2VTopA and T2VTopB, making training more challenging.\n*   **Pre-training**: The pre-training of Step-Video-T2V remains insufficient.\n*   **Resolution**: Step-Video-T2V generates 540P videos, while commercial models can generate 720P or 1080P videos, which are visually more appealing."
    },
    {
        "question_id": "2502.10248v1_9",
        "answer": "Human feedback is integral to enhancing the visual quality and user alignment of videos generated by Step-Video-T2V. It's incorporated into the training pipeline through a process designed to adjust the model based on human preferences, ensuring the generated content is more appealing and relevant.\n\n***\n\n### Implementation of Human Feedback in Step-Video-T2V\n\n1.  **Data Collection**:\n    *   A diverse set of prompts is created, drawing from training data and synthesized prompts crafted by human annotators to mirror real-world user interactions.\n    *   Step-Video-T2V generates multiple videos for each prompt using different seeds.\n    *   Human annotators evaluate these videos, indicating their preferences. Quality control measures are in place to ensure the accuracy and consistency of annotations.\n    *   This annotation process results in a dataset of preference and non-preference data, forming the basis for model training.\n2.  **Direct Preference Optimization (DPO)**:\n    *   Step-Video-T2V employs **DPO** to incorporate human feedback, aiming to align the model with the generation of preferred data while avoiding non-preferred data.\n    *   A reference policy (reference model) is introduced to stabilize training and prevent the current policy from deviating excessively.\n    *   The policy objective is formulated as:\n\n        $L_{DPO} = -E_{(y,x_w,x_l) \\sim D} \\left[ \\log \\sigma \\left( \beta \\left[ \\log \frac{\\pi_\theta(x_w|y)}{\\pi_{ref}(x_w|y)} - \\log \frac{\\pi_\theta(x_l|y)}{\\pi_{ref}(x_l|y)} \right] \right) \right]$\n\n        where:\n\n        *   $\\pi_\theta$ and $\\pi_{ref}$ refer to the current policy and reference policy, respectively.\n        *   $x_w$ and $x_l$ are the preferred and non-preferred samples, respectively.\n        *   $y$ denotes the condition.\n3.  **Training Process**:\n    *   During each training step, a prompt and its corresponding positive and negative sample pairs are selected.\n    *   Each sample is generated by the model itself, ensuring smooth updates and improving overall training stability.\n    *   The positive and negative samples are aligned by fixing the initial noise and timestep, contributing to a more stable training process.\n    *   The training objective is based on the **DiffusionDPO** method and **DPO**, with modifications to extend it to the Flow Matching framework.\n4.  **Reward Model Training and Integration**:\n    *   A reward model is trained using human-annotated feedback data to dynamically evaluate the quality of newly generated samples during training.\n    *   The reward model is periodically fine-tuned with newly annotated human feedback to maintain alignment with the evolving policy.\n    *   By integrating the reward model into the pipeline, training data is scored and ranked on-the-fly (on-policy), thereby improving data efficiency.\n\n***\n\n### Benefits and Considerations\n\n*   **Visual Quality Improvement**: Human feedback enhances the visual quality of generated videos, ensuring better alignment with user prompts.\n*   **Consistency and Realism**: The **DPO** baseline improves the realism and consistency of generated videos.\n*   **Alignment with Prompts**: **DPO** enhances the alignment with given prompts, resulting in more accurate and relevant video generation.\n*   **Data Efficiency**: Training a reward model using human-annotated feedback data and integrating it into the pipeline improves data efficiency by scoring and ranking training data on-the-fly.\n*   **Potential Issues**:\n    *   The trajectory from initial noise to timestep-specific latents is underutilized due to computational limitations.\n    *   A tradeoff exists between sparse and imprecise feedback, especially in video diffusion models.\n    *   Diffusion models rely on regression, which may result in less efficient preference optimization compared to **LLMs**."
    }
]