[
    {
        "question_id": "2502.10248v1_0",
        "answer": "The paper defines two levels for building video foundation models, each representing a different stage of capability and complexity.\n\n***\n\n### Level 1: Translational Video Foundation Model\n\n*   This level focuses on cross-modal translation.\n*   The model functions as a system that can generate videos from various inputs such as text, visual data, or multimodal contexts.\n*   Current diffusion-based text-to-video models like **Sora**, **Veo**, **Kling**, **Hailuo**, and **Step-Video** fall into this category.\n*   These models excel at generating high-quality videos from text prompts, making video creation more accessible.\n*   However, they often struggle with complex action sequences or adherence to physical laws, as they primarily learn mappings between inputs and videos without explicitly modeling underlying causal relationships.\n\n***\n\n### Level 2: Predictable Video Foundation Model\n\n*   This level involves a prediction system similar to large language models (**LLMs**).\n*   The model can forecast future events based on text, visual, or multimodal contexts.\n*   It handles more advanced tasks such as reasoning with multimodal data or simulating real-world scenarios.\n*   This type of model introduces causal modeling by predicting the next video token, frame, or clip.\n*   It aims to overcome the limitations of Level 1 models by understanding and predicting temporal dependencies and causal relationships within videos."
    },
    {
        "question_id": "2502.10248v1_1",
        "answer": "Step-Video-T2V's Video-VAE achieves high spatial and temporal compression while maintaining video reconstruction quality through a combination of architectural innovations and a multi-stage training process. Here's a breakdown:\n\n### Architecture\n\n*   **Dual-Path Architecture**: The Video-VAE introduces a novel dual-path architecture in the later stages of the encoder and the early stages of the decoder. This design facilitates unified spatial-temporal compression.\n    *   **Conv Path**: This path uses causal 3D convolutions combined with pixel unshuffling to maintain high-frequency details. The equation for this path is:\n\n        $H_{conv} = U^{(3)}_s(C3D(X))$\n\n        Where $U^{(3)}_s$ is the 3D pixel unshuffle operation with spatial stride $s_s = 2$ and temporal stride $s_t = 2$, and $C3D$ denotes causal 3D convolution.\n\n    *   **Shortcut Path**: This path preserves structural semantics through grouped channel averaging. The equation for this path is:\n\n        $H_{avg} = \frac{1}{s^3} \\sum_{k=0}^{s^3-1} U^{(3)}_s (X)[..., kC_z:(k+1)C_z]$\n\n        Where $U^{(3)}_s$ implements 3D pixel unshuffle with spatial-temporal blocking, and $C_z$ is the latent dimension of the next stage.\n\n    *   **Fusion**: The outputs of both paths are combined through residual summation:\n\n        $Z = H_{conv} \bigoplus H_{avg}$\n\n*   **Causal 3D Convolutional Modules**: The encoder's early stages consist of three stages, each featuring two Causal Res3DBlocks and corresponding downsample layers. A MidBlock combines convolutional layers with attention mechanisms for further refinement. The temporal causality is implemented as:\n\n    $C3D(X)_t = \begin{cases} Conv3D([0, ..., X_t], \\Theta) & t = 0 \\ Conv3D([X_{t-k}, ..., X_t], \\Theta) & t > 0 \\end{cases}$\n\n    Where $k$ is the temporal kernel size, ensuring frame $t$ only depends on previous frames.\n\n*   **Pixel Shuffling and Unshuffling**: The architecture uses 3D pixel unshuffle operations in the encoder and 3D pixel shuffle operations in the decoder to efficiently compress and unfold information in spatial-temporal dimensions.\n\n*   **Spatial Groupnorm**: In the ResNet backbone, all groupnorm layers are replaced with spatial groupnorm to avoid temporal flickering between different chunks.\n\n***\n\n### Training Process\n\n*   **Multi-Stage Training**: The VAE training process is divided into multiple stages:\n    1.  **Initial Training**: A VAE with a 4x8x8 compression ratio is trained without a dual-path structure, jointly on images and videos.\n    2.  **Dual-Path Enhancement**: Two dual-path modules are incorporated in both the encoder and decoder, gradually unfreezing the dual-path modules, the mid-block, and the ResNet backbone.\n\n*   **Loss Functions**: The training uses a combination of:\n    *   L1 reconstruction loss\n    *   **Video-LPIPS**\n    *   KL-divergence constraint\n    *   GAN loss (introduced after the initial losses converge)\n\n***\n\n### Key Factors for Maintaining Reconstruction Quality\n\n*   **Dual-Path Latent Fusion**: By maintaining high-frequency details through convolutional processing and preserving low-frequency structure via channel averaging, the dual-path architecture overcomes blurring artifacts typically associated with traditional VAEs.\n\n*   **Temporal Causal 3D Convolution**: This enables joint image and video modeling, ensuring that the model captures temporal dependencies effectively.\n\n*   **Staged Training**: This approach ensures a robust VAE capable of handling complex video data efficiently by gradually increasing the complexity of the model.\n\n*   **Loss Function Combination**: The combination of L1 reconstruction loss, **Video-LPIPS**, KL-divergence constraint, and GAN loss refines the model\u2019s performance, balancing reconstruction accuracy and perceptual quality.\n\nBy implementing these architectural and training strategies, Step-Video-T2V\u2019s Video-VAE achieves a high compression ratio of 16x16 spatially and 8x temporally while maintaining state-of-the-art video reconstruction quality."
    },
    {
        "question_id": "2502.10248v1_2",
        "answer": "Direct Preference Optimization (DPO) is a technique used to align a model's output with human preferences. It's particularly useful in scenarios where defining an explicit reward function is difficult, but human feedback on the quality of generated content is readily available. In the context of Step-Video-T2V, DPO is employed to enhance the visual quality of the videos generated by the model.\n\n***\n\nHere's a breakdown of how DPO works and its impact on Step-Video-T2V:\n\n**1. The Basic Idea**\n\nDPO aims to train a model to generate outputs that humans prefer over alternatives. Instead of using a reward model to guide the training process (as in Reinforcement Learning from Human Feedback - RLHF), DPO directly optimizes the policy (i.e., the video generation model) based on pairwise preference data. This means the model learns by comparing preferred videos to non-preferred videos, adjusting its parameters to favor the characteristics of the preferred ones.\n\n**2. The DPO Process in Step-Video-T2V**\n\n*   **Data Collection:**\n    *   A diverse set of prompts is created, including prompts from the training data and prompts synthesized by human annotators.\n    *   For each prompt, Step-Video-T2V generates multiple videos using different random seeds.\n    *   Human annotators evaluate these videos and indicate their preferences, selecting the preferred and non-preferred samples. Quality control measures are implemented to ensure annotation accuracy and consistency.\n*   **Training Objective:**\n    *   The DPO loss function is designed to increase the likelihood of generating preferred samples while decreasing the likelihood of generating non-preferred samples.\n    *   A reference policy is used to prevent the current policy from deviating too far during training, which helps stabilize the process.\n    *   The policy objective is mathematically formulated as:\n\n    $L_{DPO} = -E_{(y,x_w,x_l) \\sim D} \\left[ log \\sigma \\left( \beta \\left[ log \frac{\\pi_\theta(x_w|y)}{\\pi_{ref}(x_w|y)} - log \frac{\\pi_\theta(x_l|y)}{\\pi_{ref}(x_l|y)} \right] \right) \right]$\n\n    where:\n\n    *   $\\pi_\theta$ is the current policy (the model being trained).\n    *   $\\pi_{ref}$ is the reference policy (a fixed version of the model).\n    *   $x_w$ is the preferred (winning) sample.\n    *   $x_l$ is the non-preferred (losing) sample.\n    *   $y$ is the condition (the text prompt).\n    *   $\beta$ is a hyperparameter controlling the strength of the preference.\n*   **Training Dynamics:**\n    *   During training, the model selects a prompt and its corresponding positive and negative sample pairs.\n    *   Each sample is generated by the model itself, ensuring consistent updates and improved training stability.\n    *   The initial noise and timestep are fixed to align the positive and negative samples, further stabilizing the training process.\n*   **Addressing Gradient Issues:**\n    *   The original DiffusionDPO method used a large $\beta$ value and a very low learning rate to ensure stable training, which led to slow convergence.\n    *   Step-Video-T2V addresses this by reducing $\beta$ and increasing the learning rate, resulting in faster convergence.\n\n**3. How DPO Improves Visual Quality**\n\n*   **Alignment with Human Preferences:** DPO directly optimizes the model to produce videos that are more visually appealing and aligned with what humans find desirable.\n*   **Enhanced Plausibility and Consistency:** Human feedback enhances the plausibility and consistency of the generated videos, making them more realistic and coherent.\n*   **Improved Prompt Alignment:** DPO enhances the alignment with the given prompts, resulting in more accurate and relevant video generation.\n\n***\n\n**4. Limitations and Solutions**\n\n*   **Data Utilization:** The training data used in Video-DPO is generated by earlier versions of the model. As the policy evolves, this data becomes outdated, leading to inefficient data utilization.\n*   **Solution:** A reward model is trained using human-annotated feedback data. This reward model dynamically evaluates the quality of newly generated samples during training and is periodically fine-tuned to maintain alignment with the evolving policy. This on-policy scoring and ranking of training data improves data efficiency.\n\n***\n\n**5. Remaining Issues**\n\n*   The trajectory from initial noise to timestep-specific latents acts as implicit dynamic conditions beyond text prompts but remains underutilized due to computational limitations.\n*   A tradeoff exists between sparse and imprecise feedback, especially in video diffusion models where only a few pixels may be problematic, yet feedback often lacks precision.\n*   Diffusion models rely on regression, which may result in less efficient preference optimization compared to LLMs that use token-level softmax."
    },
    {
        "question_id": "2502.10248v1_3",
        "answer": "The Step-Video-T2V model leverages a Diffusion Transformer (DiT) architecture with 3D full attention, offering several key advantages for video generation:\n\n*   **Superior Modeling of Spatial and Temporal Information:** 3D full attention is theoretically superior for modeling both spatial and temporal information within videos compared to spatial-temporal attention mechanisms. This allows the model to better capture the relationships between objects and their movements across frames.\n*   **Enhanced Motion Dynamics:** The 3D full attention mechanism leads to improved generation of videos with smooth and consistent motion. This is crucial for creating realistic and visually appealing video content.\n*   **High-Quality Video Generation:** By effectively capturing both spatial and temporal information, the model can generate high-quality videos that exhibit strong motion dynamics, high aesthetic appeal, and consistent content.\n*   **Effective Use of Text Prompts:** The DiT architecture incorporates cross-attention layers that allow the model to attend to textual information while processing visual features. This enables the generation of videos conditioned on the input prompt.\n*   **Efficient Training:** The model uses Adaptive Layer Normalization (AdaLN) with optimized computation to reduce the computational overhead of traditional AdaLN operations, improving overall model efficiency.\n*   **Flexible Positional Encoding:** The model employs RoPE-3D, an extension of Rotation-based Positional Encoding (RoPE), to handle video data by accounting for temporal (frame) and spatial (height and width) dimensions. This allows the model to process videos with different frame counts and resolutions without being restricted by fixed positional encoding lengths.\n*   **Stabilized Self-Attention:** The model uses Query-Key Normalization (QK-Norm) to stabilize the self-attention mechanism. QK-Norm normalizes the dot product between the query (Q) and key (K) vectors, addressing numerical instability caused by large dot products that can lead to vanishing gradients or overly concentrated attention."
    },
    {
        "question_id": "2502.10248v1_4",
        "answer": "Step-Video-T2V addresses bilingual text prompts (English and Chinese) using a dual text encoder system. This system leverages two distinct encoders: Hunyuan-CLIP and Step-LLM, each with unique strengths.\n\n*   **Hunyuan-CLIP**: This is a bidirectional encoder, meaning it processes text in both forward and backward directions to understand context from all parts of the sentence. It's designed to align text representations with visual data, making it effective for text-to-image or text-to-video tasks where the text needs to correspond closely to visual elements. However, it has a limitation of a maximum input length of 77 tokens.\n*   **Step-LLM**: This is a unidirectional encoder, processing text sequentially. It is pre-trained to predict the next token in a sequence, making it good at understanding the flow and structure of language. It incorporates a redesigned Alibi-Positional Embedding to improve efficiency and accuracy in sequence processing and has no input length restrictions.\n\n***\n\nThe advantages of using two separate text encoders are:\n\n1.  **Handling Varying Text Lengths**: Hunyuan-CLIP is effective for shorter prompts due to its CLIP training mechanism that aligns text with visual space. Step-LLM is better suited for longer, more complex text sequences because it has no input length restrictions.\n2.  **Robust Text Representations**: By combining the outputs of both encoders, the model can create more robust text representations. The outputs from these two encoders are concatenated along the sequence dimension, creating the final text embedding sequence. This combined embedding is then injected into the cross-attention layer, allowing the model to generate videos conditioned on the input prompt. This allows the model to capture different aspects of the text, such as visual alignment and language structure, more effectively than a single encoder might.\n3.  **Improved Performance**: The dual-encoder approach allows Step-Video-T2V to perform well with a variety of user prompts, enhancing the model's versatility and overall performance in text-to-video generation."
    },
    {
        "question_id": "2502.10248v1_5",
        "answer": "Current diffusion-based text-to-video models, including **Step-Video-T2V**, encounter several significant challenges:\n\n### Key Challenges in Diffusion-Based Text-to-Video Models\n\n1.  **High-Quality Labeled Data**:\n\n    *   **Challenge**: Existing video captioning models often struggle with **hallucination** issues, leading to unstable training. Human annotations are expensive and difficult to scale.\n    *   **Step-Video-T2V's Approach**: Step-Video-T2V leverages high-quality, human-labeled data in **SFT (Supervised Fine-Tuning)** to improve overall video quality. The paper emphasizes that the quality and diversity of data outweigh its scale and that certain characteristics of curated datasets generalize well across prompts.\n2.  **Instruction Following**:\n\n    *   **Challenge**: Current models struggle with complex action sequences and combining multiple concepts with low occurrence in training data. The distribution of **cross-attention scores** can be highly concentrated, leading to missing objects or incorrect details.\n    *   **Step-Video-T2V's Approach**: The report mentions that Step-Video-T2V, despite having 30B parameters, still faces challenges in generating videos involving complex action sequences or multiple low-occurrence concepts. The paper suggests that ensuring all elements in the prompt receive appropriate attention is crucial, and balancing this attention is a task for future work.\n3.  **Adherence to the Laws of Physics**:\n\n    *   **Challenge**: Models often fail to accurately simulate the real world and generate videos that adhere to the laws of physics. Successes are often due to overfitting to specific annotations rather than generalization.\n    *   **Step-Video-T2V's Approach**: The paper acknowledges that all leading text-to-video models, including Step-Video-T2V, struggle with accurately simulating the real world. Future work involves developing more advanced model paradigms, such as combining autoregressive and diffusion models, to better adhere to the laws of physics.\n4.  **Computational Cost**:\n\n    *   **Challenge**: Training and generating long-duration, high-resolution videos pose significant computational hurdles.\n    *   **Step-Video-T2V's Approach**: Step-Video-T2V employs a deep compression **Video-VAE (Variational Autoencoder)**, achieving 16x16 spatial and 8x temporal compression ratios to reduce the computational complexity of large-scale video generation training.\n\n***"
    },
    {
        "question_id": "2502.10248v1_6",
        "answer": "Here's an explanation of how **Step-Video-T2V** employs **Flow Matching** during training and the advantages it brings to video generation:\n\nThe training process involves the following key steps:\n\n1.  **Noise Sampling**: The process starts by sampling random **Gaussian noise** ($X_0$) from a standard normal distribution.\n2.  **Timestep Selection**: A random timestep ($t$) is selected between 0 and 1.\n3.  **Input Construction**: An input ($X_t$) is created through linear interpolation between the initial noise ($X_0$) and the target sample ($X_1$), which corresponds to the noise-free input. The formula for this is:\n\n    $X_t = (1 - t) \\cdot X_0 + t \\cdot X_1$\n4.  **Velocity Calculation**: The ground truth velocity ($V_t$) is calculated, representing the rate of change of $X_t$ with respect to the timestep $t$. This is expressed as:\n\n    $V_t = \frac{dX_t}{dt} = X_1 - X_0$\n\n    The velocity captures the magnitude and direction of change from the initial noise to the target data.\n5.  **Model Training**: The model is trained to predict the velocity at timestep $t$, given the input $X_t$ and any conditioning input $y$ (such as a bilingual sentence). The training objective is to minimize the **Mean Squared Error (MSE)** loss between the predicted velocity and the true velocity. The loss function is:\n\n    $\text{loss} = \\mathbb{E}_{t, X_0, X_1, y} \\left[ \\|u(X_t, y, t; \theta) - V_t\\|^2 \right]$\n\n    Here, $u(X_t, y, t; \theta)$ represents the model\u2019s predicted velocity, and $\theta$ denotes the model parameters.\n\n***\n\nDuring inference, the process is as follows:\n\n1.  **Initial Noise**: Begin by sampling random noise ($X_0$) from a standard normal distribution.\n2.  **Iterative Denoising**: The goal is to recover the denoised sample ($X_1$) by iteratively refining the noise using an **ODE-based method**. A sequence of timesteps ${t_0, t_1, ..., t_n}$ is defined, where $t_0 = 0$, $t_n = 1$, and $t_0 < t_1 < ... < t_n$. The denoising process integrates over these timesteps.\n3.  **Denoising**: The denoised sample ($X_1$) is expressed as:\n\n    $X_1 = \\sum_{i=0}^{n-1} u(X_{t_i}, y, t_i; \theta) \\cdot (t_{i+1} - t_i)$\n\n***\n\n### Benefits of Flow Matching for Video Generation\n\n*   **Stable Training**: Flow matching provides a more stable and efficient training process compared to other diffusion-based methods. By directly modeling the velocity field between noise and data, it avoids the complexities of estimating the score function, leading to faster convergence.\n*   **High-Quality Samples**: Flow matching enables the generation of high-quality video samples with detailed motion dynamics and coherent content. The direct velocity prediction ensures that the model learns smooth transitions between frames, resulting in visually appealing videos.\n*   **Flexibility**: The approach is flexible and can be adapted to various video generation tasks, including text-to-video generation, by conditioning the model on different types of input data.\n*   **Reduced Artifacts**: By using a video-based **DPO (Direct Preference Optimization)** approach, **Step-Video-T2V** further reduces artifacts and enhances the visual quality of the generated videos, ensuring smoother and more realistic video outputs."
    },
    {
        "question_id": "2502.10248v1_7",
        "answer": "The Step-Video-T2V model employs a hierarchical data filtering approach to refine its training data, which is crucial for achieving high-quality video generation. This process involves progressively applying filters with increasing thresholds to create several pre-training subsets. Manual filtering is then used to construct the final **SFT (Supervised Fine-Tuning) dataset**.\n\nThe filtering process is structured to remove noisy or low-quality data, ensuring that the model is trained on the most relevant and clean data. The key filters applied at each stage remove data, and the remaining data is then used for subsequent training stages.\n\nThe specific filters used in this hierarchical process include:\n\n1.  **Aesthetic Score**: Using a **CLIP-based aesthetic predictor** to assess the visual appeal of video frames.\n2.  **NSFW Score**: Employing a **CLIP-based NSFW detector** to remove inappropriate content.\n3.  **Watermark Detection**: Identifying and removing videos with watermarks using an **EfficientNet image classification model**.\n4.  **Subtitle Detection**: Recognizing and filtering out videos with excessive on-screen text using **PaddleOCR**.\n5.  **Saturation Score**: Assessing color saturation by converting video frames to **HSV color space** and analyzing the saturation channel.\n6.  **Blur Score**: Detecting blurriness by applying the **variance of the Laplacian method** to measure frame sharpness.\n7.  **Black Border Detection**: Using **FFmpeg** to detect and remove black borders in frames.\n\n***\n\nThe importance of this hierarchical data filtering lies in several key aspects:\n\n*   **Improved Data Quality**: By systematically filtering out low-quality and irrelevant data, the model is trained on a cleaner dataset, leading to better video generation quality.\n*   **Efficient Learning**: High-quality data allows the model to learn more efficiently, reducing the computational resources and time required for training.\n*   **Enhanced Generalization**: Training on a refined dataset helps the model generalize better to unseen data, improving its ability to generate diverse and realistic videos.\n*   **Better Alignment with Human Perception**: Adjusting the training data to reflect human preferences (e.g., aesthetic scores) results in a significant reduction in training loss and improved video quality.\n\nIn summary, the hierarchical data filtering approach ensures that the Step-Video-T2V model is trained on a high-quality dataset, leading to improved video generation performance and better alignment with human aesthetic preferences."
    },
    {
        "question_id": "2502.10248v1_8",
        "answer": "Okay, I will focus on comparing Step-Video-T2V with commercial video generation models, without directly citing the authors' statements, and I'll highlight key terms in bold.\n\n***\n\n### Step-Video-T2V vs. Commercial Models (e.g., Sora, Veo)\n\nHere's a breakdown of how Step-Video-T2V stacks up against commercial video generation models:\n\n**1. General Performance:**\n\n*   Step-Video-T2V delivers comparable performance to commercial engines for general text prompts.\n*   It even surpasses them in specific domains.\n*   These include generating videos with high motion dynamics or text content.\n\n**2. Pipeline Complexity:**\n\n*   Commercial video generation engines often involve longer and more complex video generation pipelines.\n*   These pipelines include extensive pre- and post-processing steps.\n*   Step-Video-T2V offers a more streamlined approach.\n\n**3. Key Advantages of Step-Video-T2V:**\n\n*   Largest open-source model to date.\n*   Utilizes a high-compression **VAE** for videos.\n*   Supports bilingual text prompts in both English and Chinese.\n*   Implements a video-based **DPO** approach to further reduce artifacts and enhance visual quality.\n*   Provides comprehensive training and inference documentation.\n\n**4. Limitations of Step-Video-T2V:**\n\n*   Lacks sufficient training in the final stage of pre-training with 540P videos.\n*   Uses significantly less high-quality data in the post-training phase compared to commercial engines.\n*   Generates videos at 540P resolution, whereas some commercial models can generate higher resolution videos (e.g., 720P).\n\n**5. Motion Dynamics:**\n\n*   Step-Video-T2V has achieved strong motion dynamics modeling and generation capabilities.\n*   The paper asserts it has the strongest motion dynamics capabilities among all commercial engines.\n\n**6. Areas for Improvement:**\n\n*   Given comparable training resources and high-quality data, it is believed that Step-Video-T2V can achieve state-of-the-art results in general domains as well.\n\n***"
    },
    {
        "question_id": "2502.10248v1_9",
        "answer": "Human feedback plays a crucial role in refining the video generation quality of Step-Video-T2V by aligning the model's output with user preferences and improving visual appeal. This is achieved through a carefully designed training pipeline that incorporates human input to fine-tune the model's behavior.\n\n***\n\nHere\u2019s a breakdown of how human feedback is implemented:\n\n### Human Feedback Integration Pipeline\n\n1.  **Prompt Set Construction:**\n    *   A diverse set of prompts is created, drawing from the training data and synthesized by human annotators to mirror real-world user interactions.\n2.  **Video Generation:**\n    *   For each prompt, Step-Video-T2V generates multiple videos using different random seeds.\n3.  **Preference Annotation:**\n    *   Human annotators evaluate the generated videos and rate their preferences, indicating which videos are more aligned with the prompt and of higher quality.\n4.  **Data Collection:**\n    *   The annotation process results in a collection of preference data, consisting of preferred and non-preferred video samples for each prompt.\n5.  **Model Training:**\n    *   The collected preference data is then used to train the model, adjusting its parameters to favor the generation of preferred videos while avoiding non-preferred ones.\n\n***\n\n### Direct Preference Optimization (DPO)\n\nStep-Video-T2V employs **Direct Preference Optimization (DPO)** to incorporate human feedback. DPO aims to adjust the model (current policy) to generate preferred data while avoiding non-preferred data, given human preference data under the same conditions.\n\nThe **policy objective** can be formulated as:\n\n$L_{DPO} = -E_{(y, x_w, x_l) \\sim D} \\left[ log \\ \\sigma \\left( \beta \\left( log \\ \frac{\\pi_\theta (x_w|y)}{\\pi_{ref}(x_w|y)} - log \\ \frac{\\pi_\theta (x_l|y)}{\\pi_{ref}(x_l|y)} \right) \right) \right]$\n\nWhere:\n\n*   $\\pi_\theta$ is the current policy.\n*   $\\pi_{ref}$ is the reference policy.\n*   $x_w$ is the preferred sample.\n*   $x_l$ is the non-preferred sample.\n*   $y$ denotes the condition (prompt).\n\nTo stabilize training and prevent the current policy from deviating too far, a reference policy (reference model) is introduced.\n\n***\n\n### Training Process\n\n1.  **Sample Selection:**\n    *   At each training step, a prompt and its corresponding positive and negative sample pairs are selected.\n2.  **Data Consistency:**\n    *   Each sample is generated by the model itself to ensure smooth updates and improve training stability.\n    *   Positive and negative samples are aligned by fixing the initial noise and timestep to maintain consistency in the training data.\n3.  **Objective**:\n    *   The training objective is based on the DiffusionDPO method and DPO with slight modifications, extending its application to the video generation task.\n\n***\n\nBy integrating human feedback through DPO, Step-Video-T2V enhances the visual quality of generated videos and ensures better alignment with user prompts, leading to more realistic and aesthetically pleasing outputs."
    }
]