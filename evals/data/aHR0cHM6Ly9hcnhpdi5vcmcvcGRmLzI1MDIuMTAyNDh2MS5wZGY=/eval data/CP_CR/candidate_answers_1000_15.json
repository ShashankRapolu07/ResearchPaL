[
    {
        "question_id": "2502.10248v1_0",
        "answer": "The paper defines two levels of video foundation models, each with distinct capabilities:\n\n### Level-1: Translational Video Foundation Model\n\n*   This type of model functions as a cross-modal translation system.\n*   It can generate videos from text, visual, or multimodal contexts.\n*   Current diffusion-based text-to-video models (like Sora, Veo, Kling, Hailuo, and Step-Video) fall into this category.\n*   These models excel at generating high-quality videos from text prompts, making video content creation more accessible.\n*   However, they struggle with complex action sequences or adherence to the laws of physics.\n*   They primarily learn mappings between prompts and videos without explicitly modeling causal relationships within videos.\n\n***\n\n### Level-2: Predictable Video Foundation Model\n\n*   This model acts as a prediction system, similar to large language models (LLMs).\n*   It can forecast future events based on text, visual, or multimodal context.\n*   It can handle advanced tasks like reasoning with multimodal data or simulating real-world scenarios.\n*   These models introduce causal modeling by predicting the next video token, frame, or clip.\n*   However, they currently do not match the performance of diffusion-based models in text-to-video generation.\n\n***\n\n### Key Differences\n\nThe primary distinction lies in their capabilities beyond simple translation:\n\n*   **Level-1** models are primarily focused on generating videos from given inputs, acting as cross-modal translators.\n*   **Level-2** models aim to understand and predict video content, enabling more complex reasoning and simulation tasks."
    },
    {
        "question_id": "2502.10248v1_1",
        "answer": "Step-Video-T2V's Video-VAE achieves high spatial and temporal compression while maintaining video reconstruction quality through a combination of architectural innovations and a multi-stage training process. Here's a breakdown:\n\n### Architectural Innovations\n\n*   **Dual-Path Architecture:** The Video-VAE uses a novel dual-path architecture in the later stages of the encoder and the early stages of the decoder. This design facilitates unified spatial-temporal compression, achieving 8x16x16 downscaling.\n    *   **Convolutional Path:** Combines causal 3D convolutions with pixel unshuffling to maintain high-frequency details. The equation for this path is:\n\n        $H_{conv} = U^{(3)}_s(C3D(X))$\n\n        where $U^{(3)}_s$ is the 3D pixel unshuffle operation, $C3D$ denotes causal 3D convolution, and $X$ is the input video tensor.\n    *   **Shortcut Path:** Preserves structural semantics through grouped channel averaging. The equation for this path is:\n\n        $H_{avg} = \frac{1}{s^3} \\sum_{k=0}^{s^3-1} U^{(3)}_s (X)[..., kC_z:(k+1)C_z]$\n\n        where $U^{(3)}_s$ implements 3D pixel unshuffle with spatial-temporal blocking, and $C_z$ is the latent dimension of the next stage.\n    *   **Fusion:** The output of the two paths are combined through residual summation:\n\n        $Z = H_{conv} \\oplus H_{avg}$\n\n        where $Z$ is the latent representation.\n\n*   **Causal 3D Convolutional Modules:** The encoder uses causal Res3DBlock modules and downsample layers. A MidBlock combines convolutional layers with attention mechanisms to refine the compressed representations further. Temporal causality is implemented using the equation:\n\n    $C3D(X)_t = \begin{cases} Conv3D([0, ..., X_t], \\Theta) & t = 0 \\ Conv3D([X_{t-k}, ..., X_t], \\Theta) & t > 0 \\end{cases}$\n\n    where $k$ is the temporal kernel size.\n\n### Multi-Stage Training Process\n\n*   **Initial VAE Training:** A VAE with a 4x8x8 compression ratio is trained without a dual-path structure. This initial training is conducted jointly on images and videos to learn low-level representations.\n*   **Dual-Path Module Enhancement:** Two dual-path modules are incorporated into both the encoder and decoder. These modules are gradually unfrozen during training to allow for a refined and flexible training process.\n*   **Loss Functions:** A combination of **L1 reconstruction loss**, **Video-LPIPS**, and **KL-divergence** constrain is used to guide the model. **GAN loss** is introduced to further refine the model\u2019s performance after the initial losses have converged.\n*   **Spatial Groupnorm:** In the decoder's ResNet backbone, groupnorm is replaced with spatial groupnorm to avoid temporal flickering between different chunks.\n\n### Key Takeaways\n\n*   The dual-path architecture maintains both high-frequency details and low-frequency structure, overcoming blurring artifacts.\n*   The staged training approach ensures a robust VAE capable of handling complex video data efficiently.\n*   The use of causal 3D convolutions enables joint image and video modeling while preserving temporal causality.\n*   The architecture is designed to use parameters more efficiently, improving overall performance."
    },
    {
        "question_id": "2502.10248v1_2",
        "answer": "Direct Preference Optimization (DPO) is a technique used to align a model's output with human preferences, and in the context of Step-Video-T2V, it's specifically used to enhance the visual quality of the generated videos. Here's a breakdown:\n\n### What is Direct Preference Optimization (DPO)?\n\nDPO is a method that directly optimizes the model based on human feedback, indicating which outputs are preferred over others. Instead of using reinforcement learning, which can be complex and unstable, DPO reframes the problem as a classification task. It aims to train the model to generate outputs that are more likely to be preferred by humans, while simultaneously avoiding outputs that are considered less desirable.\n\n***\n\n### How DPO Works in Step-Video-T2V\n\n1.  **Data Collection**:\n    *   A diverse set of prompts is created, partly from the training data and partly synthesized by human annotators.\n    *   For each prompt, the Step-Video-T2V model generates multiple videos using different random seeds.\n    *   Human annotators then evaluate these videos and rank their preferences. This results in pairs of preferred and non-preferred videos for each prompt.\n\n2.  **Training Objective**:\n\n    *   The DPO loss function is designed to maximize the likelihood of generating preferred videos while minimizing the likelihood of generating non-preferred videos. A reference policy is introduced to prevent the current policy from deviating too far. The loss function is defined as:\n        $L_{DPO} = -E_{(y, x_w, x_l) \\sim D} \\left[ \\log \\sigma \\left( \beta \\left( \\log \frac{\\pi_\theta(x_w|y)}{\\pi_{ref}(x_w|y)} - \\log \frac{\\pi_\theta(x_l|y)}{\\pi_{ref}(x_l|y)} \right) \right) \right]$\n\n        Where:\n\n        *   $\\pi_\theta$ is the current policy (the model being trained).\n        *   $\\pi_{ref}$ is the reference policy (a fixed model used for stabilization).\n        *   $x_w$ is the preferred sample.\n        *   $x_l$ is the non-preferred sample.\n        *   $y$ is the condition (the prompt).\n        *   $\beta$ is a hyperparameter controlling the strength of the preference optimization.\n        *   $D$ is the dataset of preferences.\n\n3.  **Gradient Optimization**:\n\n    *   The gradients are computed to update the model's parameters.\n    *   To avoid gradient explosion, the hyperparameter $\beta$ is reduced and the learning rate is increased, which results in faster convergence.\n\n***\n\n### Benefits and Improvements\n\n1.  **Visual Quality**: DPO enhances the visual quality of the generated videos by aligning them with human aesthetic preferences.\n2.  **Consistency and Plausibility**: Human feedback improves the consistency and plausibility of the generated videos.\n3.  **Prompt Alignment**: DPO enhances the alignment with the given prompts, resulting in more accurate and relevant video generation.\n\n***\n\n### Addressing Limitations\n\n1.  **Data Utilization**: To address the issue of the training data becoming outdated, a reward model is trained using human-annotated feedback data. This reward model dynamically evaluates the quality of newly generated samples during training, improving data efficiency.\n2.  **Implicit Dynamic Conditions**: The trajectory from initial noise to timestep-specific latents is underutilized due to computational limitations.\n3.  **Feedback Precision**: A tradeoff exists between sparse and imprecise feedback, especially in video diffusion models, where only a few pixels may be problematic, yet feedback often lacks precision.\n4.  **Preference Optimization**: Unlike LLMs, which use token-level softmax to create competition between tokens, diffusion models rely on regression, which may result in less efficient preference optimization."
    },
    {
        "question_id": "2502.10248v1_3",
        "answer": "Here's a breakdown of the advantages of using a **Diffusion Transformer (DiT)** with **3D full attention** in the Step-Video-T2V model, focusing on the architectural and functional benefits:\n\n***\n\n### Model Architecture\n\n*   **Integration of Text Prompts:** The **DiT** architecture allows for direct integration of text prompts into the Transformer network.\n*   **Disentanglement:** **DiT** can disentangle text and video, which is useful for pure video prediction models that don't require text.\n*   **Spatial-Temporal Attention:** **DiT** captures spatial information among tokens with the same temporal index within spatial Transformer blocks, and temporal information across time steps in temporal Transformer blocks.\n\n***\n\n### 3D Full Attention Mechanism\n\n*   **Unified Attention Process:** This mechanism combines both spatial and temporal information in a unified attention process.\n*   **Performance:** The **3D full attention** model outperforms spatial-temporal attention-based models, particularly in generating videos with high motion dynamics.\n*   **Motion Modeling:** It provides a theoretical upper bound for modeling both spatial and temporal information in videos.\n*   **Motion Consistency:** It is superior in generating videos with smooth and consistent motion.\n\n***\n\n### Text Encoding\n\n*   **Bilingual Text Encoders:** The model uses two bilingual text encoders (**Hunyuan-CLIP** and **Step-LLM**) to process user text prompts.\n*   **Handling Prompt Length:** By combining these text encoders, the model can handle user prompts of varying lengths.\n*   **Robust Text Representations:** The model generates robust text representations that guide it in the latent space.\n\n***\n\n### Positional Encoding\n\n*   **RoPE-3D:** This method handles video data by accounting for temporal (frame) and spatial (height and width) dimensions.\n*   **Flexibility:** It can process videos with different frame counts and resolutions without being restricted by fixed positional encoding lengths.\n*   **Generalization:** It improves generalization across diverse video data.\n*   **Relationship Capture:** It effectively captures both spatial and temporal relationships within the video.\n\n***\n\n### Normalization\n\n*   **Adaptive Layer Normalization (AdaLN):** This is used to embed timestep and class label information.\n*   **QK-Norm:** This normalizes the dot product between the query (Q) and key (K) vectors, addressing numerical instability and ensuring stable attention during training."
    },
    {
        "question_id": "2502.10248v1_4",
        "answer": "Step-Video-T2V uses a dual approach to handle bilingual text prompts, employing two distinct text encoders: Hunyuan-CLIP and Step-LLM. These encoders process user text prompts in both English and Chinese, and their outputs are combined to guide the video generation process.\n\n***\n\nHere's a breakdown of how it works and the advantages of this approach:\n\n### Bilingual Text Encoding in Step-Video-T2V\n\n1.  **Two Separate Text Encoders**:\n    *   **Hunyuan-CLIP**: A bidirectional text encoder from an open-source bilingual CLIP model.\n    *   **Step-LLM**: An in-house, unidirectional bilingual text encoder pre-trained using the next-token prediction task.\n2.  **Processing Prompts**:\n    *   Both encoders process the input text prompt independently.\n    *   Hunyuan-CLIP excels at producing text representations well-aligned with the visual space due to its CLIP model training mechanism.\n    *   Step-LLM is designed to handle longer and more complex text sequences efficiently.\n3.  **Combining Outputs**:\n    *   The outputs from both encoders are concatenated along the sequence dimension.\n    *   This combined embedding is then injected into a cross-attention layer within the diffusion transformer (DiT) network.\n4.  **Cross-Attention Layer**:\n    *   The cross-attention layer allows the model to attend to textual information while processing visual features.\n    *   This enables the model to generate videos conditioned on the input prompt effectively.\n\n***\n\n### Advantages of Using Two Separate Text Encoders\n\n1.  **Handling Varying Prompt Lengths**:\n    *   Hunyuan-CLIP has a limitation of 77 tokens for input length, which can be restrictive for longer prompts.\n    *   Step-LLM has no input length restriction, making it suitable for handling lengthy and complex text sequences.\n    *   By combining both, the model can handle user prompts of varying lengths effectively.\n2.  **Robust Text Representations**:\n    *   Hunyuan-CLIP provides text representations well-aligned with the visual space, which is beneficial for generating visually coherent videos.\n    *   Step-LLM enhances both efficiency and accuracy in sequence processing, ensuring that the model captures the nuances of the text prompt.\n    *   The combined approach results in robust text representations that effectively guide the model in the latent space.\n3.  **Improved Efficiency and Accuracy**:\n    *   Step-LLM incorporates a redesigned Alibi-Positional Embedding to improve efficiency and accuracy in sequence processing.\n    *   This ensures that the model can handle complex text sequences without compromising on performance.\n4.  **Effective Guidance in Latent Space**:\n    *   By using two encoders, Step-Video-T2V can generate videos conditioned on the input prompt more effectively.\n    *   The combined text embedding is injected into the cross-attention layer, allowing the model to generate videos that align well with the input prompt.\n\n***\n\nIn summary, Step-Video-T2V leverages the strengths of both Hunyuan-CLIP and Step-LLM to handle bilingual text prompts effectively. This approach allows the model to process prompts of varying lengths, generate robust text representations, and ultimately produce high-quality videos that align well with the input text."
    },
    {
        "question_id": "2502.10248v1_5",
        "answer": "Here's a breakdown of the challenges faced by current diffusion-based text-to-video models and how Step-Video-T2V attempts to address them:\n\n***\n\n### Main Challenges\n\n*   **High-Quality Labeled Data:**\n    *   **Challenge:** Existing video captioning models often struggle with **hallucination**, creating inaccuracies. Human annotation is expensive and difficult to scale.\n*   **Instruction Following:**\n    *   **Challenge:** It's difficult to generate videos based on detailed descriptions, handle complex action sequences, and combine multiple concepts effectively. Models often miss objects, get details wrong, or produce incomplete action sequences.\n*   **Laws of Physics:**\n    *   **Challenge:** Current models struggle to accurately simulate the real world and generate videos that adhere to the laws of physics (e.g., a ball bouncing realistically).\n*   **Computational Cost:**\n    *   **Challenge:** Training and generating long-duration, high-resolution videos requires substantial computational resources.\n\n***\n\n### How Step-Video-T2V Attempts to Address These Challenges\n\n*   **High-Quality Labeled Data:**\n    *   **Approach:** Step-Video-T2V uses a small amount of high-quality, human-labeled data in **SFT** (Supervised Fine-Tuning) to improve overall video quality. The quality and diversity of data are prioritized over sheer scale. The model leverages curated datasets with diverse video styles and motion dynamics.\n    *   **Future Plans:** The team plans to build a comprehensive video knowledge base with structured labels.\n*   **Instruction Following:**\n    *   **Approach:** The model architecture and training process are designed to better interpret and follow instructions, involving various objects, actions, and details.\n    *   **Observation:** The paper notes that heuristically repeating missing objects in the prompt can improve problematic cases, emphasizing the importance of ensuring all elements in the prompt receive appropriate attention.\n    *   **Future Plans:** The team aims to refine the model\u2019s ability to better attend to and follow all elements in the prompt, balancing attention across different aspects of the instruction.\n*   **Laws of Physics:**\n    *   **Approach:** The paper acknowledges the limitations of current diffusion-based models in adhering to the laws of physics.\n    *   **Future Plans:** The team plans to develop more advanced model paradigms, such as combining autoregressive and diffusion models within a unified framework, to better simulate realistic interactions.\n*   **Computational Cost:**\n    *   **Approach:**\n        *   **Deep Compression Video-VAE:** A specially designed deep compression **Variational Auto-encoder (VAE)** achieves 16x16 spatial and 8x temporal compression ratios. This significantly reduces the computational complexity of large-scale video generation training.\n        *   **Cascaded Training Pipeline:** A cascaded training pipeline, including text-to-image pre-training, text-to-video pre-training, **SFT**, and **Direct Preference Optimization (DPO)**, accelerates model convergence and leverages video datasets of varying quality.\n        *   **Model and System Optimizations:** Optimizations of model hyper-parameters, operators, and parallelism ensure both the stability and efficiency of training from a system-level perspective.\n        *   **Bucketization:** To handle varying video lengths and aspect ratios during training, variable-length and variable-resolution strategies are employed, using length and aspect ratio buckets.\n*   **Other Approaches:**\n    *   **Video-based DPO:** A video-based **DPO** approach is applied to reduce artifacts and improve the visual quality of the generated videos.\n\n***"
    },
    {
        "question_id": "2502.10248v1_6",
        "answer": "Step-Video-T2V employs **Flow Matching** as a training objective for both video and image generation. Here's a breakdown of how it works and why it's beneficial:\n\n### Flow Matching in Step-Video-T2V\n\n1.  **Sampling Noise and Timestep**:\n\n    *   A Gaussian noise `$X_0$` is sampled from a standard normal distribution `$N(0, 1)$`.\n    *   A random timestep `$t$` is sampled from a uniform distribution between 0 and 1 (i.e., `$t \u2208 [0, 1]$`).\n\n2.  **Constructing Model Input**:\n\n    *   The model input `$X_t$` is created as a linear interpolation between the initial noise `$X_0$` and the target sample `$X_1$`, where `$X_1$` represents the noise-free input (i.e., the actual data sample).\n    *   The formula for `$X_t$` is:\n        ```\n        $X_t = (1 - t) \\cdot X_0 + t \\cdot X_1$\n        ```\n\n3.  **Defining Ground Truth Velocity**:\n\n    *   The ground truth velocity `$V_t$` represents the rate of change of `$X_t$` with respect to the timestep `$t$`.\n    *   It's defined as the difference between the target sample `$X_1$` and the initial noise `$X_0$`:\n        ```\n        $V_t = \frac{dX_t}{dt} = X_1 - X_0$\n        ```\n        In essence, `$V_t$` captures the direction and magnitude of change required to transform the noise into the data.\n\n4.  **Training the Model**:\n\n    *   The model is trained to predict the velocity field `$u(X_t, y, t; \u03b8)$` at timestep `$t$`, given the input `$X_t$`, an optional conditioning input `$y$` (e.g., a text prompt), and model parameters `$\u03b8$`.\n    *   The training loss is the mean squared error (MSE) between the predicted velocity and the ground truth velocity:\n        ```\n        $loss = E_{t, X_0, X_1, y} [\\|u(X_t, y, t; \u03b8) - V_t\\|^2]$\n        ```\n        The expectation is taken over all training samples.\n\n5.  **Inference**:\n\n    *   During inference, random noise `$X_0$` is sampled from a standard normal distribution.\n    *   The goal is to recover the denoised sample `$X_1$` by iteratively refining the noise through an ODE-based method.\n    *   A sequence of timesteps `{t0, t1, ..., tn}` is defined, where `$t_0 = 0$`, `$t_n = 1$`, and `$t_0 < t_1 < ... < t_n$`.\n    *   The denoising process is carried out by integrating over these timesteps, using a Gaussian solver. The denoised sample `$X_1$` can be expressed as:\n        ```\n        $X_1 = \\sum_{i=0}^{n-1} u(X_{t_i}, y, t_i; \u03b8) \\cdot (t_{i+1} - t_i)$\n        ```\n        This iterative process gradually transforms the initial noise into a coherent video frame or sequence.\n\n***\n\n### Benefits of Flow Matching for Video Generation\n\n1.  **Stable Training**: Flow Matching provides a more stable training process compared to other diffusion-based approaches. By directly predicting the velocity field, the model learns a smoother and more consistent mapping from noise to data.\n\n2.  **High-Quality Samples**: Models trained with Flow Matching are capable of generating high-quality video samples with realistic motion dynamics and coherent content. The direct velocity prediction helps in preserving fine details and ensuring temporal consistency in the generated videos.\n\n3.  **Flexibility**: Flow Matching can be easily conditioned on various inputs, such as text prompts, allowing for controllable video generation. The conditioning input `$y$` in the loss function enables the model to generate videos that align with specific user instructions or descriptions.\n\n4.  **Efficient Inference**: The ODE-based inference method used in Flow Matching allows for efficient generation of video samples with a relatively small number of steps. This is particularly important for video generation, where computational costs can be high due to the large data volumes involved.\n\n5.  **Reduction of Artifacts**: By training the model to predict the velocity field, Flow Matching helps in reducing artifacts and inconsistencies in the generated videos. The smoother mapping from noise to data results in more visually appealing and realistic video outputs."
    },
    {
        "question_id": "2502.10248v1_7",
        "answer": "The Step-Video-T2V model employs a hierarchical data filtering approach to refine its training data, which is crucial for achieving high-quality video generation. This process involves progressively applying filters with increasing thresholds to create several pre-training subsets. A final SFT dataset is then constructed through manual filtering.\n\n***\n\nHere's a breakdown of the key elements in this filtering strategy:\n\n### 1. Data Filtering Stages\n\n*   **Progressive Thresholds**: The data is subjected to a series of filters, each applying stricter criteria than the last. This hierarchical approach ensures that only the highest quality data is retained for the most critical stages of training.\n*   **Pre-training Subsets**: The filtering process results in multiple subsets of the data, each used for different phases of pre-training.\n*   **Manual Filtering**: The final step involves human evaluators who assess the videos for clarity, aesthetics, motion, and the absence of artifacts like watermarks or subtitles. Captions are also refined manually.\n\n### 2. Key Filters Applied\n\nThe paper mentions several key filters used in this process:\n\n*   **Aesthetic Score**: Predicted using a CLIP-based aesthetic predictor.\n*   **NSFW Score**: Detected using a CLIP-based NSFW detector.\n*   **Watermark Detection**: Identified using an EfficientNet image classification model.\n*   **Subtitle Detection**: Localized using PaddleOCR.\n*   **Saturation Score**: Assessed by converting video frames to the HSV color space and analyzing the saturation channel.\n*   **Blur Score**: Measured using the variance of the Laplacian method.\n*   **Black Border Detection**: Detected using FFmpeg.\n\n### 3. Importance of Hierarchical Data Filtering\n\n*   **Improved Data Quality**: By systematically filtering out low-quality data, the model is trained on a cleaner dataset, leading to better performance.\n*   **Efficient Learning**: Training on high-quality data allows the model to learn more effectively, as it is not distracted by noise or irrelevant information.\n*   **Enhanced Generalization**: High-quality data tends to be more representative of the true data distribution, leading to better generalization performance.\n*   **Human Intuition**: Adjusting the training data to reflect what humans consider higher quality results in a significant reduction in training loss, suggesting the model's learning process emulates human cognitive patterns to some extent.\n*   **Centroid-Based Filtering**: This method removes videos whose features deviate significantly from the cluster centroid, ensuring a diverse yet representative subset."
    },
    {
        "question_id": "2502.10248v1_8",
        "answer": "Step-Video-T2V is positioned as a competitive alternative to commercial video generation models, such as Sora and Veo, with its own set of strengths and limitations. Let's break down the comparison in terms of performance and capabilities:\n\n***\n\n### General Performance\n\n*   **Comparable Performance:** Step-Video-T2V delivers performance that is on par with commercial engines for general text prompts.\n*   **Specific Domain Superiority:** It even surpasses them in particular areas, such as generating videos with high motion dynamics or text content. This suggests a specialization or optimization within the model that allows it to excel in these niches.\n\n***\n\n### Key Differentiators\n\n*   **Model Size:** Step-Video-T2V has **30B parameters**.\n*   **Video Length:** Step-Video-T2V can generate videos up to **204 frames** in length.\n*   **Bilingual Text Prompts:** Supports both English and Chinese.\n*   **Video-Based DPO:** Implements a video-based **DPO** approach to reduce artifacts and enhance visual quality.\n*   **High-Compression VAE:** Utilizes a high-compression **VAE** for videos. The **Video-VAE** achieves **16x16 spatial** and **8x temporal compression ratios**.\n\n***\n\n### Limitations and Challenges\n\n*   **Training Data:** The model has seen **25.3M samples** during the final stage of pre-training with **540P** videos. The amount of high-quality data in the post-training phase is significantly less compared to commercial engines.\n*   **Video Resolution:** Generates **540P** videos, whereas commercial models like **T2VTopA** and **T2VTopB** generate **720P** and **1080P** videos, respectively.\n*   **Instruction Following:** Commercial models may exhibit better instruction-following capabilities due to better video captioning models and greater human effort in labeling post-training data.\n*   **Generalization:** Faces difficulties in generating videos that obey the laws of physics.\n\n***\n\n### Benchmarking\n\n*   **Step-Video-T2V-Eval:** The paper introduces **Step-Video-T2V-Eval** as a new benchmark, which includes **128 diverse prompts** across **11 categories** and video generation results from top open-source and commercial engines.\n*   **Movie Gen Video Bench:** In comparison to **Movie Gen Video**, **Step-Video-T2V** achieves a comparable performance.\n\n***\n\n### Key Takeaways\n\nStep-Video-T2V distinguishes itself through its open-source nature, high compression **VAE**, bilingual capabilities, and video-based **DPO**. While it demonstrates competitive performance and even excels in specific domains, it is still behind commercial models in terms of training data, video resolution, and instruction following."
    },
    {
        "question_id": "2502.10248v1_9",
        "answer": "Human feedback plays a crucial role in refining **Step-Video-T2V**'s video generation by improving the visual quality of the generated videos and ensuring better alignment with user prompts. This is achieved through a specific implementation in the training pipeline.\n\n***\n\nHere's a breakdown of how human feedback is incorporated:\n\n### 1. Integration of Human Feedback\n\n*   The pipeline incorporates human feedback to improve the visual quality of **Step-Video-T2V**, drawing inspiration from the successful application of similar methods in **LLMs**.\n*   The overall pipeline is designed to adjust the generated content based on human preferences.\n\n### 2. Direct Preference Optimization (DPO)\n\n*   **DPO** is selected as the method for incorporating human feedback due to its effectiveness and simplicity.\n*   The goal is to align the model with the generation of preferred data while avoiding non-preferred data.\n*   A reference policy is introduced to stabilize training and prevent the current policy from deviating too far.\n*   The policy objective is formulated as:\n\n    $L_{DPO} = -E_{(y,x_w,x_l) \\sim D} \\left[ log \\sigma \\left( \beta \\left( log \frac{\\pi_\theta(x_w|y)}{\\pi_{ref}(x_w|y)} - log \frac{\\pi_\theta(x_l|y)}{\\pi_{ref}(x_l|y)} \right) \right) \right]$\n\n    *   $\\pi_\theta$ and $\\pi_{ref}$ refer to the current policy and reference policy, respectively.\n    *   $x_w$ and $x_l$ are the preferred and non-preferred samples.\n    *   $y$ denotes the condition.\n\n### 3. Sample Collection\n\n*   A diverse prompt set is constructed by randomly selecting prompts from the training data and synthesizing prompts based on real-world user interaction patterns.\n*   **Step-Video-T2V** generates multiple videos for each prompt using different seeds.\n*   Human annotators rate the preference of these samples, with quality control to ensure accuracy and consistency.\n*   This process results in a set of preference and non-preference data for model training.\n\n### 4. Training Process\n\n*   At each training step, a prompt and its corresponding positive and negative sample pairs are selected.\n*   Each sample is generated by the model itself to ensure smooth updates and improve training stability.\n*   Positive and negative samples are aligned by fixing the initial noise and timestep to maintain consistency in the training data.\n\n### 5. Reward Model Integration\n\n*   A reward model is trained using human-annotated feedback data to dynamically evaluate the quality of newly generated samples during training.\n*   The reward model is periodically fine-tuned to maintain alignment with the evolving policy.\n*   By integrating the reward model, the training data is scored and ranked on-the-fly, improving data efficiency.\n\n### 6. Trajectory and Feedback Precision\n\n*   The trajectory from initial noise to timestep-specific latents acts as implicit dynamic conditions, but this dimension is underutilized due to computational limitations.\n*   A tradeoff exists between sparse and imprecise feedback, especially in video diffusion models, where only a few pixels may be problematic.\n\n### 7. Preference Optimization Efficiency\n\n*   Diffusion models rely on regression, which may result in less efficient preference optimization compared to **LLMs** that use token-level softmax.\n\n***"
    }
]