[
    {
        "question_id": "2502.10248v1_0",
        "answer": "The paper defines two levels of video foundation models, each with distinct capabilities:\n\n1.  **Level-1: Translational Video Foundation Model**.\n2.  **Level-2: Predictable Video Foundation Model**.\n\n***\n\n### Key Differences:\n\n*   **Level-1** models function as cross-modal translation systems. They generate videos from text, visual, or multimodal inputs. These models excel at creating high-quality videos from prompts, which lowers the barrier for content creation. However, they struggle with complex action sequences or adherence to the laws of physics because they primarily learn mappings between prompts and videos without modeling underlying causal relationships.\n*   **Level-2** models act as prediction systems, similar to large language models (LLMs). They forecast future events based on context and handle advanced tasks such as reasoning with multimodal data or simulating real-world scenarios.\n\nIn essence, **Level-1** models translate inputs into videos, while **Level-2** models predict and reason about video content."
    },
    {
        "question_id": "2502.10248v1_1",
        "answer": "Step-Video-T2V employs a specialized **Video-VAE** architecture to achieve high spatial and temporal compression while preserving video reconstruction quality. Here's a breakdown of the key components and techniques:\n\n### Architecture Overview\n\n*   **High Compression Ratios**: The **Video-VAE** achieves a 16x16 spatial and 8x temporal compression ratio.\n*   **Dual-Path Architecture**: A novel dual-path architecture is introduced in the later stage of the encoder and the early stage of the decoder. This architecture facilitates unified spatial-temporal compression.\n*   **3D Convolutions**: The architecture uses 3D convolutions for spatial-temporal downscaling.\n*   **Pixel Unshuffling**: Optimized pixel unshuffling operations contribute to the 8\u00d716\u00d716 downscaling.\n\n***\n\n### Encoder Details\n\nFor an input video tensor $X \\in R^{B \times C \times T \times H \times W}$, the encoder $E$ produces a latent representation $Z = E(X) \\in R^{B \times C_z \times \\lceil T/8 \rceil \times \\lceil H/16 \rceil \times \\lceil W/16 \rceil}$.\n\n*   **Causal 3D Convolutional Modules**: The early stage of the encoder consists of three stages, each featuring two Causal **Res3DBlock** and corresponding downsample layers. A **MidBlock** combines convolutional layers with attention mechanisms.\n*   **Temporal Causality**: Temporal causality is implemented using Causal 3D Convolutional Modules:\n\n    $C3D(X)_t = \begin{cases}\n    Conv3D([0, ..., X_t], \\Theta) & t = 0 \\\n    Conv3D([X_{t-k}, ..., X_t], \\Theta) & t > 0\n    \\end{cases}$\n\n    where $k$ is the temporal kernel size.\n*   **Dual-Path Latent Fusion**: This maintains high-frequency details through convolutional processing while preserving low-frequency structure via channel averaging.\n    1.  **Conv Path**: Combines causal 3D convolutions with pixel unshuffling:\n\n        $H_{conv} = U^{(3)}_s(C3D(X))$\n\n        where $U^{(3)}_s : R^{B \times C \times T_{st} \times H_{ss} \times W_{ss}} \rightarrow R^{B \times C \\cdot s^3 \times T/s_t \times H/s_s \times W/s_s}$ with spatial stride $s_s = 2$, temporal stride $s_t = 2$.\n    2.  **Shortcut Path**: Preserves structural semantics through grouped channel averaging:\n\n        $H_{avg} = \frac{1}{s^3} \\sum_{k=0}^{s^3 - 1} U^{(3)}_s (X)[..., kC_z:(k+1)C_z]$\n\n        where $U^{(3)}_s$ implements 3D pixel unshuffle with spatial-temporal blocking, and $C_z$ is the latent dim of the next stage.\n    3.  **Output Fusion**: Combines both paths through residual summation:\n\n        $Z = H_{conv} \\oplus H_{avg}$\n\n***\n\n### Decoder Details\n\n*   **Symmetric Dual Path Architectures**: The early stage of the decoder consists of two symmetric Dual Path architectures.\n*   **Pixel Shuffling**: The 3D pixel unshuffle operation $U$ is replaced by a 3D pixel shuffle operator $P$.\n*   **Channel Repeating**: The grouped channel averaging path is replaced by a grouped channel repeating operation.\n*   **Spatial Group Normalization**: All groupnorm layers are replaced with spatial groupnorm to avoid temporal flickering between different chunks.\n\n***\n\n### Training Details\n\n*   **Multi-Stage Training**: The **VAE** training process is designed in multiple stages.\n    1.  **Initial Training**: A **VAE** with a 4x8x8 compression ratio is trained without a dual-path structure, jointly on images and videos.\n    2.  **Enhanced Model**: Two dual-path modules are incorporated in both the encoder and decoder. The dual-path modules, mid-block, and **ResNet** backbone are gradually unfrozen.\n*   **Loss Functions**: A combination of **L1** reconstruction loss, **Video-LPIPS**, and **KL**-divergence constraint guide the model. **GAN** loss is introduced to further refine the model\u2019s performance.\n\n***\n\n### Key Factors for Maintaining Reconstruction Quality\n\n*   **Dual-Path Architecture**: Helps to maintain both high-frequency details and low-frequency structure.\n*   **Causal 3D Convolutions**: Enable effective spatial-temporal compression.\n*   **Multi-Stage Training**: Ensures robust and high-quality video data modeling.\n*   **Combination of Loss Functions**: Balances reconstruction quality and regularization.\n\nBy combining these architectural and training techniques, Step-Video-T2V\u2019s **Video-VAE** effectively achieves high spatial and temporal compression while maintaining exceptional video reconstruction quality."
    },
    {
        "question_id": "2502.10248v1_2",
        "answer": "Direct Preference Optimization (**DPO**) is a technique used to align a model's behavior with human preferences, particularly in generation tasks. It's a method that simplifies the process of incorporating human feedback into the training of generative models. Instead of using reinforcement learning, which can be complex and unstable, **DPO** directly optimizes the model based on preference data.\n\n***\n\nHere's how **DPO** works in the context of improving the visual quality of generated videos in Step-Video-T2V:\n\n1.  **Preference Data Collection**:\n    *   A diverse set of prompts is created. These prompts are designed to reflect real-world user interaction patterns.\n    *   The Step-Video-T2V model generates multiple videos for each prompt, using different random seeds to create variations.\n    *   Human annotators evaluate these videos and indicate their preferences, selecting which videos are better (preferred) and which are worse (non-preferred).\n    *   Quality control measures are implemented to ensure the accuracy and consistency of the annotations.\n\n2.  **Objective of DPO**:\n    *   The goal is to adjust the model (current policy) to generate more of the preferred videos and fewer of the non-preferred videos, given the same prompt.\n    *   A reference policy (a reference model) is used to stabilize training and prevent the current policy from deviating too far.\n\n3.  **Mathematical Formulation**:\n    *   The **DPO** loss function is defined as:\n\n        $L_{DPO} = -E_{(y, x_w, x_l) \\sim D} \\left[ \\log \\sigma \\left( \beta \\left( \\log \frac{\\pi_\theta(x_w|y)}{\\pi_{ref}(x_w|y)} - \\log \frac{\\pi_\theta(x_l|y)}{\\pi_{ref}(x_l|y)} \right) \right) \right]$\n\n        where:\n\n        *   $\\pi_\theta$ is the current policy (the model being trained).\n        *   $\\pi_{ref}$ is the reference policy (the reference model).\n        *   $x_w$ is the preferred (winner) sample.\n        *   $x_l$ is the non-preferred (loser) sample.\n        *   $y$ is the condition (prompt).\n        *   $\beta$ is a hyperparameter controlling the strength of the preference.\n        *   $\\sigma$ is the sigmoid function.\n        *   $E$ is the expectation over the dataset $D$.\n\n4.  **Training Process**:\n\n    *   At each step, a prompt and its corresponding preferred and non-preferred video pairs are selected.\n    *   Both the positive and negative samples are generated by the model itself to ensure smooth updates and training stability.\n    *   The initial noise and timestep are fixed to align the positive and negative samples, contributing to a more stable training process.\n    *   The training objective is based on Diffusion**DPO** and **DPO**, but with modifications to extend it to the Flow Matching framework used in Step-Video-T2V.\n    *   The derivative of the **DPO** loss with respect to the model parameters $\theta$ is:\n\n        $\frac{\\partial L_{DPO}}{\\partial \theta} \\propto -\beta(1 - \\sigma(\beta z)) \\cdot \frac{\\partial z}{\\partial \theta}$\n\n        where $z$ is the term inside the logarithm in the **DPO** loss function.\n\n5.  **Addressing Gradient Issues**:\n\n    *   A large $\beta$ can cause gradient explosion when $z < 0$, requiring gradient clipping and a very low learning rate, which slows convergence.\n    *   To solve this, the value of $\beta$ is reduced, and the learning rate is increased, resulting in faster convergence.\n\n6.  **Reward Model Integration**:\n\n    *   To address the issue of the model outgrowing the human-annotated data, a reward model is trained using the human feedback data.\n    *   This reward model evaluates the quality of newly generated samples during training.\n    *   The reward model is periodically fine-tuned with new human feedback to stay aligned with the evolving policy.\n    *   The reward model scores and ranks training data on-the-fly (on-policy), improving data efficiency.\n\n***\n\nIn summary, **DPO** improves the visual quality of generated videos in Step-Video-T2V by directly incorporating human preferences into the training process. It adjusts the model to generate more visually appealing and coherent videos based on feedback from human annotators, using a stable and efficient optimization process."
    },
    {
        "question_id": "2502.10248v1_3",
        "answer": "The Step-Video-T2V model leverages a Diffusion Transformer (DiT) architecture with 3D full attention, offering several key advantages for video generation. These advantages stem from the DiT's inherent properties and the specific benefits of 3D full attention.\n\n*   **Superior Modeling of Spatial and Temporal Information**:\n\n    *   3D full attention allows the model to jointly process spatial and temporal information in videos. This unified attention mechanism has a higher theoretical capacity for capturing complex relationships compared to methods that treat spatial and temporal dimensions separately.\n    *   By considering all spatial and temporal elements simultaneously, the model can better understand motion dynamics and dependencies across frames.\n*   **Enhanced Video Quality and Consistency**:\n\n    *   The use of 3D full attention results in videos with smoother and more consistent motion. This is because the model can directly learn the relationships between different parts of the video across both space and time.\n    *   The improved understanding of motion dynamics leads to higher-quality video generation, as the model can better maintain coherence and realism in the generated content.\n*   **Effective Integration of Textual Information**:\n\n    *   The DiT architecture incorporates cross-attention layers that allow the model to attend to textual information while processing visual features. This enables the generation of videos conditioned on input prompts.\n    *   By using two distinct bilingual text encoders (Hunyuan-CLIP and Step-LLM), the model can handle user prompts of varying lengths and complexities, generating robust text representations that guide the video generation process.\n*   **Efficient Handling of Computational Overhead**:\n\n    *   The model employs Adaptive Layer Normalization (AdaLN) with optimized computation to reduce the computational overhead of traditional AdaLN operations. This improves the overall efficiency of the model.\n    *   The use of RoPE-3D, an extension of Rotation-based Positional Encoding (RoPE), allows the model to handle video data by accounting for temporal and spatial dimensions. This enables the model to process videos of varying lengths and resolutions.\n*   **Scalability and Performance**:\n\n    *   The DiT architecture, with its 30B parameters, provides a strong foundation for generating high-quality videos. The model's performance benefits from its ability to scale and handle complex video data efficiently.\n    *   The architecture's design allows for optimizations such as distillation, which reduces the number of function evaluations needed during inference, improving efficiency without significant performance degradation."
    },
    {
        "question_id": "2502.10248v1_4",
        "answer": "Step-Video-T2V uses two bilingual text encoders to process user text prompts: Hunyuan-CLIP and Step-LLM.\n\n***\n\n### Hunyuan-CLIP\n\n*   It is a bidirectional text encoder of an open-source bilingual CLIP model.\n*   It produces text representations well-aligned with the visual space due to the training mechanism of the CLIP model.\n*   It has a maximum input length limited to 77 tokens, which poses challenges when processing longer user prompts.\n\n***\n\n### Step-LLM\n\n*   It is an in-house, unidirectional bilingual text encoder pre-trained using the next-token prediction task.\n*   It incorporates a redesigned Alibi-Positional Embedding to improve both efficiency and accuracy in sequence processing.\n*   It has no input length restriction, making it particularly effective for handling lengthy and complex text sequences.\n\n***\n\n### Advantages of Using Two Separate Text Encoders\n\n1.  **Handling Varying Prompt Lengths**: By combining these two text encoders, Step-Video-T2V can handle user prompts of varying lengths. Hunyuan-CLIP is suitable for shorter prompts, while Step-LLM is effective for longer and more complex text sequences.\n2.  **Generating Robust Text Representations**: The two encoders generate robust text representations that effectively guide the model in the latent space. Hunyuan-CLIP provides representations well-aligned with the visual space, while Step-LLM captures more complex textual information.\n3.  **Improved Efficiency and Accuracy**: Step-LLM incorporates a redesigned Alibi-Positional Embedding to improve both efficiency and accuracy in sequence processing. This allows the model to handle long sequences without sacrificing performance."
    },
    {
        "question_id": "2502.10248v1_5",
        "answer": "Current diffusion-based text-to-video models face several significant challenges, which Step-Video-T2V attempts to address through its architecture, training strategies, and data handling techniques. Here's a breakdown:\n\n### Main Challenges of Diffusion-Based Text-to-Video Models\n\n1.  **High-Quality Labeled Data**:\n    *   **Challenge**: Existing video captioning models often suffer from hallucination issues, leading to unstable training and poor instruction-following performance. High-quality human annotations are expensive and difficult to scale.\n    *   **Step-Video-T2V's Approach**: The model uses both automated and manual filtering techniques to curate a high-quality video dataset. Human evaluators assess each video for clarity, aesthetics, appropriate motion, smooth scene transitions, and the absence of watermarks or subtitles. Captions are also manually refined to ensure accuracy.\n2.  **Instruction Following**:\n    *   **Challenge**: Generating videos based on detailed descriptions, handling complex action sequences, and combining multiple concepts with low occurrence in the training data remain difficult.\n    *   **Step-Video-T2V's Approach**: The model employs a cascaded training pipeline, including text-to-image pre-training, text-to-video pre-training, supervised fine-tuning (SFT), and direct preference optimization (DPO), to improve its ability to follow instructions.\n3.  **Adherence to Laws of Physics**:\n    *   **Challenge**: Current models struggle to generate videos that accurately simulate the real world and adhere to the laws of physics.\n    *   **Step-Video-T2V's Approach**: While the paper acknowledges this as a limitation of diffusion models, it suggests exploring more advanced model paradigms in future work, such as combining autoregressive and diffusion models within a unified framework.\n4.  **Computational Cost**:\n    *   **Challenge**: Training and generating long-duration, high-resolution videos require significant computational resources.\n    *   **Step-Video-T2V's Approach**: The model uses a deep compression Variational Autoencoder (**VAE**) to achieve 16x16 spatial and 8x temporal compression ratios, significantly reducing the computational complexity of large-scale video generation training.\n5.  **Generating Text Content in Videos**:\n    *   **Challenge**: Accurately generating text content within videos, especially in different languages, is difficult.\n    *   **Step-Video-T2V's Approach**: The model leverages the text-to-image pre-training stage, where a portion of the images contained text, and the captions explicitly described it.\n\n***"
    },
    {
        "question_id": "2502.10248v1_6",
        "answer": "Step-Video-T2V employs Flow Matching as a core technique in its training process to generate high-quality videos. Here's how it works and why it's beneficial:\n\n***\n\n### Flow Matching in Step-Video-T2V\n\n1.  **Noise Sampling**:\n\n    *   The process begins by sampling Gaussian noise, denoted as $X_0$, from a standard normal distribution $N(0, 1)$.\n    *   A random time step $t$ is also sampled from a uniform distribution between 0 and 1 (i.e., $t \u2208 [0, 1]$).\n2.  **Input Construction**:\n\n    *   The model input $X_t$ is constructed as a linear interpolation between the initial noise $X_0$ and the target sample $X_1$, where $X_1$ represents the noise-free target video frame.\n    *   The formula for this interpolation is:\n\n        $X_t = (1 - t) \\cdot X_0 + t \\cdot X_1$\n3.  **Velocity Calculation**:\n\n    *   The ground truth velocity $V_t$ is calculated, representing the rate of change of $X_t$ with respect to time $t$.\n    *   This velocity captures the direction and magnitude of change from the initial noise $X_0$ to the target data $X_1$, and is defined as:\n\n        $V_t = \frac{dX_t}{dt} = X_1 - X_0$\n4.  **Model Training**:\n\n    *   The model is trained to predict the velocity $u(X_t, y, t; \u03b8)$ at time $t$, given the input $X_t$, an optional conditioning input $y$ (such as a text prompt), and the model parameters $\u03b8$.\n    *   The training objective is to minimize the Mean Squared Error (MSE) loss between the predicted velocity $u(X_t, y, t; \u03b8)$ and the true velocity $V_t$. The loss function is:\n\n        $\text{loss} = E_{t, X_0, X_1, y} \\left[ \\| u(X_t, y, t; \u03b8) - V_t \\|^2 \right]$\n\n        Here, the expectation is taken over all training samples, with $t$, $X_0$, $X_1$, and $y$ drawn from the dataset.\n5.  **Inference**:\n\n    *   During inference, random noise $X_0$ is sampled from a Gaussian distribution $N(0, 1)$. The goal is to recover the denoised sample $X_1$ by iteratively refining the noise through an ODE-based method.\n    *   A sequence of timesteps $\\{t_0, t_1, ..., t_n\\}$ is defined, where $t_0 = 0$, $t_n = 1$, and $t_0 < t_1 < ... < t_n$. The denoising process is carried out by integrating over these timesteps.\n    *   The denoised sample $X_1$ can be expressed as:\n\n        $X_1 = \\sum_{i=0}^{n-1} u(X_{t_i}, y, t_i; \u03b8) \\cdot (t_{i+1} - t_i)$\n\n        where $u(X_{t_i}, y, t_i; \u03b8)$ represents the predicted velocity at timestep $t_i$, given the noisy sample $X_{t_i}$ and an optional conditioning input $y$.\n\n***\n\n### Benefits of Flow Matching for Video Generation\n\n1.  **Coherent Motion Dynamics**:\n\n    *   Flow Matching facilitates the learning of continuous trajectories from noise to the final video frames. This results in more coherent and realistic motion dynamics in the generated videos.\n2.  **Stable Training**:\n\n    *   By predicting the velocity field, Flow Matching provides a more stable training process compared to directly predicting the data distribution. The model learns to approximate the underlying vector field that transforms noise into data.\n3.  **High-Quality Video Generation**:\n\n    *   The method ensures that the generated video frames are consistent and follow the text prompts accurately, leading to high aesthetic and visual appeal.\n4.  **Efficient Sampling**:\n\n    *   Flow Matching allows for efficient sampling during inference, as the iterative refinement process gradually denoises the input, leading to faster convergence and high-quality video outputs.\n5.  **Flexibility and Control**:\n\n    *   The framework accommodates conditioning inputs such as text prompts, enabling precise control over the content and style of the generated videos."
    },
    {
        "question_id": "2502.10248v1_7",
        "answer": "The Step-Video-T2V model employs a hierarchical data filtering approach to refine its training data, which is crucial for achieving high-quality video generation. This process involves multiple stages of filtering, each designed to remove lower-quality data and ensure that the model is trained on the most suitable video clips. Here's a breakdown:\n\n### Stages of Hierarchical Data Filtering\n\n1.  **Automated Filtering by Video Assessment Scores**:\n    *   The initial step involves using video assessment scores and heuristic rules to filter the entire dataset.\n    *   This reduces the dataset to a subset of 30 million videos, significantly improving overall quality.\n\n2.  **Filtering by Video Categories**:\n    *   For videos within the same cluster, the \"Distance to Centroid\" values are used to identify and remove outliers.\n    *   Videos whose distance from the centroid exceeds a predefined threshold are discarded.\n    *   This ensures that the resulting subset contains a sufficient number of videos for each cluster while maintaining diversity.\n\n3.  **Labeling by Human Annotators**:\n    *   In the final stage, human evaluators assess each video for several criteria:\n        *   Clarity\n        *   Aesthetics\n        *   Appropriate motion\n        *   Smooth scene transitions\n        *   Absence of watermarks or subtitles\n    *   Captions are manually refined to ensure accuracy and include essential details such as:\n        *   Camera movements\n        *   Subjects\n        *   Actions\n        *   Backgrounds\n        *   Lighting\n\n***\n\n### Importance of Hierarchical Data Filtering\n\n1.  **Improved Model Convergence**:\n    *   Text-to-image pre-training is essential for the video generation model to acquire rich visual knowledge, including concepts, scenes, and their spatial relationships, providing a solid foundation for the subsequent text-to-video pre-training stages.\n    *   Text-to-video pre-training at low resolution is critical for the model to learn motion dynamics. The more stable the model is trained during this stage, using as much diverse training data as possible, the easier it becomes to refine and scale the model to higher resolutions and more complex video generation tasks.\n\n2.  **Enhanced Visual Quality**:\n    *   Using high-quality videos with accurate captions and desired styles in **SFT** is crucial to the stability of the model and the style of the generated videos.\n    *   Video-based **DPO** can further enhance the visual quality by reducing artifacts, ensuring smoother and more realistic video outputs.\n\n3.  **Better Generalization**:\n    *   By training on a dataset that has been refined to include high-quality, diverse, and accurately labeled videos, the model generalizes better to a wide range of prompts and scenarios.\n    *   This leads to more consistent and reliable video generation.\n\n4.  **Reduced Artifacts and Improved Realism**:\n    *   High-quality data helps to reduce common issues such as:\n        *   Artifacts\n        *   Inconsistent styles\n        *   Unrealistic motion\n\n5.  **Emulating Human Cognitive Patterns**:\n    *   Adjusting the training data to reflect what is considered higher-quality by humans results in a significant, stepwise reduction in training loss.\n    *   This suggests that, to some extent, the model\u2019s learning process may emulate human cognitive patterns."
    },
    {
        "question_id": "2502.10248v1_8",
        "answer": "Okay, let's break down how Step-Video-T2V stacks up against commercial video generation models such as Sora and Veo.\n\n***\n\n### Performance and Capabilities: Step-Video-T2V vs. Commercial Models\n\nHere's a structured comparison based on the information provided in the technical report:\n\n**1. General Text Prompt Performance:**\n\n*   Step-Video-T2V delivers comparable performance for general text prompts.\n\n**2. Specific Domains:**\n\n*   Step-Video-T2V surpasses commercial engines in generating videos with high motion dynamics or text content.\n\n**3. Video Generation Pipeline:**\n\n*   Commercial video generation engines often involve longer and more complex video generation pipelines with extensive pre- and post-processing.\n*   Step-Video-T2V offers a more streamlined approach.\n\n**4. Key Differentiators of Step-Video-T2V:**\n\n*   **Largest Open-Source Model:** It is the largest open-source model to date.\n*   **High-Compression VAE:** It utilizes a high-compression VAE for videos.\n*   **Bilingual Text Prompts:** Supports both English and Chinese.\n*   **Video-Based DPO:** Implements a video-based Direct Preference Optimization (DPO) approach to further reduce artifacts and enhance visual quality.\n*   **Comprehensive Documentation:** Provides comprehensive training and inference documentation.\n\n**5. Comparison with T2VTopA and T2VTopB:**\n\n*   **Overall Ranking:** T2VTopA > Step-Video-T2V > T2VTopB.\n*   **Aesthetic Appeal:** Commercial models (T2VTopA, T2VTopB) generally have a higher aesthetic appeal due to higher resolutions (720P, 1080P) and high-quality aesthetic data used during post-training.\n*   **Motion Dynamics:** Step-Video-T2V consistently outperforms T2VTopA and T2VTopB in the Sports category, demonstrating strong capabilities in modeling and generating videos with high-motion dynamics.\n*   **Instruction Following:** T2VTopA has better instruction-following capability, contributing to superior performance in categories such as Combined Concepts, Surreal, and Cinematography. This is attributed to a better video captioning model and greater human effort in labeling the post-training data.\n\n**6. Limitations of Step-Video-T2V:**\n\n*   **Training Data:** Step-Video-T2V has less high-quality data in the post-training phase compared to commercial engines.\n*   **Video Length and Resolution:** The video length is 204 frames, nearly twice the length of T2VTopA and T2VTopB, making training more challenging. The resolution is 540P, lower than the 720P/1080P of commercial models, which affects aesthetic appeal.\n\n**7. Future Improvements for Step-Video-T2V:**\n\n*   Increasing the amount of high-quality data in the post-training phase.\n*   More extensive pre-training.\n*   Refining the visual style and quality of generated results with high-quality labeled data.\n*   Increasing the output resolution.\n\n***"
    },
    {
        "question_id": "2502.10248v1_9",
        "answer": "Human feedback plays a crucial role in refining **Step-Video-T2V**'s video generation by improving visual quality and ensuring better alignment with user prompts. This is achieved through a pipeline that incorporates human preferences into the training process.\n\n***\n\n### Implementation of Human Feedback in the Training Pipeline\n\n1.  **Data Collection:**\n    *   A diverse set of prompts is constructed, including random prompts from the training data and prompts synthesized by human annotators based on real-world user interaction patterns.\n    *   For each prompt, **Step-Video-T2V** generates multiple videos using different seeds.\n    *   Human annotators then rate the preference of these generated videos, with quality control personnel ensuring accuracy and consistency.\n    *   This process results in a dataset of preference and non-preference data, which serves as the foundation for model training.\n\n2.  **Direct Preference Optimization (DPO):**\n    *   **DPO** is selected as the method for incorporating human feedback because of its effectiveness and simplicity.\n    *   The goal is to adjust the model to generate preferred data while avoiding the generation of non-preferred data, given human preference data under the same conditions.\n    *   A reference policy (reference model) is introduced to stabilize training and prevent the current policy from deviating too far.\n    *   The policy objective is formulated as:\n\n        $L_{DPO} = -E_{(y, x_w, x_l) \\sim D} \\left[ log \\sigma \\left( \beta \\left( log \frac{\\pi_\theta(x_w|y)}{\\pi_{ref}(x_w|y)} - log \frac{\\pi_\theta(x_l|y)}{\\pi_{ref}(x_l|y)} \right) \right) \right]$\n\n        where:\n\n        *   $\\pi_\theta$ and $\\pi_{ref}$ refer to the current policy and reference policy, respectively.\n        *   $x_w$ and $x_l$ are the preferred and non-preferred samples, respectively.\n        *   $y$ denotes the condition (prompt).\n\n3.  **Training Process:**\n    *   At each step, a prompt and its corresponding positive and negative sample pairs are selected.\n    *   Each sample is generated by the model itself to ensure smooth updates and improve training stability.\n    *   The positive and negative samples are aligned by fixing the initial noise and timestep to maintain consistency in the training data, contributing to a more stable training process.\n    *   The training objective is based on the **DiffusionDPO** method but is modified to extend it to the **Flow Matching** framework.\n\n4.  **Reward Model Integration:**\n    *   To address the saturation of improvements when the model easily distinguishes between positive and negative samples, a reward model is trained using human-annotated feedback data.\n    *   This reward model dynamically evaluates the quality of newly generated samples during training and is periodically fine-tuned with newly annotated human feedback to maintain alignment with the evolving policy.\n    *   By integrating the reward model into the pipeline, the training data is scored and ranked on-the-fly (on-policy), thereby improving data efficiency."
    }
]