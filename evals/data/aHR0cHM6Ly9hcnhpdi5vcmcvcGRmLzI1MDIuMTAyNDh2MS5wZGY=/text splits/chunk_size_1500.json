[
    "Step-Video-T2V Technical Report: The Practice,\nChallenges, and Future of Video Foundation Model\nStep-Video Team\nStepFun\nAbstract\nWe present Step-Video-T2V, a state-of-the-art text-to-video pre-trained model\nwith 30B parameters and the ability to generate videos up to 204 frames in length. A\ndeep compression Variational Autoencoder, Video-VAE, is designed for video gen-\neration tasks, achieving 16x16 spatial and 8x temporal compression ratios, while\nmaintaining exceptional video reconstruction quality. User prompts are encoded us-\ning two bilingual text encoders to handle both English and Chinese. A DiT with 3D\nfull attention is trained using Flow Matching and is employed to denoise input noise\ninto latent frames. A video-based DPO approach, Video-DPO, is applied to reduce\nartifacts and improve the visual quality of the generated videos. We also detail\nour training strategies and share key observations and insights. Step-Video-T2V\u2019s\nperformance is evaluated on a novel video generation benchmark, Step-Video-T2V-\nEval, demonstrating its state-of-the-art text-to-video quality when compared with\nboth open-source and commercial engines. Additionally, we discuss the limitations\nof current diffusion-based model paradigm and outline future directions for video\nfoundation models. We make both Step-Video-T2V and Step-Video-T2V-Eval\navailable at https://github.com/stepfun-ai/Step-Video-T2V. The online version can\nbe accessed from https://yuewen.cn/videos as well. Our goal is to accelerate the\ninnovation of video foundation models and empower video content creators.\n1\nPreface\nA video foundation model is a model pre-trained on large video datasets that can generate videos\nin response to text, visual, or multimodal inputs from users. It can be applied to a wide range of\ndownstream video-related tasks, such as text/image/video-to-video generation, video understanding\nand editing, as well as video-based conversion, question answering, and task completion.\nBased on our understanding, we define two levels towards building video foundation models. Level-1:\ntranslational video foundation model. A model at this level functions as a cross-modal translation\nsystem, capable of generating videos from text, visual, or multimodal context. Level-2: predictable\nvideo foundation model. A model at this level acts as a prediction system, similar to large language\nmodels (LLMs), that can forecast future events based on text, visual, or multimodal context and handle\nmore advanced tasks, such as reasoning with multimodal data or simulating real-world scenarios.\nCurrent diffusion-based text-to-video models, such as Sora, Veo, Kling, Hailuo, and Step-Video (as\ndescribed in this report), belong to Level-1. These models can generate high-quality videos from\ntext prompts, lowering the barrier for creators to produce video content. However, they often fail\nto generate videos that require complex action sequences (such as a gymnastic performance) or\nadherence to the laws of physics (such as a basketball bouncing on the floor), let alone performing\ncausal or logical tasks like LLMs. Such limitations arise because these models learn only the mappings\nbetween text prompts and corresponding videos, without explicitly modeling the underlying causal\nrelationships within videos. Autoregression-based text-to-video models introduce the causal modeling\nmechanism by predicting the next video token, frame, or clip. However, these models still cannot\nachieve performance comparable to diffusion-based models on text-to-video generation.\narXiv:2502.10248v1  [cs.CV]  14 Feb 2025\nThis report will detail the practice of building Step-Video-T2V as a state-of-the-art video foundation\nmodel at Level-1. By analyzing the challenges identified through experiments, we will also highlight\nkey problems that need to be addressed in order to develop video foundation models at Level-2.\n2\nIntroduction\nLarge language models (LLMs), as part of Artificial General Intelligence (AGI), has made impressive\nprogress in recent years. These models are capable of understanding human instructions and gener-\nating coherent, fluent responses in natural language. However, language is a symbolic abstraction\nof thought, using words and concepts to represent the world. This abstraction often falls short in\ncapturing the complexity and richness of reality, particularly when it comes to dynamic processes like\nobject motion or the intricate spatial and temporal relationships between entities. As a result, video\ngeneration has emerged as an important frontier in the pursuit of AGI, offering a pathway toward\nbridging these cognitive gaps. Moreover, video content is now the dominant form of communication\nand entertainment online. Build video generation systems capable of generating high-quality videos\ncan significantly lower the barriers for content creators and democratize video production, enabling\neveryone, from amateurs to professionals, to produce compelling video content with minimal effort.\nIn this technical report, we present Step-Video-T2V, a state-of-the-art video foundation model\nwith 30B parameters, capable of generating high-quality videos from text, featuring strong motion\ndynamics, high aesthetics, and consistent content. Like most commercial video generation engines,\nStep-Video-T2V is a diffusion Transformer (DiT)-based model trained using Flow Matching. A\nspecially designed deep compression Variational Auto-encoder (VAE) achieves 16x16 spatial and\n8x temporal compression ratios, significantly reducing the computational complexity of large-scale\nvideo generation training. Two bilingual text encoders enable Step-Video-T2V to directly understand\nChinese or English prompts. A cascaded training pipeline, including text-to-image pre-training,\ntext-to-video pre-training, supervised fine-tuning (SFT), and direct preference optimization (DPO), is\nintroduced to accelerate model convergence and fully leverage video datasets of varying quality. A\nnew benchmark dataset called Step-Video-T2V-Eval is created for text-to-video generation, which\nincludes 128 diverse prompts across 11 categories, alongside video generation results from several\ntop text-to-video open-source and commercial engines for comparison.\nInsights are gained throughout the entire development of Step-Video-T2V, spanning data, model,\ntraining, and inference. First, text-to-image pre-training is essential for the video generation model to\nacquire rich visual knowledge, including concepts, scenes, and their spatial relationships, providing\na solid foundation for the subsequent text-to-video pre-training stages. Second, text-to-video pre-\ntraining at low resolution is critical for the model to learn motion dynamics. The more stable the",
    "a solid foundation for the subsequent text-to-video pre-training stages. Second, text-to-video pre-\ntraining at low resolution is critical for the model to learn motion dynamics. The more stable the\nmodel is trained during this stage, using as much diverse training data as possible, the easier it\nbecomes to refine and scale the model to higher resolutions and more complex video generation tasks.\nThird, using high-quality videos with accurate captions and desired styles in SFT is crucial to the\nstability of the model and the style of the generated videos. Fourth, video-based DPO can further\nenhance the visual quality by reducing artifacts, ensuring smoother and more realistic video outputs.\nChallenges remain in state-of-the-art video foundation models. For example, current video captioning\nmodels still face hallucination issues, leading to unstable training and poor instruction-following\nperformance. Composing multiple concepts with low occurrence in the training data (e.g., an elephant\nand a penguin) within a single generated video is still a difficult task. Additionally, training and\ngenerating long-duration, high-resolution videos still face significant computational cost hurdles. Fur-\nthermore, even a DiT-based model like Step-Video-T2V with 30B parameters struggles to generalize\nwell when generating videos involving complex action sequences or requiring adherence to the laws\nof physics. By open-sourcing Step-Video-T2V, we aim to provide researchers and engineers with a\nstrong baseline, helping them better understand these challenges and accelerate innovations in the\ndevelopment and application of video foundation models.\nThe key contributions of this technical report are as follows:\n\u2022 We present and open-source Step-Video-T2V, a state-of-the-art text-to-video pre-trained model\nwith 30B parameters, capable of understanding both Chinese and English prompts, generating\nhigh-quality videos (544x992 resolution) up to 204 frames in length, featuring strong motion\ndynamics, high aesthetics, and consistent content.\n2\n\u2022 We introduce a deep compression Video-VAE for video foundation models, achieving 16x16 spatial\nand 8x temporal compression ratios, while maintaining exceptional video reconstruction quality.\n\u2022 We detail the optimizations of model hyper-parameters, operators, and parallelism in Step-Video-\nT2V, which ensure both the stability and efficiency of training from a system-level perspective.\n\u2022 We describe the process of pre-processing large-scale videos as training data, and explain how\nthese videos are filtered and utilized at different stages of training.\n\u2022 We release Step-Video-T2V-Eval as a new benchmark, which includes 128 diverse prompts across\n11 categories and video generation results from top open-source and commercial engines.\n\u2022 We discuss the insights and challenges encountered in the development of Step-Video-T2V, and\nidentify key issues that must be addressed to advance video foundation models.\n3\nRelated Work\nVideo generation technology has seen significant progress over the past year, with advancements\nfrom Sora OpenAI [2024] to Gen-3 RunwayML [2024], Kling Kuaishou [2024], Hailuo MiniMax\n[2024], Veo DeepMind [2024], and others.\nCommercial video generation engines (e.g., Sora, Gen-3, Kling, and Hailuo) offer text-to-video\ngeneration capabilities, as well as extended applications like image-to-video generation or specialized\nvideo effect generation. Compared to these closed-source engines, which often involve longer and\nmore complex video generation pipelines with extensive pre- and post-processing, Step-Video-T2V\ndelivers comparable performance for general text prompts and even surpasses them in specific\ndomains, such as generating videos with high motion dynamics or text content.\nOpen-source video generation models, such as HunyuanVideo Kong et al. [2025], CogVideoX Yang\net al. [2024a], Open-Sora Zheng et al. [2024], and Open-Sora-Plan Lin et al. [2024], offer greater\ntransparency in their implementations, making them more accessible to researchers and content\ncreators. Both HunyuanVideo and CogVideoX are based on MMDiT Esser et al. [2024], a variation\nof the full attention Transformer architecture. Open-Sora and Open-Sora-Plan are built on DiT\nPeebles and Xie [2023], with the former using spatial-temporal attention and the latter employing\nfull attention. Compared to these open-source models, the key contributions of Step-Video-T2V\ninclude being the largest open-source model to date, utilizing a high-compression VAE for videos,\nsupporting bilingual text prompts in both English and Chinese, implementing a video-based DPO\napproach to further reduce artifacts and enhance visual quality, and providing comprehensive training\nand inference documentation, as outlined in this report.\nMovie Gen Video Polyak et al. [2024] is another video generation model from Meta, featuring a\nsimilar architecture and model size. Compared to Movie Gen Video, Step-Video-T2V stands out with\nfour unique features. First, it incorporates a more powerful high compression VAE for large-scale\nvideo generation training. Second, it supports bilingual text prompt understanding in both English\nand Chinese. Third, it adds an additional DPO stage to the training process, reducing artifacts\nand improving the visual quality of the generated videos. Fourth, it is open-source and provides\nstate-of-the-art video generation quality comparing with both open-source and commercial engines.\nVideos encompass both spatial and temporal information, leading to significantly larger data volumes\ncompared to images. Addressing the computational challenge of modeling video data efficiently is\ntherefore a fundamental problem. Various methods have been proposed to reduce the complexity of\nvideo modeling. These include approaches such as 3D Causal Convolution [Yu et al., 2024, Yang\net al., 2024a, Kong et al., 2025, Zheng et al., 2024], wavelet transform [Nvidia, 2025, Li et al., 2024a],\nand Residual Autoencoding in images [Chen et al., 2025]. While these methods show promise in\nterms of either reconstruction quality or compression ratio, achieving a balance between high quality\nand effective compression remains difficult. Our work addresses this challenge, providing a solution\nthat opens new possibilities in video generation, such as extending the context length or scaling up\nthe DiT model more aggressively.\n4\nModel\nThe overall architecture of Step-Video-T2V is given in Figure 1. Videos are represented by a high-\ncompression Video-VAE, achieving 16x16 spatial and 8x temporal compression ratios. User prompts\n3\nFigure 1: Architecture overview of Step-Video-T2V. Videos are represented by a high-compression",
    "compression Video-VAE, achieving 16x16 spatial and 8x temporal compression ratios. User prompts\n3\nFigure 1: Architecture overview of Step-Video-T2V. Videos are represented by a high-compression\nVideo-VAE, achieving 16x16 spatial and 8x temporal compression ratios. User prompts are encoded\nusing two bilingual pre-trained text encoders to handle both English and Chinese. A DiT with 3D full\nattention is trained using Flow Matching and is employed to denoise input noise into latent frames,\nwith text embeddings and timesteps serving as conditioning factors. To further enhance the visual\nquality of the generated videos, a video-based DPO approach is applied, which effectively reduces\nartifacts and ensures smoother, more realistic video outputs.\nare encoded using two bilingual pre-trained text encoders to handle both English and Chinese. A\nDiT with 3D full attention is trained using Flow Matching [Lipman et al., 2023] and is employed to\ndenoise input noise into latent frames, with text embeddings and timesteps serving as conditioning\nfactors. To further enhance the visual quality of the generated videos, a video-based DPO approach is\napplied, which effectively reduces artifacts and ensures smoother, more realistic video outputs.\nNext, we will introduce the implementation details of Video-VAE, bilingual text encoders, DiT with\n3D full attention, and Video-DPO, respectively.\n4.1\nVideo-VAE\n4.1.1\nLatent Space Compression in Video Generation\nState-of-the-art video models, such as HunyuanVideo [Kong et al., 2025], CogVideoX [Yang et al.,\n2024a], and Meta Movie Gen [Polyak et al., 2024], leverage Variational Autoencoders (VAEs) with\nspatial-temporal downscaling factors of 4\u00d78\u00d78 or 8\u00d78\u00d78. These VAEs map 3-channel RGB inputs\nto 16-channel latent representations, achieving compression ratios as high as 1:96. To further reduce\nthe number of tokens, these systems typically employ patchifiers that group 2\u00d72\u00d71 latent patches\ninto individual tokens.\nWhile this two-stage process of compression and tokenization is effective, it introduces architectural\ncomplexity and can potentially degrade the performance of the subsequent diffusion stages. The\nefficiency of text-to-video diffusion-transformer models is fundamentally dependent on their ability\nto operate within compressed latent spaces. Given that computational costs scale quadratically with\nthe number of tokens due to attention operations, it is crucial to mitigate spatial-temporal redundancy\nthrough effective compression. This not only accelerates training and inference but also aligns with\nthe diffusion process\u2019s inherent preference for condensed representations.\n4\nRes3DModule\nRes3DModule\nDownSample\nRes3DModule\nDownSample\nMidBlock\nRes+Attn+Res\nConv Path\nShortcut Path\nAdd\nConv Path\nShortcut Path\nAdd\nConv Path\nShortcut Path\nAdd\nConv Path\nShortcut Path\nAdd\nConv in\nMidBlock\nRes+Attn+Res\nRes3DModule\nRes3DModule\nUpSample\nRes3DModule\nUpSample\nConv out\nZ\nX\nZ\nX\nDecoder\nEncoder\nFigure 2: Architecture overview of Video-VAE.\n4.1.2\nAdvancing Compression through New Architecture\nOur Video-VAE introduces a novel dual-path architecture at the later stage of the encoder and the\nearly stage of the decoder, featuring unified spatial-temporal compression. This design achieves\n8\u00d716\u00d716 downscaling through the synergistic use of 3D convolutions and optimized pixel unshuf-\nfling operations. For an input video tensor X \u2208RB\u00d7C\u00d7T \u00d7H\u00d7W , the encoder E produces latent\nrepresentation Z = E(X) \u2208RB\u00d7Cz\u00d7\u2308T/8\u2309\u00d7\u2308H/16\u2309\u00d7\u2308W/16\u2309through:\nCausal 3D Convolutional Modules\nThe early stage of the encoder consists of three stages, each\nfeaturing two Causal Res3DBlock and corresponding downsample layers. Following this, a MidBlock\ncombines convolutional layers with attention mechanisms to further refine the compressed represen-\ntations. To enable joint image and video modeling, we employ temporal causal 3D convolution. Our\narchitecture implements temporal causality through:\nC3D(X)t =\n\u001aConv3D([0, ..., Xt], \u0398)\nt = 0\nConv3D([Xt\u2212k, ..., Xt], \u0398)\nt > 0\n(1)\nwhere k is the temporal kernel size, ensuring frame t only depends on previous frames.\nDual-Path Latent Fusion\nThe primary motivation for Dual-Path Latent Fusion is to maintain\nhigh-frequency details through convolutional processing while preserving low-frequency structure\nvia channel averaging. Notably, Chen et al. [2025] identify similar mechanisms within the realm\nof image VAE modeling. Our approach, however, introduces a unified structure adept at handling\nboth image and video data. This approach allows the network to use its parameters more efficiently,\nthereby overcoming the blurring artifacts typically associated with traditional VAEs.\n1. Conv Path: Combines causal 3D convolutions with pixel unshuffling,\nHconv = U(3)\ns\n(C3D(X))\n(2)\nwhere U(3)\ns\n: RB\u00d7C\u00d7 T\nst \u00d7 H\nss \u00d7 W\nss \u2192RB\u00d7C\u00b7s3\u00d7 T\nst \u00d7 H\nss \u00d7 W\nss with spatial stride ss = 2, temporal\nstride st = 2, and C3D denoting our causal 3D convolution.\n2. Shortcut Path: Preserves structural semantics through grouped channel averaging,\nHavg = 1\ns3\ns3\u22121\nX\nk=0\nU(3)\ns (X)[...,kCz:(k+1)Cz]\n(3)\nwhere U(3)\ns\nimplements 3D pixel unshuffle with spatial-temporal blocking, Cz is the latent dim of\nnext stage.\n5\nThe output of fusion combines both paths through residual summation:\nZ = Hconv \u2295Havg\n(4)\n4.1.3\nDecoder Architecture\nThe early stage of the decoder consists of two symmetric Dual Path architectures. In these architec-\ntures, the 3D pixel unshuffle operation U is replaced by 3D pixel shuffle operator P, the grouped\nchannel averaging path is replaced by a grouped channel repeating operation, which efficiently",
    "tures, the 3D pixel unshuffle operation U is replaced by 3D pixel shuffle operator P, the grouped\nchannel averaging path is replaced by a grouped channel repeating operation, which efficiently\nunfolds the compressed information into spatial-temporal dimensions. In ResNet backbone, we\nreplace all groupnorm with spatial groupnorm to avoid temporal flickering between different chunks.\n4.1.4\nTraining Details\nOur VAE training process is meticulously designed in multiple stages, which is the key reason for\nachieving our final goal of efficient and high-quality video data modeling.\nIn the first stage, we train a VAE with a 4x8x8 compression ratio without employing a dual-path\nstructure. This initial training is conducted jointly on images and videos of varying frame counts,\nadhering to a preset ratio. In this stage, we set a lower compress goal for model to sufficiently learn\nlow level representations.\nIn the second stage, we enhance the model by incorporating two dual-path modules in both the\nencoder and decoder, replacing the latter part after the mid-block. During this phase, we gradually\nunfreeze the dual-path modules, the mid-block, and the ResNet backbone, allowing for a more refined\nand flexible training process.\nThroughout the training, we utilize a combination of L1 reconstruction loss, Video-LPIPS, and\nKL-divergence constrain to guide the model. Once these losses have converged, we introduce GAN\nloss to further refine the model\u2019s performance. This staged approach ensures a robust and high-quality\nVAE capable of handling complex video data efficiently.\n4.2\nBilingual Text Encoder\nThe text encoder plays a crucial role in text-to-video generation by guiding the model in the latent\nspace. In Step-Video-T2V, we use two bilingual text encoders to process user text prompts: Hunyuan-\nCLIP and Step-LLM.\nHunyuan-CLIP is the bidirectional text encoder of an open-source bilingual CLIP model Li et al.\n[2024b]. Due to the training mechanism of the CLIP model, Hunyuan-CLIP can produce text\nrepresentations well-aligned with the visual space. However, because its maximum input length is\nlimited to 77 tokens, Hunyuan-CLIP faces challenges when processing longer user prompts.\nStep-LLM, on the other hand, is an in-house, unidirectional bilingual text encoder pre-trained using\nthe next-token prediction task. It incorporates a redesigned Alibi-Positional Embedding Press et al.\n[2022] to improve both efficiency and accuracy in sequence processing. Unlike Hunyuan-CLIP,\nStep-LLM has no input length restriction, making it particularly effective for handling lengthy and\ncomplex text sequences.\nBy combining these two text encoders, Step-Video-T2V is able to handle user prompts of varying\nlengths, generating robust text representations that effectively guide the model in the latent space.\n4.3\nDiT w/ 3D Full Attention\nLayers\nAttention Heads\nHead Dim\nFFN Dim\nCross-Attn Dim\nActivation Function\nNormalization\n48\n48\n128\n24,576\n(6,144, 1,024)\nGELU-approx\nRMSNorm\nTable 1: Hyper-parameters used in Step-Video-T2V.\nStep-Video-T2V is built on the DiT Peebles and Xie [2023] architecture, which consists of 30B\nparameters and contains 48 layers. Each layer contains 48 attention heads, with each head\u2019s dimension\nset to 128. The setting of hyper-parameters used in Step-Video-T2V is outlined in Table 1.\n6\nFigure 3: The model architecture of our bilingual text encoder and DiT with 3D Attention.\n3D Full-Attention: We employ the 3D full-attention in Step-Video-T2V instead of the spatial-temporal\nattention, which is more computationally efficient. This choice is driven by its theoretical upper\nbound for modeling both spatial and temporal information in videos, as well as its superiority in\ngenerating videos with smooth and consistent motion observed from large-scale experiments.\nCross-Attention for Text Prompt: We introduce a cross-attention layer between the self-attention\nand feed-forward network (FFN) in each transformer block to incorporate text prompts. This layer\nenables the model to attend to textual information while processing visual features. The prompt is\nembedded using two distinct bilingual text encoders, Hunyuan-CLIP and Step-LLM, as described in\n\u00a74.2. The outputs from these two encoders are concatenated along the sequence dimension, creating\nthe final text embedding sequence. This combined embedding is then injected into the cross-attention\nlayer, allowing the model to generate videos conditioned on the input prompt.\nAdaptive Layer Normalization (AdaLN) with Optimized Computation: In standard DiT, each block\nincludes an adaptive layer normalization (AdaLN) operation to embed timestep and class label\ninformation. Since the text-to-video task does not require class labels, we remove class labels from\nAdaLN. Furthermore, we follow Chen et al. [2023] and adopt the AdaLN-Single structure to reduce\nthe computational overhead of traditional AdaLN operations and improve overall model efficiency. In\nthe first layer of the model, AdaLN uses an MLP block to embed timestep information. In subsequent\nlayers, a learnable parameter is initialized to summarize the timestep embeddings, which are then\nused as parameters for the adaptive normalization in each block.\nRoPE-3D: We use RoPE-3D, an extension of the traditional Rotation-based Positional Encoding\n(RoPE) Su et al. [2023], specifically designed to handle video data by accounting for temporal (frame)\nand spatial (height and width) dimensions. The original RoPE-1D applies a rotational transformation\nto positional encodings to enable flexible and continuous representation of positions in sequences\nof varying lengths. The rotational transformation is applied by rotating the positional encoding\nEi at position i by an angle \u03b8i = 2\u03c0i\n\u03c4 , where \u03c4 is a period controlling the rotation rate, and the\n7\nresulting encoding Pi = Rot(Ei, \u03b8i) is obtained. To extend this to video data, we introduce RoPE-\n3D. This method splits the query and key tensors along the channel dimension, applying RoPE-1D\nindependently to each tensor for the temporal (frame) and spatial (height and width) dimensions.\nThe resulting encodings are then concatenated. This approach enables the model to handle video",
    "independently to each tensor for the temporal (frame) and spatial (height and width) dimensions.\nThe resulting encodings are then concatenated. This approach enables the model to handle video\ninputs with varying lengths and resolutions effectively. RoPE-3D offers several advantages, such as\nthe ability to process videos with different frame counts and resolutions without being restricted by\nfixed positional encoding lengths. It improves generalization across diverse video data and effectively\ncaptures both spatial and temporal relationships within the video. By providing a continuous and\nflexible encoding for three-dimensional video data, RoPE-3D enhances the model\u2019s capacity to\nprocess and generate high-quality video content.\nQK-Norm: We use Query-Key Normalization (QK-Norm) to stabilize the self-attention mechanism.\nQK-Norm normalizes the dot product between the query (Q) and key (K) vectors, addressing\nnumerical instability caused by large dot products that can lead to vanishing gradients or overly\nconcentrated attention. This normalization ensures stable attention during training, accelerates\nconvergence, and improves efficiency, allowing the model to focus on learning meaningful patterns.\nAdditionally, QK-Norm helps maintain a balanced distribution of attention weights, enhancing the\nmodel\u2019s ability to capture relationships within the input sequence.\n4.3.1\nTraining Objective for Video and Image Generation\nWe use Flow Matching in the training of Step-Video-T2V. At each training step, we begin by sampling\na Gaussian noise, X0 \u223cN(0, 1), and a random timestep t \u2208[0, 1]. We then construct the model\ninput Xt as a linear interpolation between X0 and X1, where X1 is the target sample corresponding\nto the noise-free input. Specifically, we define Xt as: Xt = (1 \u2212t) \u00b7 X0 + t \u00b7 X1. The ground truth\nvelocity Vt, which represents the rate of change of Xt with respect to the timestep t, is defined as:\nVt = dXt\ndt\n= X1 \u2212X0.\n(5)\nIn other words, Vt captures the direction and magnitude of change from the initial noise X0 to the\ntarget data X1. The model is then trained by minimizing the mean squared error (MSE) loss between\nthe predicted velocity u(Xt, y, t; \u03b8) and the true velocity Vt. Here, u(Xt, y, t; \u03b8) denotes the model\u2019s\npredicted velocity at timestep t, given input Xt and an optional conditioning input y (e.g., a bilingual\nsentence). The training loss is given by:\nloss = Et,X0,X1,y\n\u0002\n\u2225u(Xt, y, t; \u03b8) \u2212Vt\u22252\u0003\n,\n(6)\nwhere the expectation is taken over all training samples, with t being the random timestep, and X0,\nX1, and y drawn from the dataset. The term \u03b8 represents model parameters. This approach ensures\nthat the model learns to predict the instantaneous rate of change of the noisy sample Xt with respect\nto t, which can later be used to reverse the diffusion process and recover data samples from noise.\n4.3.2\nInference\nDuring inference, we begin by sampling random noise X0 \u223cN(0, 1). The goal is to recover the\ndenoised sample X1 by iteratively refining the noise through an ODE-based method. For simplicity,\nwe adopt a Gaussian solver and define a sequence of timesteps {t0, t1, . . . , tn}, where t0 = 0, tn = 1,\nand t0 < t1 < \u00b7 \u00b7 \u00b7 < tn. The denoising process is then carried out by integrating over these timesteps.\nSpecifically, the denoised sample X1 can be expressed as:\nX1 =\nn\u22121\nX\ni=0\nu(Xti, y, ti; \u03b8) \u00b7 (ti+1 \u2212ti),\n(7)\nwhere u(Xti, y, ti; \u03b8) represents the predicted velocity at timestep ti, given the noisy sample Xti\nand an optional conditioning input y. The integral is computed over the timesteps from t0 to tn,\nwith each term u(Xti, y, ti; \u03b8) multiplied by the corresponding timestep difference (ti+1 \u2212ti). This\niterative process allows the model to gradually denoise the input sample, starting from the noise X0\nand progressing toward the target sample X1 over the defined timesteps.\n4.4\nVideo-DPO\nThe integration of human feedback has been widely validated in the domain of LLMs, particularly\nthrough methods such as Reinforcement Learning with Human Feedback (RLHF) Ouyang et al.\n8\nPrompt Pool\nTraining data\nHandcrafted\nby annotators\nStep-Video-T2V\n\"A ballet dancer\npracticing in the\ndance studio\"\nreward model\nHuman\u00a0\nAnnotation\nFeedback\nFigure 4: Overall pipeline of incorporating human feedback.\n[2022], Christiano et al. [2017], where models adjust their generated content based on human\nfeedback. Recently, this practice has also been applied in image and video generation, yielding\nsignificant advancements. To improve the visual quality of Step-Video-T2V, we design a pipeline to\nintroduce human feedback. The overall pipeline is shown in Figure 4, and details are discussed in the\nfollowing.\nIn Step-Video-T2V, we select Direct Preference Optimization (DPO) Rafailov et al. [2024] as the\nmethod for incorporating human feedback. It has been proven effective across a variety of generation\ntasks Wallace et al. [2024], Yang et al. [2024b], and the essence of the method is simple, making it\nboth intuitive and easy to implement. Intuitively, given human preference data and non-preference\ndata under the same conditions, the goal is to adjust the current policy (i.e., the model) to be more\naligned with the generation of preferred data, while avoiding the generation of non-preferred data. To\nstabilize training, the reference policy (i.e., the reference model) is introduced to prevent the current\npolicy from deviating too far from the reference policy. The policy objectvie can be formulated as:\nLDPO = \u2212E(y,xw,xl)\u223cD\n\u0014\nlog \u03c3\n\u0012\n\u03b2\n\u0012\nlog \u03c0\u03b8(xw|y)\n\u03c0ref(xw|y) \u2212log \u03c0\u03b8(xl|y)",
    "\u0014\nlog \u03c3\n\u0012\n\u03b2\n\u0012\nlog \u03c0\u03b8(xw|y)\n\u03c0ref(xw|y) \u2212log \u03c0\u03b8(xl|y)\n\u03c0ref(xl|y)\n\u0013\u0013\u0015\n(8)\nwhere \u03c0\u03b8 and \u03c0ref refers to current policy and reference policy, respectively, xw and xl are the\npreferred sample and non-preferred sample, and y denotes the condition.\nTo collect these samples (xw, xl given y) for training, we construct a diverse prompt set. First, we\nrandomly select a subset of prompts from the training data to ensure prompt diversity. Second, we\ninvite human annotators to synthesize prompts based on a carefully designed guideline that mirrors\nreal-world user interaction patterns. And then, for each prompt, Step-Video-T2V generates multiple\nvideos using different seeds. Human annotators rate the preference of these samples. The annotation\nprocess is monitored by quality control personnel to ensure accuracy and consistency. This process\nresults in a set of preference and non-preference data, which serves as the foundation for model\ntraining. Two labeled examples are shown in Figure 5.\nAt each training step, we select a prompt and its corresponding positive and negative sample pairs\ndescribed above. Each sample is generated by the model itself, ensuring smooth updates and\nimproving overall training stability. In addition, to maintain consistency in the training data, we\nalign the positive and negative samples by fixing the initial noise and timestep, which contributes\nto a more stable training process. Our training objective in Eqn. 8 is based on the DiffusionDPO\nmethod Wallace et al. [2024] and DPO Rafailov et al. [2024] but with slight modifications, extending\nit to the Flow Matching framework. By denoting the policy-related terms in Eqn. 8 as inside term z,\nit can be derived that:\n\u2202LDPO\n\u2202\u03b8\n\u221d\u2212\u03b2(1 \u2212\u03c3(\u03b2z)) \u00b7 \u2202z\n\u2202\u03b8 ,\n(9)\nwhich indicates large \u03b2 (e.g., 5,000 in DiffusionDPO) may cause gradient explode when z < 0, as it\namplifies gradients by \u03b2 times. As a result, gradient clipping and an extreme low learning rate (e.g.,\n1e-8 in DiffusionDPO) are required to ensure stable training, leading to slow convergence.To address\nthis, we reduce \u03b2 and increase the learning rate, results much faster convergence.\nHuman feedback effectively improves visual quality. However, we observe that the improvements\nsaturate when the model can easily distinguish between positive and negative samples. This phe-\nnomenon may stem from the following reason: the training data used in Video-DPO is generated\n9\n(a) an example of non-preferred data\n(b) an example of preferred data\nFigure 5: We generate different samples with same prompt (\"A ballet dancer practicing in the dance\nstudio\" in this case), and annotate these samples as non-preferred (a) or preferred (b).\nby earlier versions of the model. After multiple iterations of DPO, the current policy has evolved\nsignificantly (e.g., distortions are now rare) and no longer aligns with the policies from previous\niterations. Consequently, updating the current policy with outdated data from earlier iterations leads\nto inefficient data utilization. To address this, we propose training a reward model using human-\nannotated feedback data. This reward model dynamically evaluates the quality of newly generated\nsamples during training. The reward model is periodically fine-tuned with newly annotated human\nfeedback to maintain alignment with the evolving policy. By integrating it into the pipeline, we score\nand rank training data on-the-fly (on-policy), thereby improving data efficiency.\n5\nDistillation for Step-Video-T2V Turbo\nDiffusion models for video generation typically require substantial computational resources during\ninference, often necessitating more than 50 steps of ODE integration to produce a video. Reducing the\nnumber of function evaluations (NFE) is crucial for improving inference efficiency. We demonstrate\nthat a large-scale trained Video DiT can reduce NFE to as few as 8 steps with negligible performance\ndegradation. This is achieved through self-distillation with a rectified flows objective and a specifically\ndesigned inference strategy.\nOur base model is trained using rectified flow, and the distillation objective aims to train a 2-rectified-\nflow model [Liu et al., 2022], which facilitates more direct ODE paths during inference. As discussed\nby Lee et al. [2024], the loss function for the 2-rectified flow can be formulated as follows:\nL(\u03b8, t) := 1\nt2 E[\u2225v \u2212u\u03b8(xt, t)\u22252\n2] = 1\nt2 E[\u2225x \u2212E[x|xt]\u22252\n2] + \u02dcL(\u03b8, t).\n(10)\nSince all training samples are generated by the base 1-rectified model, the irreducible loss (first term)\nis relatively small. The reducible error (second term) can be efficiently optimized by assigning more\nweight to timesteps that are more challenging. Specifically, the training loss of 2-rectified flow is\nlarge at each end of the interval t \u2208[0, 1] and small in the middle.\nWe sampled approximately 95,000 data samples using a curated distribution of SFT data prompts with\n50 NFE and carefully designed positive and negative prompts to formulate a distillation dataset. We\nmodified the timestep sampling strategy to a U-shaped distribution, specifically pt(u) \u221dexp(au) +\nexp(\u2212au) on u \u2208[0, 1], with a larger a = 5 as the time shift required by the video model is higher.\nDuring inference, we observed that as the training progresses, the model requires more significant\nsampling time shifts and a lower classifier-free guidance (CFG) scale. By combining this with a\nlinear diminishing CFG schedule as described in Eqn. 11, our model can achieve comparable sample\nquality with up to 10 times fewer steps. Figure 6, shows generated samples with 204 frames from our\nturbo model with 10 NFE.\n10\ncfgt = max(cfgmax \u22129t(cfgmax \u22121), 1)\nfor\n0 \u2264t \u22641\n(11)\nFigure 6: Generated samples with Step-Video-T2V Turbo with 10 NFE.\n6\nSystem\nThis section describes our infrastructure that facilitates the efficient and robust training of Step-Video-\nT2V at scale. The discussion starts with a comprehensive system overview, providing a holistic\nperspective of the workflow, followed by an in-depth examination of each constituent component.",
    "T2V at scale. The discussion starts with a comprehensive system overview, providing a holistic\nperspective of the workflow, followed by an in-depth examination of each constituent component.\nFurthermore, we present our insights and practical experiences gained from our training platform\nimplementation and routine operational management.\n6.1\nOverview\nFigure 7 shows the overall workflow of Step-Video-T2V training system. The workflow comprises\nseveral stages. The offline stage, based on our in-house training emulator (Step Emulator, \u00a76.2), is\nspecifically designed to estimate and determine optimal resource allocation and training parallelism\nstrategies. This determination is achieved through systematic analysis of model architectures and\nresource configurations provided as input parameters. Next, with the theoretical optimal resource\nallocation plan, we deploy the training job with GPUs allocated in the training and inference clusters,\nrespectively. The training clusters, responsible for training video DiT, uses the parallelization strategy\nrecommended by the emulator, which has been specifically optimized to maximize the Model Flops\nUtilization (MFU). The other side with VAE and Text Encoder, runs on the inference clusters, and\nconstantly provides the processed input data (pairs of image/video latent and text embedding) needed\nfor DiT training. Data transmission between clusters is facilitated by StepRPC (\u00a76.3), our high-\nperformance RPC framework that seamlessly integrates both TCP and RDMA protocols, enabling\nefficient cross-cluster communication with robust fault tolerance.\nTo enable systematic monitoring and analysis during large-scale training, we implement a dual-layer\nmonitoring approach through StepTelemetry (\u00a76.4). This system collects detailed data statistics\nfrom inference clusters while simultaneously collecting iteration-level, fine-grained performance\nmetrics from training clusters. The resulting telemetry data provides multidimensional insights into\nthe training system, enabling precise identification of algorithmic patterns and systematic detection\nof potential performance bottlenecks across the entire infrastructure.\nWe have constructed a datacenter comprising thousands of NVIDIA H800 GPUs interconnected by a\nrail-optimized RoCEv2 fabric (1.6Tbps bandwidth per node). Nodes of the datacenter can be dynami-\ncally assigned to inference clusters or training clusters according to GPU resource requirements. To\nsupport a single large-scale training job with thousands of GPUs spanning multiple GPU clusters\nconcurrently, we have gained valuable insights from addressing challenges related to the training\nplatform (StepMind) and its operational complexities. A detailed examination of these findings,\n11\nText Encoder \nServer\nVAE\nVAE\nVAE \nServer\nVideo \nDiT\nVideo \nDiT\nVideo \nDiT\nInference Clusters\nTraining Clusters\nStepRPC\nModel \nConfig\nTraining \nEmulator\nResource \nConfig\nResource \nPlan\nParallelism \nStrategy\noptimal training parallelism\nImage/Video \nLatent\nText Embed\noptimal GPU \nalloc ratio\nStep Emulator\nStepTelemetry\nData Statistics\nFine-grained  \nperf monitor\nTraining \nPlatform\nStepMind\nFigure 7: The workflow of Step-Video-T2V training system.\nincluding specific implementation strategies and best practices, will be presented in \u00a76.5. Through\ncomprehensive improvements to infrastructure reliability, we have achieved 99% effective GPU\ntraining time over more than one month.\n6.2\nTraining Framework Optimizations\n6.2.1\nStep Emulator\nThe large model size and extended context length of video require partitioning both the model\nparameters and activations/gradients across devices using multiple parallelism strategies during\ntraining, such as Tensor-parallelism (TP), Sequence-parallelism (SP), Context-parallelism (CP),\nPipeline-parallelism (PP) and Virtual Pipeline-parallelism (VPP) Narayanan et al. [2021], Korthikanti\net al. [2023], Liu et al. [2023], Jacobs et al. [2023]. However, the large scale of GPU cluster required\nfor DiT training poses significant challenges in tuning and validating architecture designs and\noptimizations. To address this, we developed Step Emulator (SEMU), a highly accurate and efficient\nsimulator designed to estimate resource consumption and end-to-end performance during training,\nunder various model architecture and parallelism configurations. Specifically, to accommodate the\ndynamic and mixed input data for DiT training, SEMU allows customization of input data with\nvarying frames and resolutions. SEMU helps to design model parameters, architecture and the\nassociated optimal parallelism strategies. It also determines the resource allocation of inference (i.e.,\ntext-encoder and VAE) and training (i.e., video DiT) clusters before the training actually starts.\n6.2.2\nDistributed Training\nParallelism Strategy\nTable 2 outlines the MFU of different configurations for 540P video pre-\ntraining obtained by SEMU. As shown in the table, simply applying PP on top of TP does not achieve\na high MFU. This is because PP only reduces memory usage for model parameters and gradients by\nabout 20GB after 8-way TP, and it can only disable a small portion of activation checkpointing, given\nthe 120GB of activation memory. While CP directly reduces activation memory, its communication\ncost through the NIC is comparable to the TP cost via NVLink. To reduce CP overhead, we apply\nhead-wise CP Jacobs et al. [2023] to the self-attention block, leveraging the MHA in the DiT model,\nand sequence-wise CP Liu et al. [2023] to the cross-attention block, due to the relatively short\nsequence length of k and v from the prompts. Despite these optimizations, the CP cost remains\nnon-negligible, and relying solely on CP does not lead to a high MFU.\nAs a result, the optimal MFU is always achieved by combining TP, CP, PP, and VPP. However, for\nlarge-scale GPU cluster training, it is crucial to keep the backend framework as simple as possible for\nrobustness and easy identification of stragglers during training. This hinders us from adopting PP\nsince it generally lacks the necessary flexibility. As a trade-off, we adopt an 8-way tensor parallelism\n(TP) strategy combined with sequence parallelism (SP) and Zero1 Rajbhandari et al. [2020]. This\nconfiguration results in a MFU that is marginally lower (-0.88%) than the theoretical optimum. In\npractice, the actual training MFU reaches 32%, which is slightly below the estimated value due to\nmetric collection overhead and minor delays caused by stragglers.\nTP overlap\nTo minimize TP overhead, we have developed StepCCL, a proprietary collective\ncommunication library that implements advanced communication-computation overlap techniques.",
    "metric collection overhead and minor delays caused by stragglers.\nTP overlap\nTo minimize TP overhead, we have developed StepCCL, a proprietary collective\ncommunication library that implements advanced communication-computation overlap techniques.\nStepCCL directly utilizes the DMA engine for data transmission, completely bypassing the Stream\n12\nTP\nCP\nPP\nVPP\nCheckpointing (%)\nMFU\n4\n1\n2\n24\n93.75\n35.90\n4\n24\n93.75\n35.71\n8\n1\n1\n1\n83.33\n35.59\n2\n24\n72.91\n36.06\n4\n12\n72.91\n35.76\n2\n1\n1\n62.50\n31.79\n4\n12\n31.25\n35.11\n3\n1\n1\n31.25\n33.41\n4\n12\n11.53\n36.47\nTable 2: Estimated MFU from SEMU of different parallelism strategies under 540P video pre-training\nstage.\nMultiprocessors (SMs). This design enables simultaneous execution of StepCCL operations and\nGEMM computations on the same GPU, achieving true concurrency without mutual performance\ninterference, thereby maximizing hardware utilization and computational throughput. More details\ncan be found at Section 7 of Zhang et al. [2024].\nDP overlap\nIn the first two stages (i.e. Image and 192P video pre-training), the context length\nis below 10K and activation memory does not pose a limiting factor. The primary memory usage\nstems from model parameters, which are handled via 8-way TP. While the performance bottleneck\narises from gradient reduce-scatter and parameter all-gather operations introduced by DP, which can\ntake up more than 30% of the training time. To mitigate this, we developed DP overlap, where the\nparameter all-gather is performed during the forward pass of the first micro-batch, while the gradient\nreduce-scatter overlaps with the backward pass of the last micro-batch. Note that in DiT training,\nthe activation norm is typically a key metric in the training process, which registers forward hooks\nfor monitoring. These forward hooks can slow down the kernel launch in forward process, further\nrendering the forward overlap of DP communication ineffective. Therefore, the effectiveness of\nforward overlap may vary depending on the scenario, and the decision to enable it should be made\ncarefully on a case-by-case basis.\n6.2.3\nTailored Computation and Communication Modules\nVAE Computation\nTo accelerate the convolution op (the most compute-intensive) in VAE, we\nemploy the channel-last principle that is more GPU-friendly PyTorch [2023] than naive PyTorch\nimplementation. Specifically, a raw PyTorch tensor uses the NCHW memory format by default, while\nGPU tensorcores only support NHWC format essentially, causing additional format transformation\noverhead that slows down overall speed. We solve this by performing format permutation at the\nbeginning, putting the channel to the last dimension physically. We modify each op (e.g., Conv,\nGroupNorm) along the computation graph to adapt to channel-last format. Overall, we achieve up to\n7x VAE encode throughput with this optimization.\nMulti-GPUs VAE\nTo further reduce VAE latency and also support long, high resolution videos,\nusing multiple GPUs is necessary to reduce computation and memory footprint on a single device.\nWe support both Temporal and Spatial Parallel for the convolution op. As an example, for Temporal\nParallel, we divide the video latent along the frame dimension and let each GPU hold only a subset of\nvideo frames. If a downstream Conv op requires cross-frame computation, we transfer the overlapped\nframes using all-to-all communication. The overhead is typically small (< 1%) compared to the\ncomputation time.\nDiT\nThe plain RoPE implementation Touvron et al. [2023] is inefficient due to the numerous time-\nconsuming slice and concat operations required for building the embedding table and indexing. We\ndeveloped a custom RoPE-3D kernel that replaces these indexing operations with efficient embedding\ncomputation, significantly improving performance. The timestep modulation in the DiT model results\n13\nin high activation memory usage, as the timestep is repeated across the sequence length, which is\nredundant since it remains the same within a single video clip. We implement a memory-efficient\nmodulation operation where the timestep is repeated only during the forward process, and the non-\nrepeated timestep is saved for the backward process. To further reduce memory costs, we fuse the\nLayerNorm op and the downstream timestep modulation op, eliminating the need for saving an\nintermediate output.\n6.2.4\nDP Load Balance\nA critical challenge in large-scale video generation arises when processing mixed-resolution videos\nand images within the same global iteration. Conventional approaches that segregate different\nresolutions into separate batches lead to significant FLOPs disparities across model instances, resulting\nin GPU under-utilization due to load imbalance. Table 3 outlines the FLOPs per sample of different\nresolutions.\nResolution (F, H, W)\nTFLOPs per sample\n204 \u00d7 256 \u00d7 256\n1,717.20\n204 \u00d7 192 \u00d7 320\n1,592.61\n136 \u00d7 256 \u00d7 256\n1,079.85\n136 \u00d7 192 \u00d7 320\n1,004.89\n68 \u00d7 256 \u00d7 256\n509.31\n68 \u00d7 192 \u00d7 320\n475.87\n1 \u00d7 256 \u00d7 256\n44.99\nTable 3: FLOPs per sample of different resolutions.\nTo address this issue, we propose a hybrid-grained load balancing strategy that operates through two\ncomplementary stages, as illustrated in Figure 8. In the first stage, we perform coarse-grained FLOPs\nalignment by adjusting batch sizes of videos with different resolutions. For each resolution r, we\nestimate its FLOPs per sample Fr and compute optimal batch sizes Br through:\nBr =\n\u0016Ftarget\n\u03b1Fr\n\u0017\n(12)\nwhere Ftarget represents the target FLOPs per batch (typically the batch of the highest resolution\nvideos) and \u03b1 is a normalization factor to ensure the consistency of global batch size.\nFLOPs\nBatchs\nVideo Samples Pipe\nImage Samples Pipe\nFigure 8: Load balancing with hybrid granularity.\nThe second stage addresses residual FLOPs variations through fine-grained image padding. Our\nsystem caches N video batches and calculates required image supplements based on a predetermined\nvideo-to-image ratio \u03b2. Using a greedy heuristic, we iteratively allocate images to the batch with the\nsmallest current FLOPs until all supplements are distributed.",
    "video-to-image ratio \u03b2. Using a greedy heuristic, we iteratively allocate images to the batch with the\nsmallest current FLOPs until all supplements are distributed.\nThe hybrid-grained approach effectively balances computational loads while maintaining practical\ndeployability. Our solution requires only superficial awareness of data distribution, needing merely\n14\nbatch size adjustments and supplemental image padding rather than deep architectural changes. This\nminimal intervention preserves the original training pipeline\u2019s integrity while introducing small\nmemory overhead.\n6.3\nStepRPC\nTo facilitate data transfer, we developed StepRPC, a high-performance communication framework.\nStepRPC leverages distributed named pipes as the core programming abstraction, enabling a large\nnumber of servers to communicate seamlessly by declaring pipes with the identical name. The\nspraying mode distributes data evenly across training servers. Compared to existing communication\nframeworks Moritz et al. [2018], Damania et al. [2023], Qin et al. [2024], StepRPC incorporates the\nfollowing essential engineering optimizations.\nTensor-Native Communication over RDMA and TCP\nIn existing frameworks, tensor transfer\ntypically entails heavy-weight serialization and deserialization overheads, amounting to tens of\nmilliseconds. To address the inefficiency, StepRPC implements tensor-native communication that\ndirectly transfers bits within tensor memory, thereby eliminating the overheads associated with serial-\nization and deserialization. StepRPC harnesses the power of RDMA to implement direct transfer\nof both GPU and CPU tensors. When RDMA-capable networks are not available, StepRPC can be\nseamlessly configured to utilize TCP transports. Note that TCP only supports CPU tensors. Conse-\nquently, transferring GPU tensors over TCP introduces additional overheads due to the necessity of\ncopying memory between GPUs and CPUs. To mitigate the overheads, StepRPC proposes to overlap\nCudaMemcpy with TCP send and recv operations. Such optimization hides the latency of memory\ncopying, thereby improving overall communication performance in non-RDMA environments.\nFlexible Workload Patterns with High Resilience\nTo optimize GPU utilization, we leverage\nthe same inference servers to generate data for large-scale pre-training experiments and small-scale\nablation experiments simultaneously. StepRPC facilitates this via a combination of broadcasting\nand spraying communications. First, StepRPC broadcasts the data from inference servers to all\ntraining jobs. This ensures that each job receives the necessary data without redundant computations.\nSecond, within an individual job, StepRPC sprays data to each training server. Though ingesting data\nfrom same inference servers, training jobs can operate independently and elastically with the help\nof StepRPC, meaning that jobs can begin, terminate or scale as needed without affecting the others.\nMeanwhile, StepRPC isolates failures across jobs, preventing cascading effects that could destabilize\nthe entire system.\nEnhanced Visbility for Real-Time Failure Detection and Resource Optimization\nStepRPC\noffers comprehensive performance metrics for deep insights into the communication process. The\nmetrics encompass critical aspects such as data counts, queuing latency and transmission cost. The\nenhanced visibility serves multiple purposes, empowering both operators and researchers to optimize\nperformance and resource utilization effectively. Firstly, by monitoring the counts of produced and\nconsumed data, StepRPC enables real-time failure detection. Discrepancies between these counts can\nindicate potential issues such as data loss, communication failures, or bottlenecks. This proactive\napproach allows operators to promptly identify and address failures. Next, researchers can leverage\nthe metrics like queuing latency and API invoking latency to assess whether inference or training\nprocesses constitute the overall performance bottleneck. Furthermore, armed with the metrics like\nrates of producing and consuming data, researchers can make informed decisions regarding GPU\nresource allocation for inference and training jobs.\n6.4\nStepTelemetry\nThe lack of the observability of training framework makes analyzing it\u2019s inner state and debugging\njob failure difficult. Thus StepTelemetry, an observability suite for training frameworks, is introduced.\nThis suite\u2019s goal is not only to enhance anomaly detection capabilities but also to establish a reusable\npipeline for collecting, post-processing, and analyzing any training-related data.\nStepTelemetry employs a simple and asynchronous data collection pipeline. It offers a Python SDK\nfor easy integration with the training framework, supporting both batch and streaming data writes to\nfiles on local disk. An additional consumer process is responsible for collecting, transforming, and\n15\nwriting data into various remote databases. StepTelemetry benefits Step-Video-T2V training in the\nfollowing aspects.\nAnomaly Detection\nCommon profiling tools like PyTorch Profiler and Megatron-LM Timer in-\ntroduce approximately 10% to 15% overhead, and struggle to support collaborative analysis among\nmultiple ranks. Instead, StepTelemetry adopts a CUDA event-based approach without any unneces-\nsary synchronizations. This enables continuously collecting timer data of all ranks during training\nwith almost zero overhead. By providing various data visualizations and supporting data drill-down,\nStepTelemetry helps pinpointing root cause in case of hardware and software failure. As an example,\nduring one training session, the training efficiency fell below expectations, yet no hardware alerts\nwere triggered. Upon analyzing the collected data, we identified that the backward propagation time\nfor certain ranks was abnormally prolonged. Since the backward process primarily involves tensor\nparallelism (TP) group communication and computation, it is highly probable that the machines\nhosting these ranks were underperforming. After removing these machines from the training cluster,\nthe training efficiency returned to the expected level.\nData Statistics\nDuring video training, it is vital to monitor data consumption. Instead of just\ncounting tokens, it is required to record consumed videos\u2019 metadata. The legacy approach was to\ndump metadata to files on local disk, and then use scripts to parse them offline, which is particularly\ninefficient and inconvenient. By instrumenting dataloader with StepTelemetry, the metadata is\nwritten to database, thus OLAP is enabled. Visualizations such as duplicated data filtering and data\ndistribution monitoring based on source url is provided to researchers, which help evaluating the\nmodel.\nPerformance Optimization\nStepTelemetry provides insight for performance optimization. By\nvisualizing the time consumption of each stage within an iteration, it provides developers with a\ncomprehensive overview, enabling them to identify and optimize performance bottlenecks in critical\npaths. Additionally, dataloader statistics reveal the actual throughput of the training process. Although\nimage and video data are supplied in a mixed manner, the iteration time remained unchanged after\naddressing the data parallelism (DP) imbalance issue. Nevertheless, the observed increase in data\nthroughput demonstrates a significant improvement in system efficiency.\n6.5\nStepMind\nTo ensure high availability of computing resources for large-scale Step-Video-T2V training tasks, we",
    "throughput demonstrates a significant improvement in system efficiency.\n6.5\nStepMind\nTo ensure high availability of computing resources for large-scale Step-Video-T2V training tasks, we\nhave invested substantial efforts into developing StepMind, a distributed training platform designed\nfor large-scale machine learning workloads. StepMind has successfully achieved an effective GPU\nutilization rate exceeding 99.0% for Step-Video-T2V training, primarily through the implementation\nof the following key techniques.\nFine Grained Monitoring at Full Coverage\nTo maximize distributed training efficiency, we\ndeveloped a fine-grained monitoring system at full coverage that rapidly identifies faulty nodes. The\nmonitoring system collects metrics at seconds-granularity across hardware, e.g., CPU/GPU/memo-\nry/PCIe/network/storage/power/fans, and software, e.g., OS stack, enabling rapid and full coverage\nfault detection. Based on our operation experiences, faulty nodes can be generally classified into\ntwo categories: a) Nodes with Fatal Errors (about 86.2% of failures). These nodes can interrupt the\ntraining process immediately. Upon detection of these nodes, we will replace them with healthy nodes\nand restart the job. In order to avoid incorrect restarts due to false alarms, we develop a multi-signal\napproach to ascertain whether a job requires restarting. The signals incorporated in this approach\nencompass RoCEv2 traffic disruption, low GPU power usage, and the cessation of updates in job\ntraining logs. Once being identified as failed, the job will be restarted immediately, thereby reducing\nthe time cost of unavailability resulting from node malfunctions. b) Nodes with Non-Fatal Errors\n(about 13.8% of failures). Although these nodes do not immediately disrupt the training task, they can\ndegrade training efficiency. Detecting such nodes is challenging, and we have developed specialized\nmethods to identify them. These nodes are scheduled for replacement during planned maintenance,\ntypically after a checkpoint is saved, to minimize the wasting time of computational resource. Table 4\nshows more detailed statistics.\n16\nFault\nCategory\nCount\nGPU_DBE_ERROR\nGPU\n3\nGPU_LOCKED\nGPU\n1\nLINK_DOWN\nNetwork\n1\nNODE_SHUTDOWN\nHost\n2\nSOFTWARE_FAULT\nSoftware\n11\nCUDA_OOM\nSoftware\n7\nNON_FATAL\nHardware\n4\nTable 4: Over a month of Step-Video-T2V training, fatal hardware failures occurred only 7 times.\nGPU Machine High Quality Ensurance\nTraining GPU nodes exhibit significant quality variations,\ni.e., their failure probabilities differ substantially. Some servers have much higher failure risks\nthan others, necessitating the selection of the most reliable servers for large-scale training tasks to\nminimize the job interruptions. We developed an innovative node quality assessment framework that\nsystematically integrates historical alert patterns, maintenance logs, stress test results, and load test\ndurations to generate comprehensive quality scores. When node failures occur within production\nresource pools, replacement units are selectively deployed from a dedicated buffer pool following\na prioritized matching rule: buffer machines\u2019 quality scores must meet or exceed the operational\nrequirements of the target resource pool\u2019s priority tier. This methodology has achieved a statistically\nsignificant reduction in failure rates for critical resource pools (i.e., video pool) from an original\nmonthly average of 7.0% to 0.9%. Correspondingly, the daily restart rate per 1,000 GPUs caused by\nhardware issues decreased to approximately 1/11 of that reported in LLaMA3.1 [LlamaTeam, 2024].\nStep-Video-T2V\nLLaMA3.1\nCause of Restart\nHardware\nTotal Unexpected\nHardware\nTotal Unexpected\nAvg Daily Restarts/1k GPUs\n0.037\n0.095\n0.422\n0.485\nTable 5: Restart count statistics during training for Step-Video-T2V and LLaMA3.1 .\nFewer restarts ultimately helped us achieve 99% effective training time over a training period\nexceeding one month.\nEffective Training Time = Total Iteration Time\nTotal Training Time\nCompletely Automated Server Launch Process\nWhen faulty machines are taken offline, they\nmust undergo rapid repairs and meet stringent operational standards before being reintroduced into\nthe service pool. This ensures that defective units do not negatively impact training jobs. Three key\nmeasures are implemented to achieve this:\n\u2022 Automated reboot-repair for transient failures. A large proportion of node failures, approxi-\nmately above 60%, are transient failures. Examples include GPU DBE errors, GPU card discon-\nnections, and network card disconnections. The transient failures can be effectively resolved by a\nsimple restart. To speed up GPU repairing, we\u2019ve created an automated system that quickly reboots\nservers based on the identified failure type. By integrating this reboot system with follow-up health\nchecks and stress tests, we ensure servers can be brought online rapidly and with assured quality.\n\u2022 Comprehensive health checks via extensive diagnostic scripts. We encode human expertise into\nreusable scripts to conduct comprehensive checks on the hardware and software configurations\nof GPU nodes. These checks include GPU, NIC, software driver, and firmware configurations,\nensuring that servers in operation have uniform and correct hardware and software setups. In our\nexperience, this practice prevents nodes with abnormal configurations from running training jobs,\nthereby reducing the likelihood of job interruptions.\n17\nFigure 9: The pipeline of Step-Video-T2V data process.\n\u2022 Rigorous stress testing and admission protocols to validate performance. Our comprehensive\nstress testing ensures each machine delivers peak performance by evaluating two key areas: 1)\nSingle-Machine Performance: We validate GPU AI computing power (TOPS), HBM bandwidth,\nhost-device data transfer speeds (H2D/D2H), and NVLink/PCIe connectivity between GPUs\nto guarantee maximum hardware capability. 2) RDMA Network Verification Using PyTorch\noperations to simulate distributed training patterns (TP/EP/PP/DP), we test real-world network\nperformance. Small-group testing helps swiftly identify faulty nodes, cables, or switches. Cross-\nGPU traffic routing through NICs enables network validation within individual machines for rapid\ntroubleshooting. These tests improve node reliability and performance while preventing job failures,\nsignificantly boosting overall cluster stability and availability.\n7\nData\n7.1\nPre-training Data\nWe constructed a large-scale video dataset comprising 2B video-text pairs and 3.8B image-text pairs.\nLeveraging a comprehensive data pipeline, we transformed raw videos into high-quality video-text\npairs suitable for model pre-training. As illustrated in Figure 9, our pipeline consists of several\nkey stages: Video Segmentation, Video Quality Assessment, Video Motion Assessment, Video",
    "pairs suitable for model pre-training. As illustrated in Figure 9, our pipeline consists of several\nkey stages: Video Segmentation, Video Quality Assessment, Video Motion Assessment, Video\nCaptioning, Video Concept Balancing and Video-Text Alignment. Each stage plays a crucial role in\nconstructing the dataset, and we describe them in detail below.\nVideo Segmentation\nWe began by processing raw videos using the AdaptiveDetector function\nin the PySceneDetect [PySceneDetect Developers] toolkit to dentify scene changes and use FFm-\npeg [FFmpeg Developers] to split them into single-shot clips. We adjusted the splitting process for\nhigh-resolution videos not encoded with libx264 to include necessary reference frames\u2014specifically\nby properly setting the crop start time in FFmpeg; this prevented visual artifacts or glitches in the\noutput video. We also removed the first three frames and the last three frames of each clip, following\npractices similar to Panda70M Chen et al. [2024a] and Movie Gen Video Polyak et al. [2024]. Ex-\ncluding these frames eliminates unstable camera movements or transition effects often present at the\nbeginnings and endings of videos.\nVideo Quality Assessment\nTo construct a refined dataset optimized for model training on high-\nquality, we systematically evaluated and filtered video clips by assigning multiple Quality Assessment\ntags based on specific criteria. We uniformly sampled eight frames from each clip to compute these\ntags, providing a consistent and comprehensive assessment of each video.\n18\n\u2022 Aesthetic Score: We used the public LAION CLIP-based aesthetic predictor Schuhmann et al.\n[2022] to predict the aesthetic scores of eight frames from each clip and calculated their average.\n\u2022 NSFW Score: We employed the public LAION CLIP-based NSFW detector LAION [2021], a\nlightweight two-class classifier using CLIP ViT-L/14 embeddings, to identify content inappropriate\nfor safe work environments.\n\u2022 Watermark Detection: Employing an EfficientNet image classification model Tan and Le [2019],\nwe detected the presence of watermarks within the videos.\n\u2022 Subtitle Detection: Utilizing PaddleOCR Contributors [2023], we recognized and localized text\nwithin video frames, identifying clips with excessive on-screen text or captions.\n\u2022 Saturation Score: We assessed color saturation by converting video frames from BGR to HSV\ncolor space and extracting the saturation channel, using OpenCV OpenCV Developers [2021].\nWe computed statistical measures\u2014including mean, maximum, and minimum saturation val-\nues\u2014across the frames.\n\u2022 Blur Score: We detect blurriness by applying the variance of the Laplacian method Pech-Pacheco\net al. [2000] to measure the sharpness of each frame. Low variance values indicate blurriness\ncaused by camera shake or lack of clarity.\n\u2022 Black Border Detection: We use FFmpeg to detect black borders in frames and record their\ndimensions to facilitate cropping, ensuring that the model trains on content free of distracting\nedges.\nVideo Motion Assessment\nRecognizing that motion content is crucial for representing dynamic\nscenes and ensuring effective model training, we calculate the motion score by averaging the mean\nmagnitudes of the optical flow OpenCV Developers [2021] between pairs of resized grayscale frames,\nusing the Farneback algorithm. We introduced three evaluative tags centered around motion scores:\n\u2022 Motion_Mean: The average motion magnitude across all frames in the clip, indicating the general\nlevel of motion. This score helps us identify clips with appropriate motion; clips with extremely\nlow Motion_Mean values suggest static or slow motion scenes that may not effectively contribute\nto training models focused on dynamic content.\n\u2022 Motion_Max: The maximum motion magnitude observed in the clip, highlighting instances of\nextreme motion or motion distortion. High Motion_Max values may indicate the presence of\nframes with excessive or jittery motion.\n\u2022 Motion_Min: The minimum motion magnitude in the clip, identifying clips with minimal motion.\nClips with very low Motion_Min may contain idle frames or abrupt pauses, which could be\nundesirable for training purposes.\nVideo Captioning\nRecent studies [OpenAI, 2024, Betker et al., 2023] have highlighted that both\nprecision and richness of captions are crucial in enhancing the prompt-following ability and output\nquality of generative models.\nMotivated by this, we introduced three types of caption labeling into our video captioning process by\nemploying an in-house Vision Language Model (VLM) designed to generate both short and dense\ncaptions for video clips.\n\u2022 Short Caption: The short caption provides a concise description, focusing solely on the main\nsubject and action, closely mirroring real user prompts.\n\u2022 Dense Caption: The dense caption integrates key elements, emphasizing the main subject, events,\nenvironmental and visual aspects, video type and style, as well as camera shots and movements.\nTo refine camera movements, we manually collected annotated data and performed SFT on our\nin-house VLM, incorporating common camera movements and shooting angles.\n\u2022 Original Title: We also included a variety of caption styles by incorporating a portion of the\noriginal titles from the raw videos, adding diversity to the captions.\nVideo Concept Balancing\nTo address category imbalances and facilitate deduplication in our\ndataset, we computed embeddings for all video clips using an internal VideoCLIP model and applied\nK-means clustering MacQueen [1967] to group them into over 120,000 clusters, each representing\n19\na specific concept or category. By leveraging the cluster size and the distance to centroid tags, we\nbalanced the dataset by filtering out clips that were outliers within their respective clusters. As part of\nthis process, we added two new tags to each clip:\n\u2022 Cluster_Cnt: The total number of clips in the cluster to which the clip belongs.\n\u2022 Center_Sim: The cosine distance between the clip\u2019s embedding and the cluster center.\nVideo-Text Alignment\nRecognizing that accurate alignment between video content and textual\ndescriptions is essential to generate high-quality output and effective data filtering, we compute a\nCLIP Score to measure video-text alignment. This score assesses how well the captions align with\nthe visual content of the video clips.\n\u2022 CLIP Score: We begin by uniformly sampling eight frames from the given video clip. Using\nthe CLIP model Yang et al. [2022], we then extract image embeddings for these frames and a\ntext embedding for the video caption. The CLIP Score is computed by averaging the cosine\nsimilarities between each frame embedding and the caption embedding.\n7.2\nPost-training Data\nFor SFT in post-training, we curate a high-quality video dataset that captures good motion, realism,",
    "similarities between each frame embedding and the caption embedding.\n7.2\nPost-training Data\nFor SFT in post-training, we curate a high-quality video dataset that captures good motion, realism,\naesthetics, a broad range of concepts, and accurate captions. Inspired by Dai et al. [2023], Polyak\net al. [2024], Kong et al. [2025], we utilize both automated and manual filtering techniques:\n\u2022 Filtering by Video Assessment Scores: Using video assessment scores and heuristic rules, we\nfilter the entire dataset to a subset of 30M videos, significantly improving its overall quality.\n\u2022 Filtering by Video Categories: For videos within the same cluster, we use the \"Distance to\nCentroid\" values to remove those whose distance from the centroid exceeds a predefined threshold.\nThis ensures that the resulting video subset contains a sufficient number of videos for each cluster\nwhile maintaining diversity within the subset.\n\u2022 Labeling by Human Annotators: In the final stage, human evaluators assess each video for clarity,\naesthetics, appropriate motion, smooth scene transitions, and the absence of watermarks or subtitles.\nCaptions are also manually refined to ensure accuracy and include essential details such as camera\nmovements, subjects, actions, backgrounds, and lighting.\n8\nTraining Strategy\ntraining stage\ndataset\nbs/node\nlearning rate\n#iters\n#seen samples\nStep-1: T2I Pre-training (256px)\nO(1)B images\n40\n1e-4\n53k\n0.8B\nO(1)B images\n40\n1e-4\n200k\n3B\nTotal\n253k\n3.8B\nStep-2: T2VI Pre-training (192px)\nO(1)B video clips\n4\n6e-5\n171k\n256M\nO(100)M video clips\n4\n6e-5\n101k\n151M\nO(100)M video clips\n4\n6e-5\n158k\n237M\nTotal\n430k\n644M\nStep-2: T2VI Pre-training (540px)\nO(100)M video clips\n2\n2e-5\n23k\n17.3M\nO(10)M video clips\n2\n1e-5\n17k\n8.5M\nO(1)M video clips\n1\n1e-5\n6k\n1.5M\nTotal\n46k\n27.3M\nTable 6: Pre-training details of Step-Video-T2V. 256px, 192px, and 540px denote resolutions of\n256x256, 192x320, and 544x992, respectively.\nA cascaded training strategy is employed in Step-Video-T2V, which mainly includes four steps: text-\nto-image (T2I) pre-training, text-to-video/image (T2VI) pre-training, text-to-video (T2V) fine-tuning,\nand direct preference optimization (DPO) training. The pre-training recipe is summarized in Table 6.\n20\nFigure 10: Training curve of different training stages, where si denotes the ith dataset used in the\ncorresponding stage.\nStep-1: T2I Pre-training\nIn the initial step, we begin by training Step-Video-T2V with a T2I\npre-training approach from scratch. We intentionally avoid starting with T2V pre-training directly,\nas doing so will significantly slow down model convergence. This conclusion stems from our early\nexperiments with the T2V pre-training from scratch on the 4B model, where we observed that the\nmodel struggled to learn new concepts and was much slower to converge. By first focusing on T2I,\nthe model can establish a solid foundation in understanding visual concepts, which can later be\nexpanded to handle temporal dynamics in the T2V phase.\nStep-2: T2VI Pre-training\nAfter acquiring spatial knowledge from T2I pre-training in Step-1,\nStep-Video-T2V progresses to a T2VI joint training stage, where both T2I and T2V are incorporated.\nThis step is further divided into two stages. In the first stage, we pre-train Step-Video-T2V using\nlow-resolution (192x320, 192P) videos, allowing the model to primarily focus on learning motion-\nrelated knowledge rather than fine details. In the second stage, we increase the video resolution to\n544x992 (540P) and continue pre-training to enable the model to learn more intricate details. We\nobserved that during the first stage, the model concentrates on learning motion, while in the second\nstage, it shifts its focus more toward learning fine details. Based on these observations, we allocate\nmore computational resources to the first stage in Step-2 to better capture motion knowledge.\nStep-3: T2V Fine-tuning\nDue to the diversity in pre-training video data across different domains\nand qualities, using a pre-trained checkpoint usually introduces artifacts and varying styles in the\ngenerated videos. To mitigate these issues, we continue the training pipeline with a T2V fine-tuning\nstep. In this stage, we use a small number of text-video pairs and remove T2I, allowing the model to\nfine-tune and adapt specifically to text-to-video generation.\nSimilar to Movie Gen Video, we found that averaging models fine-tuned with different SFT datasets\nimproves the quality and stability of the generated videos, outperforming the Exponential Moving\nAverage (EMA) method. Even averaging checkpoints from the same data source enhances stability\nand reduces distortions. Additionally, we select model checkpoints based on the period after the\ngradient norm peaks, ensuring both the gradient norm and loss have decreased for improved stability.\nStep-4: DPO Training\nAs described in \u00a74.4, video-based DPO training is employed to enhance\nthe visual quality of the generated videos and ensure better alignment with user prompts.\nHierarchical Data Filtering\nWe apply a series of filters to the data, progressively increasing their\nthresholds to create six pre-training subsets for Step-2: T2VI Pre-training, as shown in Table 6. The\nfinal SFT dataset is then constructed through manual filtering. Figure 11 illustrates the key filters\napplied at each stage, with gray bars representing the data removed by each filter, and colored bars\nindicating the remaining data at each stage.\nObservations from Pre-training Curve\nDuring pre-training, we observe a notable reduction in\nloss, which correlates with the improved quality of the training data, as illustrated in Figure 10.\n21\nFigure 11: Hierarchical data filtering for pre-training and post-training.",
    "loss, which correlates with the improved quality of the training data, as illustrated in Figure 10.\n21\nFigure 11: Hierarchical data filtering for pre-training and post-training.\nAdditionally, a sudden drop in loss occurs as the quality of the training dataset improves. This\nimprovement is not directly driven by supervision through a loss function during model training,\nbut rather follows human intuition (e.g., filtering via CLIP scores, aesthetic scores, etc.). While the\nflow matching algorithm does not impose strict requirements on the distribution of the model\u2019s input\ndata, adjusting the training data to reflect what is considered higher-quality by humans results in\na significant, stepwise reduction in training loss. This suggests that, to some extent, the model\u2019s\nlearning process may emulate human cognitive patterns.\nBucketization for Variable Duration and Size\nTo accommodate varying video lengths and aspect\nratios during training, we employed variable-length and variable-resolution strategies Chen et al.\n[2023], Zheng et al. [2024]. We defined four length buckets (1, 68, 136, and 204 frames) and\ndynamically adjusted the number of latent frames based on the video length. Additionally, we\ngrouped videos into three aspect ratio buckets\u2014landscape, portrait, and square\u2014according to the\nclosest height-to-width ratio.\n9\nExperiments\n9.1\nBenchmark and Metric\nWe build Step-Video-T2V-Eval, a new benchmark for assessing the quality of text-to-video models.\nThis benchmark consists of 128 Chinese prompts sourced from real users and is designed to evaluate\nthe quality of generated videos across 11 categories, including Sports, Food, Scenery, Animals,\nFestivals, Combined Concepts, Surreal, People, 3D Animation, Cinematography, and Style.\nAdditionally, we propose two human evaluation metrics based on Step-Video-T2V-Eval, which can\nbe used to compare the performance of Step-Video-T2V with that of a target model:\n\u2022 Metric-1 compares Step-Video-T2V with a target model by having each human annotator assign a\nWin/Tie/Loss label to each generated video pair from the two models for the same prompt, with the\nmodel names masked. A \"Win\" means Step-Video-T2V performs better than the target model, a\n\"Loss\" means it performs worse, and a \"Tie\" indicates the models have similar quality.\n\u2022 Metric-2 assigns four scores to each generated video to measure its quality across the following\n4 dimensions: (1) instruction following, (2) motion smoothness, (3) physical plausibility, and (4)\naesthetic appeal. The two models are then compared based on their labeled scores.\nThe criteria for scoring each dimension in Metric-2 are outlined below:\n\u2022 Instruction Following. Score=5: The video is fully consistent with the prompt, with all elements\nand details generated accurately, and the expression of complex scenarios is flawless. Score=4:\nThe video is generally consistent with the prompt, but there are slight discrepancies in some minor\n22\ndetails. Score=3: The video mostly meets the prompt\u2019s requirements, but there are noticeable\ndeviations in several details or core content. Score=2: The video is clearly inconsistent with the\nprompt, with significant detail omissions or overall deviations. Score=1: The video is completely\ninconsistent with the prompt, with major scenes or subjects completely incorrect.\n\u2022 Motion Smoothness. Score=5: The motion is smooth and natural, with all movements and\ntransitions flowing seamlessly. Score=4: The motion is generally smooth, but there are occasional\nslight unnatural movements in certain scenes. Score=3: The motion has slight unnatural or\nstuttering elements, but it doesn\u2019t affect overall understanding. Score=2: The motion is unnatural\nor disconnected, with noticeable stuttering. Score=1: The motion is very unnatural, with frequent\nstuttering, making it difficult to understand.\n\u2022 Physical Plausibility. Score=5: All object interactions and movements adhere to real-world\nphysical laws, with accurate lighting, shadow, and collision effects, and smooth motion. Score=4:\nMost of the physical behavior is reasonable, with occasional minor unnatural collisions or lighting\nissues, but they don\u2019t affect the overall effect. Score=3: Several instances of object motion, lighting,\nor interactions conflict with physical logic, but the main actions still have a degree of coherence.\nScore=2: The physical behavior is unrealistic, with lighting or object interactions violating physical\nlaws, making the scene appear unnatural. Score=1: The physical behavior is completely incorrect,\nwith severe distortion in object interactions or lighting, making the scene difficult to understand.\nAesthetic Appeal.\nScore=5: Highly captivating, deeply moving, with significant artistic value\nand visual appeal. Score=4: Pleasant and engaging, effectively capturing the audience\u2019s attention\nwith good visual value. Score=3: Somewhat appealing, but overall performance is mediocre\nand doesn\u2019t leave a lasting impression. Score=2: Average, lacking in appeal, and may cause\nthe audience to lose interest. Score=1: Unpleasant, lacking in appeal, and the overall effect is\ndisappointing.\n9.2\nComparisons to Open-source Model\nWe first compare Step-Video-T2V with HunyuanVideo on Step-Video-T2V-Eval.\nStep-Video-T2V vs. HunyuanVideo (Win-Tie-Loss)\nAnnotator-1\nAnnotator-2\nAnnotator-3\nOverall\n59-22-47\n46-47-35\n54-41-33\nSports\n6-3-3\n5-5-2\n6-6-0\nFood\n5-2-4\n5-4-2\n3-7-1\nScenery\n5-3-4\n2-9-1\n7-1-4\nAnimals\n6-0-6\n3-6-3\n2-7-3\nFestivals\n4-4-3\n5-2-4\n4-5-2\nCombined Concepts\n5-2-5\n6-3-3\n8-1-3\nSurreal\n4-2-5\n5-2-4\n6-2-3\nPeople\n6-2-4\n3-4-5\n5-2-5\n3D Animation\n7-1-4\n4-5-3\n6-3-3\nCinematography\n5-1-5\n2-5-4\n1-4-6\nStyle\n6-2-4\n6-2-4\n6-3-3\nTable 7: Comparison with HunyuanVideo using Metric-1.",
    "2-5-4\n1-4-6\nStyle\n6-2-4\n6-2-4\n6-3-3\nTable 7: Comparison with HunyuanVideo using Metric-1.\nFrom Table 7 and Table 8 we got three observations.\nFirst, Step-Video-T2V demonstrates state-of-the-art performance as the strongest open-source text-to-\nvideo generation model to date. This success is attributed to multiple factors, including the model\u2019s\nstructural design and its pre-training and post-training strategies. Second, in some categories like\nAnimals, Step-Video-T2V performs worse than HunyuanVideo, as shown in Table 7. This is primarily\ndue to aesthetic issues, as verified by the Aesthetic Appeal score in Table 8. Third, Video-VAE\nachieves compression ratios of 16x16 spatial and 8x temporal, compared to HunyuanVideo\u2019s 8x8\nspatial and 4x temporal compression. This higher compression rate enables Step-Video-T2V to\ngenerate videos up to 204 frames, nearly double the 129-frame maximum of HunyuanVideo.\n23\nStep-Video-T2V vs. HunyuanVideo\nInstruction Following\nMotion Smoothness\nPhysical Plausibility\nAesthetic Appeal\nOverall\n1,273-1,221\n1,407-1,327\n1,417-1,238\n1,312-1,238\nSports\n130-111\n120-104\n113-99\n110-98\nFood\n85-92\n110-97\n107-93\n111-90\nScenery\n130-129\n139-126\n134-120\n125-122\nAnimals\n104-106\n123-114\n110-107\n99-108\nFestivals\n102-91\n110-102\n97-90\n103-94\nCombined Concepts\n132-115\n139-136\n139-135\n118-115\nSurreal\n99-101\n138-139\n135-134\n125-126\nPeople\n115-117\n129-129\n148-150\n115-112\n3D Animation\n113-109\n137-133\n149-146\n139-135\nCinematography\n121-117\n121-122\n132-133\n116-115\nStyle\n142-133\n141-125\n153-134\n151-123\nTable 8: Comparison with HunyuanVideo using Metric-2. We invited three human annotators to\nevaluate each video. For each category and evaluation dimension, we aggregated the scores given by\nall annotators across all prompts within the category for that dimension.\n9.3\nComparisons to Commercial Model\nWe then compare Step-Video-T2V with two leading text-to-video engines in China, T2VTopA\n(2025-02-10 version) and T2VTopB (2025-02-10 version), on Step-Video-T2V-Eval.\nStep-Video-T2V vs. T2VTopA (Win-Tie-Loss)\nAnnotator-1\nAnnotator-2\nAnnotator-3\nOverall\n44-13-69\n41-13-72\n46-25-55\nSports\n6-2-4\n7-0-5\n7-3-2\nFood\n5-2-4\n6-1-4\n4-2-5\nScenery\n1-0-10\n4-0-7\n1-2-8\nAnimals\n1-3-8\n1-3-8\n3-1-8\nFestivals\n6-2-3\n7-2-2\n5-3-3\nCombined Concepts\n2-0-10\n1-3-8\n8-0-4\nSurreal\n4-1-6\n3-2-6\n4-2-5\nPeople\n2-1-8\n2-1-8\n6-1-4\n3D Animation\n6-0-6\n3-0-9\n5-3-4\nCinematography\n5-1-5\n4-1-6\n1-3-7\nStyle\n6-1-5\n3-0-9\n2-5-5\nTable 9: Comparison with T2VTopA using Metric-1. A total of 126 prompts were evaluated, rather\nthan 128, as T2VTopA rejected 2 prompts.\nFrom Table 9, Table 10, and Table 11 we got three observations.\nFirst, the overall ranking of the three models in Table 9 and Table 10 is as follows: T2VTopA >\nStep-Video-T2V > T2VTopB. We analyzed categories such as Scenery, Animals, People, and Style,\nwhere Step-Video-T2V performs worse than the other two models, and found that the primary reason\nlies in their generally higher aesthetic appeal. We believe this advantage mainly stems from the\nhigher resolutions of the generated videos (720P in T2VTopA, 1080P in T2VTopB, and 540P in\nStep-Video-T2V) and the high-quality aesthetic data used during their post-training stages. Table 11\nalso shows that 4 out of 6 annotators rate T2VTopA and T2VTopB as having higher aesthetic appeal.\nSecond, Step-Video-T2V consistently outperforms T2VTopA and T2VTopB in the Sports category in\nTable 9 and Table 10, demonstrating its strong capability in modeling and generating videos with\nhigh-motion dynamics. Table 11 also highlights Step-Video-T2V\u2019s superiority in Motion Smoothness\nand Physical Plausibility.\nThird, we observed that T2VTopA has better instruction-following capability, which contributes to\nits superior performance in categories such as Combined Concepts, Surreal, and Cinematography.\nWe believe the key reasons for this are better video captioning model and the greater human effort\ninvolved in labeling the post-training data used by T2VTopA.\n24\nStep-Video-T2V vs. T2VTopB (Win-Tie-Loss)\nAnnotator-1\nAnnotator-2\nAnnotator-3\nOverall\n36-35-51\n67-10-45\n55-22-45\nSports\n8-2-2\n10-1-1\n8-2-2\nFood\n3-4-3\n7-1-2\n7-2-1\nScenery\n2-6-4\n5-2-5\n5-4-3\nAnimals\n5-1-5\n3-1-7\n2-2-7\nFestivals\n6-1-4",
    "2-6-4\n5-2-5\n5-4-3\nAnimals\n5-1-5\n3-1-7\n2-2-7\nFestivals\n6-1-4\n6-0-5\n2-4-5\nCombined Concepts\n1-4-7\n6-1-5\n4-2-6\nSurreal\n2-0-6\n3-0-5\n2-1-5\nPeople\n1-3-7\n4-1-6\n3-1-7\n3D Animation\n5-3-4\n11-0-1\n11-0-1\nCinematography\n3-3-5\n4-2-5\n3-1-7\nStyle\n0-8-4\n8-1-3\n8-3-1\nTable 10: Comparison with T2VTopB using Metric-1. A total of 122 prompts were evaluated, rather\nthan 128, as T2VTopB rejected 6 prompts.\nModel\nInstruction Following\nMotion Smoothness\nPhysical Plausibility\nAesthetic Appeal\nAnnotator-1\nStep-Video-T2V\n204\n210\n203\n187\nT2VTopA\n211\n200\n198\n196\nT2VTopB\n185\n184\n178\n175\nAnnotator-2\nStep-Video-T2V\n211\n243\n256\n217\nT2VTopA\n241\n243\n242\n228\nT2VTopB\n234\n236\n229\n204\nAnnotator-3\nStep-Video-T2V\n170\n197\n172\n178\nT2VTopA\n177\n177\n153\n171\nT2VTopB\n164\n163\n139\n148\nAnnotator-4\nStep-Video-T2V\n199\n232\n230\n225\nT2VTopA\n217\n221\n201\n199\nT2VTopB\n194\n219\n194\n194\nAnnotator-5\nStep-Video-T2V\n218\n225\n213\n211\nT2VTopA\n221\n220\n213\n212\nT2VTopB\n209\n217\n202\n196\nAnnotator-6\nStep-Video-T2V\n187\n213\n251\n211\nT2VTopA\n193\n201\n259\n197\nT2VTopB\n201\n224\n271\n227\nTable 11: Comparison with T2VTopA and T2VTopB using Metric-2. We invited six human annotators\nto evaluate each video. For each evaluation dimension, we aggregated the scores given by each\nannotator across all prompts for that dimension. Prompts that were rejected by any model were\nexcluded from the analysis for all models.\nNote that Step-Video-T2V still lacks sufficient training in the final stage of pre-training with 540P\nvideos, having only seen 25.3M samples (as shown in Table 6). Additionally, compared to these\ntwo commercial engines, we are using significantly less high-quality data in the post-training phase,\nwhich will be continuously improved in the future. Finally, the video length is 204 frames, nearly\ntwice the length of T2VTopA and T2VTopB, making our training more challenging. We assert\nthat Step-Video-T2V has already achieved the strongest motion dynamics modeling and generation\ncapabilities among all commercial engines. Given comparable training resources and high-quality\ndata, we believe it can achieve state-of-the-art results in general domains as well.\n9.4\nEvaluation on Movie Gen Video Bench\nMovie Gen Video Bench Polyak et al. [2024] is another existing benchmark for the text-to-video\ngeneration task. It includes 1,003 prompts across multiple categories, covering human activities,\nanimals, nature and scenery, physics, as well as unusual subjects and activities. Although Movie Gen\nVideo has not been open-sourced, its generated results on the Movie Gen Video Bench are publicly\n25\navailable (https://github.com/facebookresearch/MovieGenBench). Therefore, we also compare Step-\nVideo-T2V with Movie Gen Video and HunyuanVideo in Table 12 using this benchmark.\nCategory\nStep-Video-T2V vs. Movie Gen Video\n(Win-Tie-Loss)\nStep-Video-T2V vs. HunyuanVideo\n(Win-Tie-Loss)\n# of Prompts\nOverall\n485-315-489\n615-313-361\n1,289\nhuman\n123-58-160\n181-64-96\n341\nphysics\n61-54-64\n87-47-45\n179\nunusual activity & subject\n110-74-108\n136-75-81\n292\nanimal\n39-37-42\n47-30-41\n118\nscene\n84-53-63\n91-58-51\n200\nsequential motion\n9-2-2\n6-2-5\n13\ncamera motion\n59-37-50\n67-37-42\n146\nTable 12: Comparison of Movie Gen Video and HunyuanVideo using the Movie Gen Video Bench.\nThe total number of evaluations (1,289) is greater than 1,003 due to some prompts having multiple\ncategory tags. This evaluation involved six human annotators.\nCompared to Movie Gen Video, Step-Video-T2V achieves a comparable performance. We got several\nobservations from this comparison. First, the pre-training of Step-Video-T2V remains insufficient.\nWhile Movie Gen Video was trained on 73.8M videos during its high-resolution pre-training phase,\nStep-Video-T2V was trained on only 27.3M videos\u2014about one-third of the number used by Movie\nGen Video. Additionally, we observed that the training curves for all pre-training stages in Step-\nVideo-T2V continue to show a downward trend. Due to resource limitations, we plan to conduct more\nextensive pre-training as part of our future work. Second, the Movie Gen Video paper highlights the\nsignificant human effort involved in labeling the high-quality SFT dataset. However, due to limited\nhuman resources, we lack enough high-quality labeled data at this stage to effectively refine the visual\nstyle and quality of the generated results. Third, Movie Gen Video can generate 720P videos, which\nare visually more appealing than the 540P resolution produced by Step-Video-T2V. Feedback from\nhuman annotators suggests that high resolution can often be a key factor in determining which model\nperforms better. Compared to HunyuanVideo, Step-Video-T2V achieves significant improvements",
    "human annotators suggests that high resolution can often be a key factor in determining which model\nperforms better. Compared to HunyuanVideo, Step-Video-T2V achieves significant improvements\nacross all categories, solidifying its position as the state-of-the-art open-source text-to-video model.\n9.5\nGenerating Text Content in Videos\nWe also compare Step-Video-T2V with open-source and commercial engines on a list of prompts\nsuch as \"a squirrel holding a sign that says \u2019hello\u2019.\", where the model is required to generate videos\nthat include text content as well.\nOur observations show that Step-Video-T2V outperforms all other models in generating basic English\ntext. We attribute this capability to the T2I pre-training stage, where a portion of the images contained\ntext, and the captions explicitly described it. However, the accuracy of text generation remains far\nfrom ideal. Furthermore, due to the complexity of Chinese characters, Step-Video-T2V is currently\nable to generate only a limited number of them. Enhancing text generation capabilities for both\nEnglish and Chinese will be a focus of our future work.\nFigure 12: Four frames sampled from the video generated based on the prompt \"In the video, a\nChinese girl is dressed in an exquisite traditional outfit, smiling with a confident and graceful\nexpression. She holds a piece of paper with the words \"we will open source\" clearly written on it.\nThe background features an ancient and elegant setting, complementing the girl\u2019s demeanor. The\nentire scene is clear and has a realistic style.\".\n26\n9.6\nVAE Video Reconstruction\nModel\nDownsample Factor\nSSIM\u2191\nPSNR\u2191\nrFVD\u2193\nOpenSora-1.2 (Zheng et al. [2024])\n4 \u00d7 8 \u00d7 8\n0.9126\n31.41\n20.42\nCogvideoX-1.5 (Yang et al. [2024a])\n4 \u00d7 8 \u00d7 8\n0.9373\n38.10\n16.33\nHunyuanVideo (Kong et al. [2025])\n4 \u00d7 8 \u00d7 8\n0.9710\n39.56\n4.17\nCosmos-VAE (Nvidia [2025])\n4 \u00d7 8 \u00d7 8\n0.9315\n37.66\n9.10\nCosmos-VAE (Nvidia [2025])\n8 \u00d7 16 \u00d7 16\n0.8862\n34.82\n40.33\nVideo-VAE (Ours)\n8 \u00d7 16 \u00d7 16\n0.9776\n39.37\n3.61\nTable 13: Comparison of reconstruction metrics.\nWe compare Video-VAE with several open-source baselines using 1,000 test videos from various\ndomains, each with dimensions of 50(frames)\u00d7480(height)\u00d7768(width). As shown in Table 13,\ndespite having a compression ratio 8 times larger than most baselines, our reconstruction quality still\nmaintains state-of-the-art performance. While Cosmos-VAE also offers a high-compression version\nwith a factor of 8\u00d716\u00d716, its reconstruction quality falls significantly behind our method.\nFigure 13 illustrates typical challenge cases in video reconstruction, including high-motion (first row),\ntext (second row), texture (third row), high-motion combined with text (fourth row), and high-motion\ncombined with texture (fifth row). Our models significantly outperform other baselines, even with\nhigher compression ratios.\n9.7\nDPO\nTo assess the effectiveness of the proposed Video-DPO algorithm, we conduct inference on 300\ndiverse prompts. The evaluation involves two models: the baseline model and the baseline model\nwith the Video-DPO enhancement (baseline w/. DPO). Both models are sampled under identical\ninitial noise conditions to control for extraneous variables and ensure a fair comparison. For each\ngenerated video, three independent annotators are tasked with evaluating their preference between\nthe two models, with an option to select \"no preference\". The evaluation protocol is as follows:\n\u2022 If an annotator prefers the video generated by \"baseline w/. DPO\", the model receives 1 point.\n\u2022 If an annotator prefers the \"baseline\" video, the baseline model receives 1 point.\n\u2022 If an annotator indicates \"no preference,\" both models receive 0.5 points.\nUpon aggregating the scores, we find that the baseline model with DPO (baseline w/. DPO) achieves\na preference score of 55%, outperforming the baseline model (45%). This result demonstrates\nthe efficacy of Video-DPO in generating videos more aligned with user preferences. The visual\ncomparison is shown in Figure 14, demonstrates that human feedback enhances the plausibility\nand consistency of generated videos. Additionally, we observe that the DPO baseline enhances the\nalignment with the given prompts, resulting in more accurate and relevant video generation.\nWhile Video-DPO demonstrates effectiveness, several issues remain. (1) The trajectory from initial\nnoise to timestep-specific latents acts as implicit dynamic conditions beyond text prompts \u2014 yet this\ndimension remains underutilized due to computational limitations. (2) A tradeoff exists between\nsparse and imprecise feedback, especially in video diffusion models. For instance, in videos with\nover 100 million pixels, only a few pixels may be problematic, yet feedback often comes as a single\nscalar or lacks precision. (3) Unlike LLMs, which use token-level softmax to create competition\nbetween tokens, diffusion models rely on regression, which may result in less efficient preference op-\ntimization. We hope these discussions provide insights and inspire further algorithmic advancements\nin incorporating human feedback.\n10\nDiscussion\n10.1\nModel Architecture\nUnlike DiT, which relies on a modulation mechanism to condition the network on the text prompt,\nMMDiT integrates the text prompt directly into the Transformer, separates the weights for text and\n27\nFigure 13: Video reconstruction results compared with public available models, in scenarios including\nhigh-motion (first row), text (second row), texture (third row), high-motion combined with text (fourth\nrow), and high-motion combined with texture (fifth row).\nvideo, and uses a shared attention mechanism to merge the latent representations of both modalities.\nWe compared the training curves of DiT and MMDiT in the early stages and found both architectures\nexhibited similar performance. Given these comparable results, along with DiT\u2019s ability to disentangle\ntext and video and its natural extension to pure video prediction models without text, we ultimately\nselected DiT as the model architecture for Step-Video-T2V. Due to computational cost constraints,\nwe did not train MMDiT-based model for an extended period to assess its upper performance limit.\nWe also compared spatial-temporal attention and 3D full attention mechanisms within DiT. In the",
    "we did not train MMDiT-based model for an extended period to assess its upper performance limit.\nWe also compared spatial-temporal attention and 3D full attention mechanisms within DiT. In the\nspatial-temporal attention mechanism, the model captures spatial information among tokens with\nthe same temporal index within spatial Transformer blocks, and temporal information across time\nsteps in temporal Transformer blocks. In contrast, 3D full attention mechanism combines both spatial\nand temporal information in a unified attention process, offering higher performance potential but\nat the cost of increased computational demands. We trained two DiT models\u2014one using spatial-\ntemporal attention and the other using 3D full attention\u2014both in a 4B setting. Upon comparing their\nperformances, we found that 3D full attention-based model outperforms spatial-temporal attention-\nbased model, particularly in generating videos with high motion dynamics. Given its superior quality,\nwe ultimately selected the 3D full attention setting.\nIn addition, 3D full attention is known for its high training and inference cost, so we are still actively\ninvestigating more efficient way to reduce the computation overhead, while preserving the same\nmodel quality Tan et al. [2025].\n28\nw/o. DPO\nw/o. DPO\nw/. DPO\nw/. DPO\nA kitten in a grey \nwolf-themed outfit \nand furry pointy \nears confidently \nwalks down a \nfashion catwalk \nalongside a \nmajestic wolf.\nPrompt\nThe video presents \na close-up of two \nchameleons under \nthe night sky in a \nrealistic style.\n(a) Videos generated with the DPO baseline show greater realism and improved consistency.\n(b) DPO baseline videos align better.\nFigure 14: Visual comparison of video generation with and without the DPO baseline.\n10.2\nInstruction Following\nBased on the evaluation results, we found that even a DiT-based model like Step-Video-T2V, with\n30B parameters, struggles to generate videos involving complex action sequences. Additionally,\ngenerating videos that incorporate multiple concepts with low occurrence in the training data (e.g.,\nan elephant and a penguin) remains challenging in Step-Video-T2V and other leading text-to-video\ngeneration models. Both of these challenges can be viewed as instruction-following problems.\nWe examine the instruction-following capability of Step-Video-T2V, focusing on how it interprets\ninstructions involving various objects, actions, and other details. Our analysis reveals that the\ndistribution of cross-attention scores is occasionally highly concentrated, with a strong focus on\nspecific objects or actions. This pronounced attention can result in missing objects, wrong details, or\nincomplete action sequences in the generated videos.\nBy heuristically repeating the missing objects in the prompt, some of the problematic cases can be\nsignificantly improved. This demonstrates the importance of ensuring that all elements in the prompt\nreceive appropriate attention. We leave the task of balancing this attention for future work, aiming to\nrefine the model\u2019s ability to better attend and follow all elements in the prompt.\n10.3\nLaws of Physics Following\nWe analyzed a number of videos generated by leading text-to-video models, including Sora, Hailuo,\nKling, and Step-Video-T2V, and found that all of these models struggle to accurately simulate the\nreal world and generate videos that adhere to the laws of physics\u2014such as a ball bouncing on the\nfloor or a drop of water falling into a cup. Some text-to-video engines can produce good results for\ncertain prompts, but these successes are often due to the model over-fitting to specific annotations,\nand cannot generalize well.\nThis finding highlights a key limitation of diffusion-based models in text-to-video generation. To\naddress this challenge, we plan to develop more advanced model paradigms in future work, such\nas combining autoregressive and diffusion models within a unified framework (Chen et al. [2024b],\nHaCohen et al. [2024], Zhou et al. [2025]), to better adhere to the laws of physics and more accurately\nsimulate realistic interactions.\n10.4\nHigh-quality Labeled Data for Post-training\nBy applying a small amount of high-quality human-labeled data in SFT, Step-Video-T2V achieves\nsignificant improvements in the overall video quality, demonstrating that the quality and diversity\nof the data outweigh its sheer scale. We also observed that certain characteristics of these curated\n29\nhigh-quality datasets, such as video style and the degree of motion dynamics, generalize well across\na broader range of prompts. This further underscores the importance of high-quality, small-scale, and\ndiverse datasets for post-training.\nCurating such datasets is both expensive and time-consuming, involving tasks such as selecting\nhigh-quality videos from a large pool, labeling them with accurate captions, and ensuring the dataset\ncovers a diverse range of objects, actions, styles, and domains. We plan to build a comprehensive\nvideo knowledge base with structured labels as part of our future work.\n10.5\nRL-based Optimization Mechanism for Post-training\nWe employed a simple yet effective DPO-based model for video generation and also explored training\na reward model to automate the entire post-training process. However, the proposed method still\nrequires human labeling efforts in the early stages and is time-consuming when extending it to general\ndomains. On the other hand, RL-based approaches have achieved great success in LLMs, such as\nOpenAI-O1 and DeepSeek-R1 DeepSeek-AI et al. [2025]. However, unlike RL-focused natural\nlanguage tasks, such as solving math problems or generating code, which have well-defined problems\nwith clear answers, it remains challenging to define similar tasks in the video generation domain. We\nconsider this a key challenge for future research exploration.\n11\nConclusion and Future Work\nThis technical report introduces and open-sources Step-Video-T2V, a state-of-the-art pre-trained\nvideo generation model from text, featuring 30B parameters, a deep compression Video-VAE, a DPO\napproach for video generation, and the ability to generate videos up to 204 frames in length. We\nprovide a comprehensive overview of our pre-training and post-training strategies and introduce\nStep-Video-T2V-Eval as a new benchmark for evaluating text-to-video generation models.\nWe highlight challenges faced by current text-to-video models. First, high-quality labeled data\nremains a significant hurdle. Existing video captioning models often struggle with hallucination\nissues, and human annotations are expensive and difficult to scale. Second, instruction-following",
    "remains a significant hurdle. Existing video captioning models often struggle with hallucination\nissues, and human annotations are expensive and difficult to scale. Second, instruction-following\nrequires more attention, as it encompasses a wide range of scenarios, from generating videos based\non detailed descriptions to handling complex action sequences and combinations of multiple concepts.\nThird, current models still face difficulties in generating videos that obey the laws of physics, an\nissue stemming from the inherent limitations of diffusion models. Lastly, RL-based optimization\nmechanisms are areas worth exploring for post-training improvements in video generation models.\nLooking ahead, we plan to launch a series of open-source projects focused on the development of\nvideo foundation models, starting with Step-Video-T2V. We hope these efforts will drive innovation\nin video foundation models and empower video content creators.\nReferences\nOpenAI. Video generation models as world simulators. https://openai.com/index/video-generation-models-as-\nworld-simulators, 2024.\nRunwayML. Gen-3 alpha. https://runwayml.com/research/introducing-gen-3-alpha, 2024.\nKuaishou. Kling. https://klingai.kuaishou.com, 2024.\nMiniMax. Hailuo. https://hailuoai.com/video, 2024.\nDeepMind. Veo 2. https://deepmind.google/technologies/veo/veo-2, 2024.\nWeijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu,\nJianwei Zhang, Kathrina Wu, Qin Lin, Junkun Yuan, Yanxin Long, Aladdin Wang, Andong Wang, Changlin\nLi, Duojun Huang, Fang Yang, Hao Tan, Hongmei Wang, Jacob Song, Jiawang Bai, Jianbing Wu, Jinbao\nXue, Joey Wang, Kai Wang, Mengyang Liu, Pengyu Li, Shuai Li, Weiyan Wang, Wenqing Yu, Xinchi Deng,\nYang Li, Yi Chen, Yutao Cui, Yuanbo Peng, Zhentao Yu, Zhiyu He, Zhiyong Xu, Zixiang Zhou, Zunnan Xu,\nYangyu Tao, Qinglin Lu, Songtao Liu, Dax Zhou, Hongfa Wang, Yong Yang, Di Wang, Yuhong Liu, Jie Jiang,\nand Caesar Zhong. Hunyuanvideo: A systematic framework for large video generative models, 2025. URL\nhttps://arxiv.org/abs/2412.03603.\n30\nZhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong,\nXiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer.\narXiv preprint arXiv:2408.06072, 2024a.\nZangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen, Shenggui Li, Hongxin Liu, Yukun Zhou, Tianyi\nLi, and Yang You. Open-sora: Democratizing efficient video production for all. March 2024. URL\nhttps://github.com/hpcaitech/Open-Sora.\nBin Lin, Yunyang Ge, Xinhua Cheng, Zongjian Li, Bin Zhu, Shaodong Wang, Xianyi He, Yang Ye, Shenghai\nYuan, Liuhan Chen, et al. Open-sora plan: Open-source large video generation model. arXiv preprint\narXiv:2412.00131, 2024.\nPatrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas M\u00fcller, Harry Saini, Yam Levi, Dominik\nLorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion English, Kyle Lacey, Alex Goodwin,\nYannik Marek, and Robin Rombach. Scaling rectified flow transformers for high-resolution image synthesis,\n2024. URL https://arxiv.org/abs/2403.03206.\nWilliam Peebles and Saining Xie. Scalable diffusion models with transformers, 2023. URL https://arxiv.\norg/abs/2212.09748.\nAdam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra, Animesh Sinha, Ann Lee, Apoorv Vyas, Bowen\nShi, Chih-Yao Ma, Ching-Yao Chuang, David Yan, Dhruv Choudhary, Dingkang Wang, Geet Sethi, Guan\nPang, Haoyu Ma, Ishan Misra, Ji Hou, Jialiang Wang, Kiran Jagadeesh, Kunpeng Li, Luxin Zhang, Mannat\nSingh, Mary Williamson, Matt Le, Matthew Yu, Mitesh Kumar Singh, Peizhao Zhang, Peter Vajda, Quentin\nDuval, Rohit Girdhar, Roshan Sumbaly, Sai Saketh Rambhatla, Sam Tsai, Samaneh Azadi, Samyak Datta,\nSanyuan Chen, Sean Bell, Sharadh Ramaswamy, Shelly Sheynin, Siddharth Bhattacharya, Simran Motwani,\nTao Xu, Tianhe Li, Tingbo Hou, Wei-Ning Hsu, Xi Yin, Xiaoliang Dai, Yaniv Taigman, Yaqiao Luo, Yen-\nCheng Liu, Yi-Chiao Wu, Yue Zhao, Yuval Kirstain, Zecheng He, Zijian He, Albert Pumarola, Ali Thabet,\nArtsiom Sanakoyeu, Arun Mallya, Baishan Guo, Boris Araya, Breena Kerr, Carleigh Wood, Ce Liu, Cen\nPeng, Dimitry Vengertsev, Edgar Schonfeld, Elliot Blanchard, Felix Juefei-Xu, Fraylie Nord, Jeff Liang,\nJohn Hoffman, Jonas Kohler, Kaolin Fire, Karthik Sivakumar, Lawrence Chen, Licheng Yu, Luya Gao,\nMarkos Georgopoulos, Rashel Moritz, Sara K. Sampson, Shikai Li, Simone Parmeggiani, Steve Fine, Tara\nFowler, Vladan Petrovic, and Yuming Du. Movie gen: A cast of media foundation models, 2024. URL\nhttps://arxiv.org/abs/2410.13720.",
    "Fowler, Vladan Petrovic, and Yuming Du. Movie gen: A cast of media foundation models, 2024. URL\nhttps://arxiv.org/abs/2410.13720.\nLijun Yu, Jose Lezama, Nitesh Bharadwaj Gundavarapu, Luca Versari, Kihyuk Sohn, David Minnen, Yong\nCheng, Agrim Gupta, Xiuye Gu, Alexander G Hauptmann, Boqing Gong, Ming-Hsuan Yang, Irfan Essa,\nDavid A Ross, and Lu Jiang. Language model beats diffusion - tokenizer is key to visual generation. In The\nTwelfth International Conference on Learning Representations, 2024. URL https://openreview.net/\nforum?id=gzqrANCF4g.\nNvidia. Cosmos world foundation model platform for physical ai. arXiv preprint arXiv:2501.03575, 2025.\nZongjian Li, Bin Lin, Yang Ye, Liuhan Chen, Xinhua Cheng, Shenghai Yuan, and Li Yuan. Wf-vae: Enhancing\nvideo vae by wavelet-driven energy flow for latent video diffusion model. arXiv preprint arXiv:2411.17459,\n2024a.\nJunyu Chen, Han Cai, Junsong Chen, Enze Xie, Shang Yang, Haotian Tang, Muyang Li, and Song Han. Deep\ncompression autoencoder for efficient high-resolution diffusion models. In The Thirteenth International Con-\nference on Learning Representations, 2025. URL https://openreview.net/forum?id=wH8XXUOUZU.\nYaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for\ngenerative modeling, 2023. URL https://arxiv.org/abs/2210.02747.\nZhimin Li, Jianwei Zhang, Qin Lin, Jiangfeng Xiong, Yanxin Long, Xinchi Deng, Yingfang Zhang, Xingchao\nLiu, Minbin Huang, Zedong Xiao, Dayou Chen, Jiajun He, Jiahao Li, Wenyue Li, Chen Zhang, Rongwei\nQuan, Jianxiang Lu, Jiabin Huang, Xiaoyan Yuan, Xiaoxiao Zheng, Yixuan Li, Jihong Zhang, Chao Zhang,\nMeng Chen, Jie Liu, Zheng Fang, Weiyan Wang, Jinbao Xue, Yangyu Tao, Jianchen Zhu, Kai Liu, Sihuan\nLin, Yifu Sun, Yun Li, Dongdong Wang, Mingtao Chen, Zhichao Hu, Xiao Xiao, Yan Chen, Yuhong Liu, Wei\nLiu, Di Wang, Yong Yang, Jie Jiang, and Qinglin Lu. Hunyuan-dit: A powerful multi-resolution diffusion\ntransformer with fine-grained chinese understanding, 2024b. URL https://arxiv.org/abs/2405.08748.\nOfir Press, Noah A. Smith, and Mike Lewis. Train short, test long: Attention with linear biases enables input\nlength extrapolation, 2022. URL https://arxiv.org/abs/2108.12409.\nJunsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok,\nPing Luo, Huchuan Lu, and Zhenguo Li. Pixart-\u03b1: Fast training of diffusion transformer for photorealistic\ntext-to-image synthesis, 2023. URL https://arxiv.org/abs/2310.00426.\n31\nJianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. Roformer: Enhanced\ntransformer with rotary position embedding, 2023. URL https://arxiv.org/abs/2104.09864.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with\nhuman feedback. Advances in neural information processing systems, 35:27730\u201327744, 2022.\nPaul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement\nlearning from human preferences. Advances in neural information processing systems, 30, 2017.\nRafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn. Direct\npreference optimization: Your language model is secretly a reward model. Advances in Neural Information\nProcessing Systems, 36, 2024.\nBram Wallace, Meihua Dang, Rafael Rafailov, Linqi Zhou, Aaron Lou, Senthil Purushwalkam, Stefano Ermon,\nCaiming Xiong, Shafiq Joty, and Nikhil Naik. Diffusion model alignment using direct preference optimization.\nIn Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8228\u20138238,\n2024.\nKai Yang, Jian Tao, Jiafei Lyu, Chunjiang Ge, Jiaxin Chen, Weihan Shen, Xiaolong Zhu, and Xiu Li. Using\nhuman feedback to fine-tune diffusion models without any reward model. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, pages 8941\u20138951, 2024b.\nXingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data\nwith rectified flow. ArXiv, abs/2209.03003, 2022. URL https://api.semanticscholar.org/CorpusID:\n252111177.\nSangyun Lee, Zinan Lin, and Giulia Fanti.\nImproving the training of rectified flows.\narXiv preprint\narXiv:2405.20320, 2024.\nDeepak Narayanan, Mohammad Shoeybi, Jared Casper, Patrick LeGresley, Mostofa Patwary, Vijay Korthikanti,\nDmitri Vainbrand, Prethvi Kashinkunti, Julie Bernauer, Bryan Catanzaro, et al. Efficient large-scale language\nmodel training on gpu clusters using megatron-lm. In Proceedings of the International Conference for High\nPerformance Computing, Networking, Storage and Analysis, pages 1\u201315, 2021.",
    "model training on gpu clusters using megatron-lm. In Proceedings of the International Conference for High\nPerformance Computing, Networking, Storage and Analysis, pages 1\u201315, 2021.\nVijay Anand Korthikanti, Jared Casper, Sangkug Lym, Lawrence McAfee, Michael Andersch, Mohammad\nShoeybi, and Bryan Catanzaro. Reducing activation recomputation in large transformer models. Proceedings\nof Machine Learning and Systems, 5:341\u2013353, 2023.\nHao Liu, Matei Zaharia, and Pieter Abbeel. Ring attention with blockwise transformers for near-infinite context.\narXiv preprint arXiv:2310.01889, 2023.\nSam Ade Jacobs, Masahiro Tanaka, Chengming Zhang, Minjia Zhang, Shuaiwen Leon Song, Samyam Rajb-\nhandari, and Yuxiong He. Deepspeed ulysses: System optimizations for enabling training of extreme long\nsequence transformer models. arXiv preprint arXiv:2309.14509, 2023.\nSamyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimizations toward\ntraining trillion parameter models. In SC20: International Conference for High Performance Computing,\nNetworking, Storage and Analysis, pages 1\u201316. IEEE, 2020.\nZili Zhang, Yinmin Zhong, Ranchen Ming, Hanpeng Hu, Jianjian Sun, Zheng Ge, Yibo Zhu, and Xin Jin.\nDisttrain: Addressing model and data heterogeneity with disaggregated training for multimodal large language\nmodels. arXiv preprint arXiv:2408.04275, 2024.\nPyTorch.\nChannels Last Memory Format in PyTorch.\nPyTorch, https://pytorch.org/tutorials/\nintermediate/memory_format_tutorial.html, 2023. Accessed: Oct 4, 2023.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix,\nBaptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation\nlanguage models. arXiv preprint arXiv:2302.13971, 2023.\nPhilipp Moritz, Robert Nishihara, Stephanie Wang, Alexey Tumanov, Richard Liaw, Eric Liang, Melih Elibol,\nZongheng Yang, William Paul, Michael I. Jordan, and Ion Stoica. Ray: A distributed framework for emerging\nAI applications. In 13th USENIX Symposium on Operating Systems Design and Implementation (OSDI 18),\npages 561\u2013577, Carlsbad, CA, October 2018. USENIX Association. ISBN 978-1-939133-08-3.\nPritam Damania, Shen Li, Alban Desmaison, Alisson Azzolini, Brian Vaughan, Edward Yang, Gregory Chanan,\nGuoqiang Jerry Chen, Hongyi Jia, Howard Huang, et al. Pytorch rpc: Distributed deep learning built on\ntensor-optimized remote procedure calls. Proceedings of Machine Learning and Systems, 5:219\u2013231, 2023.\n32\nRuoyu Qin, Zheming Li, Weiran He, Mingxing Zhang, Yongwei Wu, Weimin Zheng, and Xinran Xu. Mooncake:\nA kvcache-centric disaggregated architecture for llm serving, 2024. URL https://arxiv.org/abs/2407.\n00079.\nMeta LlamaTeam. The llama 3 herd of models, April 2024. URL https://ai.meta.com/research/\npublications/the-llama-3-herd-of-models/.\nPySceneDetect Developers. PySceneDetect. PySceneDetect. https://www.scenedetect.com/.\nFFmpeg Developers. FFmpeg. FFmpeg. https://ffmpeg.org/.\nTsai-Shien Chen, Aliaksandr Siarohin, Willi Menapace, Ekaterina Deyneka, Hsiang-wei Chao, Byung Eun Jeon,\nYuwei Fang, Hsin-Ying Lee, Jian Ren, Ming-Hsuan Yang, et al. Panda-70m: Captioning 70m videos with\nmultiple cross-modality teachers. In Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 13320\u201313331, 2024a.\nChristoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo\nCoombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset\nfor training next generation image-text models. Advances in Neural Information Processing Systems, 35:\n25278\u201325294, 2022.\nLAION. Clip-based nsfw detector. https://github.com/LAION-AI/CLIP-based-NSFW-Detector, 2021.\nAccessed: [Insert Access Date].\nMingxing Tan and Quoc V Le. Efficientnet: Rethinking model scaling for convolutional neural networks. In\nProceedings of the 36th International Conference on Machine Learning, pages 6105\u20136114. PMLR, 2019.\nPaddleOCR Contributors. Paddleocr. https://github.com/PaddlePaddle/PaddleOCR, 2023. Accessed:\n[Insert Access Date].\nOpenCV Developers. OpenCV. OpenCV, https://opencv.org/, 2021. Accessed: August 1, 2023.\nJos\u00e9 Luis Pech-Pacheco, Gabriel Crist\u00f3bal, Jes\u00fas Chamorro-Martinez, and Joaqu\u00edn Fern\u00e1ndez-Valdivia. Diatom\nautofocusing in brightfield microscopy: a comparative study. In Proceedings 15th International Conference\non Pattern Recognition. ICPR-2000, volume 3, pages 314\u2013317. IEEE, 2000.\nJames Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang,\nJoyce Lee, Yufei Guo, et al. Improving image generation with better captions. Computer Science. https://cdn.\nopenai. com/papers/dall-e-3. pdf, 2(3):8, 2023.\nJ MacQueen. Some methods for classification and analysis of multivariate observations. In Proceedings of 5-th\nBerkeley Symposium on Mathematical Statistics and Probability/University of California Press, 1967.",
    "J MacQueen. Some methods for classification and analysis of multivariate observations. In Proceedings of 5-th\nBerkeley Symposium on Mathematical Statistics and Probability/University of California Press, 1967.\nXuejin Yang, Xingsheng Zhu, Han Zhang, Zhenyong Hou, and Rui Wang. Chinese clip: Contrastive vision-\nlanguage pretraining in chinese. arXiv preprint arXiv:2211.01335, 2022.\nXiaoliang Dai, Ji Hou, Chih-Yao Ma, Sam Tsai, Jialiang Wang, Rui Wang, Peizhao Zhang, Simon Vandenhende,\nXiaofang Wang, Abhimanyu Dubey, et al. Emu: Enhancing image generation models using photogenic\nneedles in a haystack. arXiv preprint arXiv:2309.15807, 2023.\nXin Tan, Yuetao Chen, Yimin Jiang, Xing Chen, Kun Yan, Nan Duan, Yibo Zhu, Daxin Jiang, and Hong\nXu. DSV: Exploiting Dynamic Sparsity to Accelerate Large-Scale Video DiT Training. arXiv preprint\narXiv:2502.07590, 2025.\nBoyuan Chen, Diego Marti Monso, Yilun Du, Max Simchowitz, Russ Tedrake, and Vincent Sitzmann. Diffusion\nforcing: Next-token prediction meets full-sequence diffusion, 2024b. URL https://arxiv.org/abs/\n2407.01392.\nYoav HaCohen, Nisan Chiprut, Benny Brazowski, Daniel Shalem, Dudu Moshe, Eitan Richardson, Eran Levin,\nGuy Shiran, Nir Zabari, Ori Gordon, Poriya Panet, Sapir Weissbuch, Victor Kulikov, Yaki Bitterman, Zeev\nMelumian, and Ofir Bibi. Ltx-video: Realtime video latent diffusion, 2024. URL https://arxiv.org/\nabs/2501.00103.\nDeyu Zhou, Quan Sun, Yuang Peng, Kun Yan, Runpei Dong, Duomin Wang, Zheng Ge, Nan Duan, Xiangyu\nZhang, Lionel M. Ni, and Heung-Yeung Shum. Taming teacher forcing for masked autoregressive video\ngeneration, 2025. URL https://arxiv.org/abs/2501.12389.\n33\nDeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu,\nShirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong\nShao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu,\nChenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li,\nFangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei\nXu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li,\nJiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin\nChen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang\nZhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang,\nMeng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu\nChen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen,\nShanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou,\nShuting Pan, S. S. Li, Shuang Zhou, Shaoqing Wu, Shengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun, T. Wang,\nWangding Zeng, Wanjia Zhao, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W. L.\nXiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie,\nXingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin Shen,\nXiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y. K.\nLi, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu,\nYichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan\nLiu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yunfan Xiong, Yuxiang\nLuo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu, Yanhong Xu, Yanping Huang, Yaohui Li, Yi Zheng,\nYuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha, Yuting Yan, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu,\nZhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu,\nZijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu",
    "Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu\nZhang, and Zhen Zhang. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning,\n2025. URL https://arxiv.org/abs/2501.12948.\n34\nContributors and Acknowledgments\nWe designate core contributors as those who have been involved in the development of Step-Video-\nT2V throughout its entire process, while contributors are those who worked on the early versions or\ncontributed part-time. All contributors are listed in alphabetical order by first name.\n\u2022 Core Contributors:\n\u2013 Model & Training: Guoqing Ma, Haoyang Huang, Kun Yan, Liangyu Chen, Nan Duan,\nShengming Yin.\n\u2013 Infrastructure: Changyi Wan, Ranchen Ming, Xiaoniu Song, Xing Chen, Yu Zhou.\n\u2013 Data & Evaluation: Deshan Sun, Deyu Zhou, Jian Zhou, Kaijun Tan, Kang An, Mei Chen,\nWei Ji, Qiling Wu, Wen Sun, Xin Han, Yanan Wei, Zheng Ge.\n\u2022 Contributors: Aojie Li, Bin Wang, Bizhu Huang, Bo Wang, Brian Li, Changxing Miao, Chen Xu,\nChenfei Wu, Chenguang Yu, Dapeng Shi, Dingyuan Hu, Enle Liu, Gang Yu, Ge Yang, Guanzhe\nHuang, Gulin Yan, Haiyang Feng, Hao Nie, Haonan Jia, Hanpeng Hu, Hanqi Chen, Haolong Yan,\nHeng Wang, Hongcheng Guo, Huilin Xiong, Huixin Xiong, Jiahao Gong, Jianchang Wu, Jiaoren\nWu, Jie Wu, Jie Yang, Jiashuai Liu, Jiashuo Li, Jingyang Zhang, Junjing Guo, Junzhe Lin, Kaixiang\nLi, Lei Liu, Lei Xia, Liang Zhao, Liguo Tan, Liwen Huang, Liying Shi, Ming Li, Mingliang Li,\nMuhua Cheng, Na Wang, Qiaohui Chen, Qinglin He, Qiuyan Liang, Quan Sun, Ran Sun, Rui\nWang, Shaoliang Pang, Shiliang Yang, Sitong Liu, Siqi Liu, Shuli Gao, Tiancheng Cao, Tianyu\nWang, Weipeng Ming, Wenqing He, Xu Zhao, Xuelin Zhang, Xianfang Zeng, Xiaojia Liu, Xuan\nYang, Yaqi Dai, Yanbo Yu, Yang Li, Yineng Deng, Yingming Wang, Yilei Wang, Yuanwei Lu, Yu\nChen, Yu Luo, Yuchu Luo, Yuhe Yin, Yuheng Feng, Yuxiang Yang, Zecheng Tang, Zekai Zhang,\nZidong Yang.\n\u2022 Project Sponsors: Binxing Jiao, Daxin Jiang, Heung-Yeung Shum, Jiansheng Chen, Jing Li,\nShuchang Zhou, Xiangyu Zhang, Xinhao Zhang, Yibo Zhu.\n\u2022 Corresponding Authors: Daxin Jiang (djiang@stepfun.com), Nan Duan (nduan@stepfun.com).\n35"
]